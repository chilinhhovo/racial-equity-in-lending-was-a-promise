{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2794f7bb",
   "metadata": {},
   "source": [
    "Race Code Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3db2dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_mapping = {\n",
    "    1: \"American Indian or Alaska Native\",\n",
    "    2: \"Asian\",\n",
    "    21: \"Asian Indian\",\n",
    "    22: \"Chinese\",\n",
    "    23: \"Filipino\",\n",
    "    24: \"Japanese\",\n",
    "    25: \"Korean\",\n",
    "    26: \"Vietnamese\",\n",
    "    27: \"Other Asian\",\n",
    "    3: \"Black or African American\",\n",
    "    4: \"Native Hawaiian or Other Pacific Islander\",\n",
    "    41: \"Native Hawaiian\",\n",
    "    42: \"Guamanian or Chamorro\",\n",
    "    43: \"Samoan\",\n",
    "    44: \"Other Pacific Islander\",\n",
    "    5: \"White\",\n",
    "    6: \"Information not provided\",\n",
    "    7: \"Not applicable\"\n",
    "}\n",
    "\n",
    "broad_categories = {\n",
    "    \"American Indian or Alaska Native\": \"Native American\",\n",
    "    \"Asian\": \"Asian\",\n",
    "    \"Asian Indian\": \"Asian\",\n",
    "    \"Chinese\": \"Asian\",\n",
    "    \"Filipino\": \"Asian\",\n",
    "    \"Japanese\": \"Asian\",\n",
    "    \"Korean\": \"Asian\",\n",
    "    \"Vietnamese\": \"Asian\",\n",
    "    \"Other Asian\": \"Asian\",\n",
    "    \"Black or African American\": \"Black\",\n",
    "    \"Native Hawaiian or Other Pacific Islander\": \"Pacific Islander\",\n",
    "    \"Native Hawaiian\": \"Pacific Islander\",\n",
    "    \"Guamanian or Chamorro\": \"Pacific Islander\",\n",
    "    \"Samoan\": \"Pacific Islander\",\n",
    "    \"Other Pacific Islander\": \"Pacific Islander\",\n",
    "    \"White\": \"White\",\n",
    "    \"Information not provided\": \"Other/Unknown\",\n",
    "    \"Not applicable\": \"Other/Unknown\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a422a5",
   "metadata": {},
   "source": [
    "Path finding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4263a49-1b1c-4e46-a519-3ef0de57faa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "unfiltered_folder = \"unfiltered\"\n",
    "strict_folder = \"filtered_data_strict\"\n",
    "os.makedirs(strict_folder, exist_ok=True)\n",
    "\n",
    "years = list(range(2018, 2025))\n",
    "unfiltered_files = [f\"{y}_hmda.csv\" for y in years]\n",
    "\n",
    "asian_race_codes = [2.0] + list(range(21, 28))\n",
    "white_race_code = [5.0]\n",
    "black_race_code = [3.0]\n",
    "all_race_codes = asian_race_codes + white_race_code + black_race_code\n",
    "\n",
    "def drop_and_log(df, condition, reason):\n",
    "    failed = df[~condition].copy()\n",
    "    failed['filter_reason'] = reason\n",
    "    kept = df[condition].copy()\n",
    "    return kept, failed\n",
    "\n",
    "def apply_filters(df, filters, year, out_folder):\n",
    "    df_filtered = df.copy()\n",
    "    logs = []\n",
    "    print(f\" Applying strict filters for {year}...\", flush=True)\n",
    "    for fname, cond in tqdm(filters, desc=f\" Filtering {year}\"):\n",
    "        before = len(df_filtered)\n",
    "        df_filtered, dropped = drop_and_log(df_filtered, cond, fname)\n",
    "        logs.append(dropped)\n",
    "        print(f\"    {fname:15s}: {before - len(df_filtered):,} rows dropped\", flush=True)\n",
    "    df_filtered['year'] = year\n",
    "    drop_log = pd.concat(logs)\n",
    "    drop_log['year'] = year\n",
    "    df_filtered.to_csv(os.path.join(out_folder, f\"{year}_filtered_hmda.csv\"), index=False)\n",
    "    drop_log.to_csv(os.path.join(out_folder, f\"{year}_dropped_log.csv\"), index=False)\n",
    "    print(f\" Saved {len(df_filtered):,} rows |  Dropped: {len(drop_log):,}\", flush=True)\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(f\"\\n Processing {year}...\", flush=True)\n",
    "    file_path = os.path.join(unfiltered_folder, unfiltered_files[i])\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"️ File not found: {file_path}\", flush=True)\n",
    "        continue\n",
    "\n",
    "    print(f\" Loading file: {file_path}\", flush=True)\n",
    "    t0 = time.time()\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "    print(f\" Loaded {len(df):,} rows in {time.time() - t0:.2f} sec\", flush=True)\n",
    "\n",
    "    race_col = 'applicant_race-1' if 'applicant_race-1' in df.columns else 'applicant_race_1'\n",
    "    df['row_id'] = df['lei'].astype(str) + \"_\" + df.index.astype(str)\n",
    "    df['total_units'] = pd.to_numeric(df['total_units'], errors='coerce')\n",
    "    df['debt_to_income_ratio_clean'] = df['debt_to_income_ratio'].apply(\n",
    "        lambda x: pd.to_numeric(\n",
    "            str(x).replace('%', '').replace('>', '').replace('<', '').replace('NA', '')\n",
    "                  .replace('Exempt', '').replace(' ', '').replace('--', '').replace('n/a', '')\n",
    "                  .replace('N/A', '').replace('Not Applicable', ''), errors='coerce'))\n",
    "\n",
    "    strict_filters = [\n",
    "        (\"action_taken\", df['action_taken'].isin([1, 2, 3])),\n",
    "        (\"income\", df['income'].notna() & (df['income'] > 0)),\n",
    "        (\"loan_type\", df['loan_type'] == 1),\n",
    "        (\"lien_status\", df['lien_status'] == 1),\n",
    "        (\"occupancy_type\", df['occupancy_type'] == 1),\n",
    "        (\"construction_method\", df['construction_method'] == 1),\n",
    "        (\"total_units\", df['total_units'] <= 4),\n",
    "        (\"loan_amount\", df['loan_amount'].notna()),\n",
    "        (\"property_value\", df['property_value'].notna()),\n",
    "        (\"dti_clean\", df['debt_to_income_ratio_clean'].notna()),\n",
    "        (\"race_code\", df[race_col].isin(all_race_codes)),\n",
    "        (\"state_code\", df['state_code'].notna())\n",
    "    ]\n",
    "\n",
    "    apply_filters(df, strict_filters, year, strict_folder)\n",
    "\n",
    "print(\"\\n DONE. All years processed with strict filters only.\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb87b974",
   "metadata": {},
   "source": [
    "Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeef3222-d696-4c28-94c7-182bc2c28af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_folder = \"filtered_data_strict\"  \n",
    "output_approval = \"approval_rates_by_lei_race_year_state.csv\"\n",
    "output_denial = \"denial_rates_by_lei_race_year_state.csv\"\n",
    "\n",
    "years = range(2018, 2025)\n",
    "island_state_codes = ['AS', 'GU', 'MP', 'PR', 'VI']\n",
    "\n",
    "race_map = {\n",
    "    2.0: 'Asian',\n",
    "    21: 'Asian', 22: 'Asian', 23: 'Asian', 24: 'Asian', 25: 'Asian', 26: 'Asian', 27: 'Asian',\n",
    "    3.0: 'Black',\n",
    "    5.0: 'White'\n",
    "}\n",
    "\n",
    "approval_rows = []\n",
    "denial_rows = []\n",
    "\n",
    "for year in years:\n",
    "    file_path = os.path.join(data_folder, f\"{year}_filtered_hmda.csv\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\" Skipping {year}, file not found: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    print(f\" Loading {file_path}\")\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "    race_col = 'applicant_race-1' if 'applicant_race-1' in df.columns else 'applicant_race_1'\n",
    "    df['race_label'] = df[race_col].map(race_map)\n",
    "    df = df[df['race_label'].notna()]\n",
    "\n",
    "    df = df[~df['state_code'].isin(island_state_codes)]\n",
    "\n",
    "    approved = df[df['action_taken'].isin([1, 2])].copy()\n",
    "    group_app = (\n",
    "        approved\n",
    "        .groupby(['lei', 'state_code', 'year', 'race_label'])\n",
    "        .size()\n",
    "        .reset_index(name='approved_count')\n",
    "    )\n",
    "\n",
    "    denied = df[df['action_taken'] == 3].copy()\n",
    "    group_den = (\n",
    "        denied\n",
    "        .groupby(['lei', 'state_code', 'year', 'race_label'])\n",
    "        .size()\n",
    "        .reset_index(name='denied_count')\n",
    "    )\n",
    "\n",
    "    total = df[df['action_taken'].isin([1, 2, 3])].copy()\n",
    "    group_total = (\n",
    "        total\n",
    "        .groupby(['lei', 'state_code', 'year', 'race_label'])\n",
    "        .size()\n",
    "        .reset_index(name='total_applications')\n",
    "    )\n",
    "\n",
    "    merged = group_total.merge(group_app, how='left', on=['lei', 'state_code', 'year', 'race_label'])\n",
    "    merged = merged.merge(group_den, how='left', on=['lei', 'state_code', 'year', 'race_label'])\n",
    "\n",
    "    merged['approved_count'] = merged['approved_count'].fillna(0)\n",
    "    merged['denied_count'] = merged['denied_count'].fillna(0)\n",
    "\n",
    "    merged['approval_rate'] = merged['approved_count'] / merged['total_applications']\n",
    "    merged['denial_rate'] = merged['denied_count'] / merged['total_applications']\n",
    "\n",
    "    approval_rows.append(merged[['lei', 'state_code', 'year', 'race_label', 'approved_count', 'total_applications', 'approval_rate']])\n",
    "    denial_rows.append(merged[['lei', 'state_code', 'year', 'race_label', 'denied_count', 'total_applications', 'denial_rate']])\n",
    "\n",
    "approval_df = pd.concat(approval_rows)\n",
    "denial_df = pd.concat(denial_rows)\n",
    "\n",
    "approval_df.to_csv(output_approval, index=False)\n",
    "denial_df.to_csv(output_denial, index=False)\n",
    "\n",
    "print(f\" Approval CSV saved to {output_approval}\")\n",
    "print(f\" Denial CSV saved to {output_denial}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e889ea7c",
   "metadata": {},
   "source": [
    "Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f5f4c7-1e01-44e3-9946-91addd0581fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_folder = \"filtered_data_strict\"  \n",
    "output_approval = \"approval_rates_by_lei_race_year_state.csv\"\n",
    "output_denial = \"denial_rates_by_lei_race_year_state.csv\"\n",
    "\n",
    "years = range(2018, 2025)\n",
    "island_state_codes = ['AS', 'GU', 'MP', 'PR', 'VI']\n",
    "\n",
    "race_map = {\n",
    "    2.0: 'Asian',\n",
    "    21: 'Asian', 22: 'Asian', 23: 'Asian', 24: 'Asian', 25: 'Asian', 26: 'Asian', 27: 'Asian',\n",
    "    3.0: 'Black',\n",
    "    5.0: 'White'\n",
    "}\n",
    "\n",
    "approval_rows = []\n",
    "denial_rows = []\n",
    "\n",
    "for year in years:\n",
    "    file_path = os.path.join(data_folder, f\"{year}_filtered_hmda.csv\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\" Skipping {year}, file not found: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    print(f\" Loading {file_path}\")\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "    race_col = 'applicant_race-1' if 'applicant_race-1' in df.columns else 'applicant_race_1'\n",
    "    df['race_label'] = df[race_col].map(race_map)\n",
    "    df = df[df['race_label'].notna()]\n",
    "\n",
    "    df = df[~df['state_code'].isin(island_state_codes)]\n",
    "\n",
    "    approved = df[df['action_taken'].isin([1, 2])].copy()\n",
    "    group_app = (\n",
    "        approved\n",
    "        .groupby(['lei', 'state_code', 'year', 'race_label'])\n",
    "        .size()\n",
    "        .reset_index(name='approved_count')\n",
    "    )\n",
    "\n",
    "    denied = df[df['action_taken'] == 3].copy()\n",
    "    group_den = (\n",
    "        denied\n",
    "        .groupby(['lei', 'state_code', 'year', 'race_label'])\n",
    "        .size()\n",
    "        .reset_index(name='denied_count')\n",
    "    )\n",
    "\n",
    "    total = df[df['action_taken'].isin([1, 2, 3])].copy()\n",
    "    group_total = (\n",
    "        total\n",
    "        .groupby(['lei', 'state_code', 'year', 'race_label'])\n",
    "        .size()\n",
    "        .reset_index(name='total_applications')\n",
    "    )\n",
    "\n",
    "    merged = group_total.merge(group_app, how='left', on=['lei', 'state_code', 'year', 'race_label'])\n",
    "    merged = merged.merge(group_den, how='left', on=['lei', 'state_code', 'year', 'race_label'])\n",
    "\n",
    "    merged['approved_count'] = merged['approved_count'].fillna(0)\n",
    "    merged['denied_count'] = merged['denied_count'].fillna(0)\n",
    "\n",
    "    merged['approval_rate'] = merged['approved_count'] / merged['total_applications']\n",
    "    merged['denial_rate'] = merged['denied_count'] / merged['total_applications']\n",
    "\n",
    "    approval_rows.append(merged[['lei', 'state_code', 'year', 'race_label', 'approved_count', 'total_applications', 'approval_rate']])\n",
    "    denial_rows.append(merged[['lei', 'state_code', 'year', 'race_label', 'denied_count', 'total_applications', 'denial_rate']])\n",
    "\n",
    "approval_df = pd.concat(approval_rows)\n",
    "denial_df = pd.concat(denial_rows)\n",
    "\n",
    "approval_df.to_csv(output_approval, index=False)\n",
    "denial_df.to_csv(output_denial, index=False)\n",
    "\n",
    "print(f\" Approval CSV saved to {output_approval}\")\n",
    "print(f\" Denial CSV saved to {output_denial}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4df338e",
   "metadata": {},
   "source": [
    "Load and prepare data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21e4ab2-8be0-4cba-8d82-88de7c90fa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"approval_rates_by_lei_race_year_state.csv\")\n",
    "\n",
    "df['year'] = df['year'].astype(str)\n",
    "\n",
    "if df['approval_rate'].max() <= 1:\n",
    "    df['approval_rate'] *= 100\n",
    "\n",
    "df['race_label'] = df['race_label'].str.title().replace({\n",
    "    \"Hawaiian\": \"Hawaiian/Pacific Islander\",\n",
    "    \"Indigenous\": \"Indigenous\",\n",
    "    \"American Indian Or Alaska Native\": \"Indigenous\",\n",
    "    \"Pacific Islander\": \"Hawaiian/Pacific Islander\"\n",
    "})\n",
    "\n",
    "main_races = [\"White\", \"Black\", \"Asian\", \"Indigenous\", \"Hawaiian/Pacific Islander\"]\n",
    "df = df[df['race_label'].isin(main_races)]\n",
    "\n",
    "avg_df = (\n",
    "    df.groupby(['state_code', 'year', 'race_label'])['approval_rate']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "race_order = [\"White\", \"Black\", \"Asian\", \"Indigenous\", \"Hawaiian/Pacific Islander\"]\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "g = sns.FacetGrid(\n",
    "    avg_df,\n",
    "    col=\"state_code\", col_wrap=5,\n",
    "    height=3, aspect=1.2,\n",
    "    sharey=True\n",
    ")\n",
    "\n",
    "g.map_dataframe(\n",
    "    sns.lineplot,\n",
    "    x=\"year\", y=\"approval_rate\",\n",
    "    hue=\"race_label\", marker=\"o\",\n",
    "    hue_order=race_order,\n",
    "    errorbar=None  # replaces deprecated ci=None\n",
    ")\n",
    "\n",
    "g.set_titles(\"{col_name}\")\n",
    "g.set_axis_labels(\"Year\", \"Approval Rate (%)\")\n",
    "g.add_legend(title=\"Race\")\n",
    "g.set(ylim=(60, 100))  # Adjust if needed\n",
    "\n",
    "plt.subplots_adjust(top=0.9)\n",
    "g.fig.suptitle(\"Mortgage Approval Rates by Race (2018–2024)\")\n",
    "plt.tight_layout()\n",
    "\n",
    "g.savefig(\"approval_rates_by_state_by_lei_all_races.svg\", format=\"svg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efc728f",
   "metadata": {},
   "source": [
    "Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e564481b-51fd-4f67-bfec-938820a8e0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"approval_rates_by_lei_race_year_state.csv\")\n",
    "\n",
    "df['year'] = df['year'].astype(str)\n",
    "\n",
    "if df['approval_rate'].max() <= 1:\n",
    "    df['approval_rate'] *= 100\n",
    "\n",
    "df['race_label'] = df['race_label'].str.strip().str.title().replace({\n",
    "    \"Hawaiian\": \"Hawaiian/Pacific Islander\",\n",
    "    \"Pacific Islander\": \"Hawaiian/Pacific Islander\",\n",
    "    \"American Indian Or Alaska Native\": \"Indigenous\",\n",
    "    \"Native American\": \"Indigenous\",\n",
    "    \"Indigenous\": \"Indigenous\"  # in case already used\n",
    "})\n",
    "\n",
    "main_races = [\"White\", \"Black\", \"Asian\", \"Indigenous\", \"Hawaiian/Pacific Islander\"]\n",
    "df = df[df['race_label'].isin(main_races)]\n",
    "\n",
    "avg_df = (\n",
    "    df.groupby(['state_code', 'year', 'race_label'])['approval_rate']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "race_order = [\"White\", \"Black\", \"Asian\", \"Indigenous\", \"Hawaiian/Pacific Islander\"]\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "g = sns.FacetGrid(\n",
    "    avg_df,\n",
    "    col=\"state_code\", col_wrap=5,\n",
    "    height=3, aspect=1.2,\n",
    "    sharey=True\n",
    ")\n",
    "\n",
    "g.map_dataframe(\n",
    "    sns.lineplot,\n",
    "    x=\"year\", y=\"approval_rate\",\n",
    "    hue=\"race_label\",\n",
    "    hue_order=race_order,\n",
    "    marker=\"o\",\n",
    "    errorbar=None\n",
    ")\n",
    "\n",
    "g.set_titles(\"{col_name}\")\n",
    "g.set_axis_labels(\"Year\", \"Approval Rate (%)\")\n",
    "g.add_legend(title=\"Race\")\n",
    "g.set(ylim=(60, 100))  # Adjust if needed\n",
    "\n",
    "plt.subplots_adjust(top=0.9)\n",
    "g.fig.suptitle(\"Mortgage Approval Rates by Race by LEI (2018–2024)\")\n",
    "plt.tight_layout()\n",
    "\n",
    "g.savefig(\"approval_rates_by_state_by_lei_all_main_races.svg\", format=\"svg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014b1a86-cb5d-425e-8e66-a467faa9898e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "data_folder = \"filtered_data_strict\"\n",
    "years = range(2018, 2025)\n",
    "\n",
    "race_map = {\n",
    "    2.0: 'Asian', 21: 'Asian', 22: 'Asian', 23: 'Asian', 24: 'Asian', 25: 'Asian', 26: 'Asian', 27: 'Asian',\n",
    "    3.0: 'Black',\n",
    "    5.0: 'White'\n",
    "}\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for year in years:\n",
    "    file_path = os.path.join(data_folder, f\"{year}_filtered_hmda.csv\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\" Skipping {year}, file not found.\")\n",
    "        continue\n",
    "\n",
    "    print(f\" Loading {file_path}\")\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "    race_col = 'applicant_race-1' if 'applicant_race-1' in df.columns else 'applicant_race_1'\n",
    "    df['race_label'] = df[race_col].map(race_map)\n",
    "    df = df[df['race_label'].notna()]\n",
    "    df['year'] = year\n",
    "\n",
    "    if 'census_tract' in df.columns and 'state_code' in df.columns and 'county_code' in df.columns:\n",
    "        df['tract_fips'] = (\n",
    "            df['state_code'].astype(str).str.zfill(2) +\n",
    "            df['county_code'].astype(str).str.zfill(3) +\n",
    "            df['census_tract'].astype(str).str.replace(r'\\.0$', '', regex=True).str.zfill(6)\n",
    "        )\n",
    "    elif 'tract_fips' not in df.columns:\n",
    "        print(f\"️ No tract info found for {year}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    all_data.append(df[['tract_fips', 'state_code', 'race_label', 'action_taken', 'year']])\n",
    "\n",
    "df_all = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "approved = df_all[df_all['action_taken'].isin([1, 2])]\n",
    "total = df_all[df_all['action_taken'].isin([1, 2, 3])]\n",
    "\n",
    "approved_counts = (\n",
    "    approved\n",
    "    .groupby(['year', 'state_code', 'tract_fips', 'race_label'])\n",
    "    .size()\n",
    "    .reset_index(name='approved')\n",
    ")\n",
    "\n",
    "total_counts = (\n",
    "    total\n",
    "    .groupby(['year', 'state_code', 'tract_fips', 'race_label'])\n",
    "    .size()\n",
    "    .reset_index(name='total')\n",
    ")\n",
    "\n",
    "merged = pd.merge(total_counts, approved_counts, how='left', on=['year', 'state_code', 'tract_fips', 'race_label'])\n",
    "merged['approved'] = merged['approved'].fillna(0).astype(int)\n",
    "merged['approval_rate'] = (merged['approved'] / merged['total']).round(3)\n",
    "\n",
    "merged = merged.rename(columns={\"tract_fips\": \"GEOID\"})\n",
    "merged['GEOID'] = merged['GEOID'].astype(str).str.zfill(11)\n",
    "\n",
    "print(\" Final dataset shape:\", merged.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06633563",
   "metadata": {},
   "source": [
    "Copy for step-by-step filtering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f23f7e-53ea-4959-b7b4-b04b1fad415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241a4474-cd45-4f0d-a0c7-750c0bdbd04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_folder = \"unfiltered\"\n",
    "output_file = \"tract_approval_rates_by_race.csv\"\n",
    "years = range(2018, 2025)\n",
    "\n",
    "race_map = {\n",
    "    2.0: 'Asian',\n",
    "    21: 'Asian', 22: 'Asian', 23: 'Asian', 24: 'Asian', 25: 'Asian', 26: 'Asian', 27: 'Asian',\n",
    "    3.0: 'Black',\n",
    "    5.0: 'White'\n",
    "}\n",
    "\n",
    "rows = []\n",
    "\n",
    "for year in years:\n",
    "    file_path = os.path.join(data_folder, f\"{year}_hmda.csv\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"️ Missing: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    print(f\" Loading {file_path}\")\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "    df['year'] = year\n",
    "\n",
    "    race_col = 'applicant_race-1' if 'applicant_race-1' in df.columns else 'applicant_race_1'\n",
    "    df['race_label'] = df[race_col].map(race_map)\n",
    "    df = df[df['race_label'].notna()]\n",
    "\n",
    "    df = df[df['census_tract'].notna() & df['state_code'].notna() & df['county_code'].notna()]\n",
    "\n",
    "    df['tract_fips'] = (\n",
    "        df['state_code'].astype(str).str.zfill(2) +\n",
    "        df['county_code'].astype(str).str.zfill(3) +\n",
    "        df['census_tract'].astype(str).str.zfill(6)\n",
    "    )\n",
    "\n",
    "    approved = df[df['action_taken'].isin([1, 2])]\n",
    "    total = df[df['action_taken'].isin([1, 2, 3])]\n",
    "\n",
    "    group_total = total.groupby(['tract_fips', 'race_label']).size().reset_index(name='total')\n",
    "    group_approved = approved.groupby(['tract_fips', 'race_label']).size().reset_index(name='approved')\n",
    "\n",
    "    merged = pd.merge(group_total, group_approved, how='left', on=['tract_fips', 'race_label'])\n",
    "    merged['approved'] = merged['approved'].fillna(0)\n",
    "    merged['approval_rate'] = merged['approved'] / merged['total']\n",
    "    merged['year'] = year\n",
    "\n",
    "    rows.append(merged)\n",
    "\n",
    "df_result = pd.concat(rows, ignore_index=True)\n",
    "df_result.to_csv(output_file, index=False)\n",
    "print(f\" Saved tract-level approval rates to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae5b9f3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdfeb5a-8d2b-47f4-abfc-5577e77f6172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d86adde5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb03d8d-1fc9-4d90-b7e3-df3d5579996d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "\n",
    "output_dir = \"shapefiles_2024\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "fips_codes = [\n",
    "    f\"{i:02d}\" for i in range(1, 57)\n",
    "    if i not in (3, 7, 14, 43, 52, 58, 59, 61, 62, 63, 64, 65, 67, 68, 70, 71, 73, 74, 75, 76, 77, 79)  # exclude gaps\n",
    "] + [\"60\", \"66\", \"69\", \"72\", \"78\"]  # Add territories\n",
    "\n",
    "base_url = \"https://www2.census.gov/geo/tiger/TIGER2024/TRACT/\"\n",
    "\n",
    "for fips in fips_codes:\n",
    "    filename = f\"tl_2024_{fips}_tract.zip\"\n",
    "    url = base_url + filename\n",
    "    print(f\"Downloading {filename}...\")\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        with ZipFile(BytesIO(response.content)) as zip_ref:\n",
    "            zip_ref.extractall(os.path.join(output_dir, f\"tl_2024_{fips}\"))\n",
    "\n",
    "        print(f\" Saved to: {output_dir}/tl_2024_{fips}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Failed for {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32b708e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e18b4e9-d590-411d-8150-89ac84b0a6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "shapefile_dirs = glob.glob(\"shapefiles_2024/tl_2024_*\")\n",
    "\n",
    "gdfs = []\n",
    "\n",
    "for folder in shapefile_dirs:\n",
    "    shp_files = glob.glob(os.path.join(folder, \"*.shp\"))\n",
    "    if not shp_files:\n",
    "        continue  # Skip if no .shp file\n",
    "    shp = shp_files[0]\n",
    "\n",
    "    try:\n",
    "        gdf = gpd.read_file(shp)[['GEOID', 'geometry']]\n",
    "        gdfs.append(gdf)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {shp}: {e}\")\n",
    "\n",
    "gdf_all = gpd.GeoDataFrame(pd.concat(gdfs, ignore_index=True)).drop_duplicates(subset='GEOID')\n",
    "gdf_all = gdf_all.set_crs(\"EPSG:4269\")  # Census default projection\n",
    "gdf_all = gdf_all.to_crs(\"EPSG:4326\")   # Convert for Plotly or web use\n",
    "\n",
    "print(f\" Combined tracts: {len(gdf_all)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eef402",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193c3181-a7fb-4e20-97e1-c32c9882d419",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gdf_all.columns)\n",
    "print(df_all.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba21fcf6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61889a63-2275-419f-9c30-ec506d1a710c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_all[\"state_code\"] = gdf_all[\"GEOID\"].str[:2]\n",
    "\n",
    "merged = gdf_all.merge(df_all, on=\"state_code\", how=\"inner\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de2b3c8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a5bf44-1755-459b-9b7e-02ded818a104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "input_dir = \"unfiltered\"  # folder with raw yearly HMDA files (e.g. 2018_hmda.csv)\n",
    "output_file = \"tract_level_approval_rates.csv\"\n",
    "years = range(2018, 2025)\n",
    "\n",
    "race_map = {\n",
    "    2.0: 'Asian',\n",
    "    21: 'Asian', 22: 'Asian', 23: 'Asian', 24: 'Asian', 25: 'Asian', 26: 'Asian', 27: 'Asian',\n",
    "    3.0: 'Black',\n",
    "    5.0: 'White'\n",
    "}\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for year in years:\n",
    "    file_path = os.path.join(input_dir, f\"{year}_hmda.csv\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\" File not found: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    print(f\" Loading {file_path}\")\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "    race_col = 'applicant_race-1' if 'applicant_race-1' in df.columns else 'applicant_race_1'\n",
    "    tract_col = 'census_tract'\n",
    "\n",
    "    df = df[df[tract_col].notna() & df[race_col].isin(race_map.keys())].copy()\n",
    "\n",
    "    df['tract_fips'] = df['state_code'].astype(str).str.zfill(2) + \\\n",
    "                       df['county_code'].astype(str).str.zfill(3) + \\\n",
    "                       df[tract_col].astype(str).str.replace('.', '', regex=False).str.zfill(6)\n",
    "\n",
    "    df['race_label'] = df[race_col].map(race_map)\n",
    "    df['year'] = year\n",
    "\n",
    "    df = df[df['action_taken'].isin([1, 2, 3])]\n",
    "\n",
    "    grouped = (\n",
    "        df.groupby(['tract_fips', 'year', 'race_label', 'action_taken'])\n",
    "          .size()\n",
    "          .reset_index(name='count')\n",
    "    )\n",
    "\n",
    "    pivot = grouped.pivot(index=['tract_fips', 'year', 'race_label'], columns='action_taken', values='count').reset_index()\n",
    "    pivot = pivot.rename(columns={1: 'loan_originated', 2: 'approved_but_not_originated', 3: 'denied'})\n",
    "\n",
    "    for col in ['loan_originated', 'approved_but_not_originated', 'denied']:\n",
    "        pivot[col] = pivot.get(col, 0).fillna(0)\n",
    "\n",
    "    pivot['approved'] = pivot['loan_originated'] + pivot['approved_but_not_originated']\n",
    "    pivot['total'] = pivot['approved'] + pivot['denied']\n",
    "    pivot['approval_rate'] = pivot['approved'] / pivot['total']\n",
    "    all_data.append(pivot)\n",
    "\n",
    "df_final = pd.concat(all_data)\n",
    "df_final.to_csv(output_file, index=False)\n",
    "print(f\" Saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f11f35",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d13383-0c9e-40c0-a2f1-e9c0841894ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "full_df = pd.read_csv(\"tract_approval_rates_by_race.csv\")\n",
    "\n",
    "full_df['tract_geoid'] = full_df['tract_geoid'].astype(str).str.zfill(11)\n",
    "\n",
    "output_folder = \"mapping\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "years = full_df['year'].unique()\n",
    "for year in years:\n",
    "    df_year = full_df[full_df['year'] == year]\n",
    "    df_year.to_csv(os.path.join(output_folder, f\"tract_approval_{year}.csv\"), index=False)\n",
    "\n",
    "print(\" All yearly files saved to the 'mapping' folder.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0dddc4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd76554-8c89-46ba-94e8-468d176dec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"mapping/tract_approval_2018.csv\")\n",
    "print(df.columns)\n",
    "print(df['tract_geoid'].astype(str).str.len().value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65358fff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc44af87-fe60-4b20-9645-c31db4327398",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tract_geoid'] = df['tract_geoid'].astype(str).str.zfill(11)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c96cd1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61af4b3-ee87-4d3f-b079-7e3ede15c901",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas geopandas shapely plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1a28fd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f115e31-a0bb-4920-afc9-1c69403325e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"tract_level_approval_rates.csv\")\n",
    "\n",
    "print(df.head())\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2accd4b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1529a40-fa86-4c32-88d4-e6e386e96c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2eefc0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b48a59c-19b8-47b7-8647-ee39ba02608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\"tract_fips\": \"GEOID\"})\n",
    "df['GEOID'] = df['GEOID'].str[-11:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ac4c70",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deaeb06-d36a-4204-875d-160ef1f3d0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "\n",
    "shapefile_root = Path(\"shapefiles_2024\")\n",
    "all_states = list(shapefile_root.glob(\"tl_2024_*\"))\n",
    "\n",
    "gdf_list = []\n",
    "for folder in all_states:\n",
    "    shp_file = next(folder.glob(\"*.shp\"), None)\n",
    "    if shp_file is not None:\n",
    "        gdf = gpd.read_file(shp_file)\n",
    "        gdf = gdf[['GEOID', 'geometry']]  # only keep necessary columns\n",
    "        gdf['GEOID'] = gdf['GEOID'].astype(str).str.zfill(11)\n",
    "        gdf_list.append(gdf)\n",
    "\n",
    "gdf_all = gpd.GeoDataFrame(pd.concat(gdf_list, ignore_index=True), crs=gdf_list[0].crs)\n",
    "\n",
    "print(f\" Total tracts loaded: {len(gdf_all)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36fb38b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556d6bd4-502b-40fd-bfa7-e73ef3d397eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"gdf_all columns:\", gdf_all.columns.tolist())\n",
    "print(\"df_all columns:\", df_all.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1676a554",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1ff8f1-5fc3-4150-ab1d-63a3ecad1d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all.rename(columns={\"tract_fips\": \"GEOID\"})\n",
    "df_all['GEOID'] = df_all['GEOID'].astype(str).str.zfill(11)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceb5588",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bb0f24-1e31-4843-b75d-2a92632ace4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = gdf_all.merge(df_all, on=\"GEOID\", how=\"inner\")\n",
    "print(f\"Merged rows: {len(merged)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c7dc06",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1d73e2-528d-47d6-b234-5347972fd43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = gdf_all.merge(df, on='GEOID', how='inner')\n",
    "print(f\" Merged records: {len(merged)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc83127",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762bb384-be46-4a3e-8982-caeb6dc2fa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from pathlib import Path\n",
    "\n",
    "Path(\"svg_maps\").mkdir(exist_ok=True)\n",
    "\n",
    "for (race, year), sub in merged.groupby(['race_label', 'year']):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    sub.plot(column='approval_rate',\n",
    "             ax=ax,\n",
    "             legend=True,\n",
    "             cmap='viridis',\n",
    "             linewidth=0.1,\n",
    "             edgecolor='white')\n",
    "\n",
    "    ax.set_title(f\"{race} - {year}\", fontsize=16)\n",
    "    ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"svg_maps/{race}_{year}.svg\", format='svg')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0227005b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053a6d9c-d80d-4c71-8a76-940db4bfb30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "Path(\"png_maps\").mkdir(exist_ok=True)\n",
    "\n",
    "for (race, year), sub in merged.groupby(['race_label', 'year']):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    sub.plot(column='approval_rate',\n",
    "             ax=ax,\n",
    "             legend=True,\n",
    "             cmap='viridis',\n",
    "             linewidth=0.1,\n",
    "             edgecolor='white')\n",
    "\n",
    "    ax.set_title(f\"{race} - {year}\", fontsize=16)\n",
    "    ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(f\"png_maps/{race}_{year}.png\", format='png', dpi=300)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19eb018",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b05484-bc61-4b30-8cf5-19fbcc43101e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b117015e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9dc82e-2421-4acc-8fb5-dec7331da3ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c78cb30c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a195b9a-5eed-4a0d-9f67-91cf42f1c71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "\n",
    "shapefile_root = 'shapefiles_2024' \n",
    "all_states = []\n",
    "\n",
    "for folder in os.listdir(shapefile_root):\n",
    "    state_path = os.path.join(shapefile_root, folder)\n",
    "    if os.path.isdir(state_path):\n",
    "        for file in os.listdir(state_path):\n",
    "            if file.endswith('.shp') and 'tract' in file:\n",
    "                shp = gpd.read_file(os.path.join(state_path, file))\n",
    "                all_states.append(shp)\n",
    "                break  # only one .shp per folder is needed\n",
    "\n",
    "tracts_all = gpd.GeoDataFrame(pd.concat(all_states, ignore_index=True))\n",
    "tracts_all['GEOID'] = tracts_all['GEOID'].astype(str)\n",
    "print(\" Combined tracts shape:\", tracts_all.shape)\n",
    "output_gpkg = 'combined_tracts_2024.gpkg'\n",
    "tracts_all.to_file(output_gpkg, layer='tracts', driver=\"GPKG\")\n",
    "print(f\"️ Saved combined geopackage to: {output_gpkg}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946bc266",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85acebbb-948e-4194-9e53-5f4061c4470b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b401b47",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c597f6-1709-4168-b92b-cdaf8331bdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.makedirs('maps', exist_ok=True)\n",
    "\n",
    "\n",
    "approval = pd.read_csv('tract_approval_rates_by_race.csv')\n",
    "approval['tract_geoid'] = approval['tract_geoid'].astype(str)\n",
    "\n",
    "for year in sorted(approval['year'].unique()):\n",
    "    for race in approval['race_label'].unique():\n",
    "        sub = approval[(approval['year'] == year) & (approval['race_label'] == race)]\n",
    "        merged = tracts_all.merge(sub, left_on='GEOID', right_on='tract_geoid')\n",
    "\n",
    "        if merged.empty:\n",
    "            continue\n",
    "\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "        merged.plot(column='approval_rate', cmap='plasma', linewidth=0, edgecolor='none',\n",
    "                    legend=True, ax=ax)\n",
    "        ax.set_title(f\"{race} Approval Rate by Tract - {year}\", fontsize=16)\n",
    "        ax.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"maps/{race.lower()}_approval_rate_{year}.png\", dpi=600)\n",
    "        plt.close()\n",
    "\n",
    "print(\" All maps saved to `maps/` folder.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334ce573",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b2dbcb-30ae-4d30-b9cb-15aeb1597d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "gdf_all_tracts = gpd.read_file(\"combined_tracts_2024.gpkg\") \n",
    "gdf_all_tracts[\"tract_geoid\"] = gdf_all_tracts[\"GEOID\"]\n",
    "all_tracts = set(gdf_all_tracts[\"tract_geoid\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4b2311",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f6519f-9ebd-41c2-91f2-8997cbd0b3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"tract_approval_rates_by_race.csv\")\n",
    "\n",
    "race = \"Asian\"\n",
    "year = 2020\n",
    "subset = df[(df[\"race_label\"] == race) & (df[\"year\"] == year)]\n",
    "tracts_with_data = set(subset[\"tract_geoid\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e011f74",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0378094c-427f-4c25-814f-2ab237a98575",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_tracts = all_tracts - tracts_with_data\n",
    "\n",
    "print(f\"{len(missing_tracts)} tracts missing for {race} - {year}\")\n",
    "print(list(missing_tracts)[:20])  # Preview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1580c8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528992be-12a8-4d70-935f-fb1d5f2df4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dropped_log_path = \"filtered_data_strict/2020_dropped_log.csv\"\n",
    "missing_tracts = [\n",
    "    '34013019200', '41039000904', '06037264103', '27053000101', '25013812001',\n",
    "    '36061003002', '36103135209', '36117020900', '36047098200', '36071013202',\n",
    "    '06037123020', '39061026800', '41027950202', '12086011203', '06085503804',\n",
    "    '06059110115', '18049953200', '34023008505', '50023955400', '45057011100'\n",
    "]\n",
    "\n",
    "asian_race_codes = [2.0, 21, 22, 23, 24, 25, 26, 27]  # All Asian HMDA race codes\n",
    "\n",
    "df_dropped = pd.read_csv(dropped_log_path, low_memory=False)\n",
    "\n",
    "race_col = 'applicant_race-1' if 'applicant_race-1' in df_dropped.columns else 'applicant_race_1'\n",
    "tract_col = 'census_tract'\n",
    "\n",
    "df_dropped = df_dropped[df_dropped[tract_col].notna()]\n",
    "df_dropped['tract_fips'] = df_dropped['state_code'].astype(str).str.zfill(2) + \\\n",
    "                           df_dropped['county_code'].astype(str).str.zfill(3) + \\\n",
    "                           df_dropped[tract_col].astype(str).str.replace('.', '', regex=False).str.zfill(6)\n",
    "\n",
    "df_asian = df_dropped[df_dropped[race_col].isin(asian_race_codes)]\n",
    "\n",
    "df_missing_asian = df_asian[df_asian['tract_fips'].isin(missing_tracts)]\n",
    "\n",
    "print(f\" Asian applications found in {df_missing_asian['tract_fips'].nunique()} of the {len(missing_tracts)} missing tracts\")\n",
    "\n",
    "summary = df_missing_asian['tract_fips'].value_counts().rename_axis('tract_fips').reset_index(name='asian_app_count')\n",
    "print(summary.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151bc76d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f41ee5-9915-4d60-b479-4ce39ce0f6a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db55437d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8535c860-c441-49b9-88c5-5b18562b596b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4549fc07",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57089776-81fe-4ab3-a83e-fa80ae212109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "input_dir = \"filtered_data_strict\"  # folder with raw yearly HMDA files\n",
    "output_file = \"tract_approval_rates_all_races.csv\"\n",
    "years = range(2018, 2025)\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for year in years:\n",
    "    file_path = os.path.join(input_dir, f\"{year}_filtered_hmda.csv\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\" File not found: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    print(f\" Loading {file_path}\")\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "    df = df[df['census_tract'].notna() & df['state_code'].notna() & df['county_code'].notna()].copy()\n",
    "\n",
    "    df['tract_geoid'] = (\n",
    "        df['state_code'].astype(str).str.zfill(2) +\n",
    "        df['county_code'].astype(str).str.zfill(3) +\n",
    "        df['census_tract'].astype(str).str.replace('.', '', regex=False).str.zfill(6)\n",
    "    )\n",
    "\n",
    "    df['year'] = year\n",
    "\n",
    "    df = df[df['action_taken'].isin([1, 2, 3])]\n",
    "\n",
    "    grouped = (\n",
    "        df.groupby(['tract_geoid', 'year', 'action_taken'])\n",
    "          .size()\n",
    "          .reset_index(name='count')\n",
    "    )\n",
    "\n",
    "    pivot = grouped.pivot(index=['tract_geoid', 'year'], columns='action_taken', values='count').reset_index()\n",
    "    pivot = pivot.rename(columns={1: 'loan_originated', 2: 'approved_but_not_originated', 3: 'denied'})\n",
    "\n",
    "    for col in ['loan_originated', 'approved_but_not_originated', 'denied']:\n",
    "        pivot[col] = pivot.get(col, 0).fillna(0)\n",
    "\n",
    "    pivot['approved'] = pivot['loan_originated'] + pivot['approved_but_not_originated']\n",
    "    pivot['total'] = pivot['approved'] + pivot['denied']\n",
    "    pivot['approval_rate'] = pivot['approved'] / pivot['total']\n",
    "\n",
    "    all_data.append(pivot)\n",
    "\n",
    "df_final = pd.concat(all_data, ignore_index=True)\n",
    "df_final.to_csv(output_file, index=False)\n",
    "print(f\" Saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a61d35c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09d807e-d8f4-4023-8519-123f97bb2079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "tracts = gpd.read_file(\"combined_tracts_2024.gpkg\")\n",
    "tracts['GEOID'] = tracts['GEOID'].astype(str)\n",
    "\n",
    "tracts = tracts.set_crs(\"EPSG:4269\")  # NAD83 (if missing)\n",
    "tracts = tracts.to_crs(\"EPSG:5070\")   # Albers Equal Area (for continental US)\n",
    "\n",
    "tracts = tracts[tracts.geometry.is_valid & tracts.geometry.notnull()]\n",
    "\n",
    "df = pd.read_csv(\"tract_approval_rates_all_races.csv\")\n",
    "df['tract_geoid_clean'] = df['tract_geoid'].str.extract(r'(\\d{11})')\n",
    "df = df[df['tract_geoid_clean'].notna()]\n",
    "\n",
    "for year in df['year'].unique():\n",
    "    year_df = df[df['year'] == year]\n",
    "    merged = tracts.merge(year_df, left_on='GEOID', right_on='tract_geoid_clean')\n",
    "\n",
    "    if merged.empty:\n",
    "        print(f\"️ No valid geometries for {year}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    merged.plot(\n",
    "        column='approval_rate',\n",
    "        ax=ax,\n",
    "        legend=True,\n",
    "        cmap='viridis',\n",
    "        legend_kwds={'label': \"Approval Rate\", 'shrink': 0.5}\n",
    "    )\n",
    "    ax.set_title(f\"Total Mortgage Approval Rates by Tract - {year}\")\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    os.makedirs(\"maps\", exist_ok=True)\n",
    "    plt.savefig(f\"maps/total_approval_rate_{year}.png\", dpi=300)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27056a3f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba93002-a545-466c-b120-802cace018a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" Example GEOID in tracts:\", tracts['GEOID'].head())\n",
    "print(\" Example tract_geoid in data:\", df['tract_geoid'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b7ccc6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe9b231-0535-4093-a267-4ceeeb5efafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"tract_geoid\": list(missing_tracts)}).to_csv(f\"missing_{race}_{year}.csv\", index=False)\n",
    "\n",
    "missing_map = gdf_all_tracts[gdf_all_tracts[\"tract_geoid\"].isin(missing_tracts)]\n",
    "missing_map.to_file(f\"missing_{race}_{year}.shp\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96320f5e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb916c01-ae47-4082-9947-e042668770dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"gdf_all columns:\", gdf_all_tracts.columns)\n",
    "print(\"df_all columns:\", df_all.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b014f3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec173879-48f4-4d97-8677-6dd15179afa7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "gdf_all_tracts = gpd.read_file(\"combined_tracts_2024.gpkg\")\n",
    "\n",
    "gdf_all_tracts[\"GEOID\"] = gdf_all_tracts[\"GEOID\"].astype(str).str.zfill(11)\n",
    "\n",
    "gdf_all_tracts[\"tract_geoid\"] = gdf_all_tracts[\"GEOID\"]\n",
    "\n",
    "all_tracts = set(gdf_all_tracts[\"GEOID\"])\n",
    "\n",
    "print(\"Loaded tracts:\", len(gdf_all_tracts))\n",
    "print(\"Unique GEOIDs:\", len(all_tracts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1352c95",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1e966c-50fb-449a-aa7b-df05437563b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[\"GEOID\"] = df_all[\"GEOID\"].astype(str).str.extract(r'(\\d{11})')[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd72d94b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe79001-625f-452c-8f07-6617b4f921dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cleaned df_all GEOID sample:\", df_all[\"GEOID\"].dropna().unique()[:5])\n",
    "print(\"Matching GEOIDs:\", len(set(df_all[\"GEOID\"]).intersection(set(gdf_all[\"GEOID\"]))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f18800",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83636f6-393b-4f0d-8da5-c49174411148",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = gdf_all.merge(df_all, on=\"GEOID\", how=\"inner\")\n",
    "merged = merged.to_crs(epsg=4326)\n",
    "print(\" Merged rows:\", len(merged))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf244663",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4710f7ff-3ff2-4913-999a-dbcd2608463e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5319b716",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3a5e22-611c-4854-91e2-3c1a434d14d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_folder = \"filtered_data_strict\"\n",
    "years = range(2018, 2025)\n",
    "\n",
    "race_map = {\n",
    "    1.0: 'Indigenous',\n",
    "    2.0: 'Asian', 21: 'Asian', 22: 'Asian', 23: 'Asian', 24: 'Asian',\n",
    "    25: 'Asian', 26: 'Asian', 27: 'Asian',\n",
    "    3.0: 'Black',\n",
    "    4.0: 'Hawaiian/Pacific Islander', 41: 'Hawaiian/Pacific Islander',\n",
    "    42: 'Hawaiian/Pacific Islander', 43: 'Hawaiian/Pacific Islander',\n",
    "    44: 'Hawaiian/Pacific Islander',\n",
    "    5.0: 'White'\n",
    "}\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for year in years:\n",
    "    path = os.path.join(data_folder, f\"{year}_filtered_hmda.csv\")\n",
    "    if not os.path.exists(path):\n",
    "        print(f\" Skipping {year}, file not found.\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "    race_col = 'applicant_race-1' if 'applicant_race-1' in df.columns else 'applicant_race_1'\n",
    "    df['race_label'] = df[race_col].map(race_map)\n",
    "    df = df[df['race_label'].notna()]\n",
    "\n",
    "    df['year'] = year\n",
    "    df['GEOID'] = df['census_tract'].astype(str).str.extract(r'(\\d{11})')[0]\n",
    "    df = df[df['GEOID'].notna()]\n",
    "\n",
    "    all_data.append(df[['GEOID', 'state_code', 'race_label', 'action_taken', 'year']])\n",
    "\n",
    "df_all = pd.concat(all_data, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dda423d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8177465-bb0a-4f38-bd9f-53ce0ab29cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "approved = df_all[df_all['action_taken'].isin([1, 2])]\n",
    "total = df_all[df_all['action_taken'].isin([1, 2, 3])]\n",
    "\n",
    "approved_counts = approved.groupby(['year', 'GEOID', 'race_label']).size().reset_index(name='approved')\n",
    "total_counts = total.groupby(['year', 'GEOID', 'race_label']).size().reset_index(name='total')\n",
    "\n",
    "merged_stats = pd.merge(total_counts, approved_counts, on=['year', 'GEOID', 'race_label'], how='left')\n",
    "merged_stats['approved'] = merged_stats['approved'].fillna(0).astype(int)\n",
    "merged_stats['approval_rate'] = merged_stats['approved'] / merged_stats['total']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc0e92e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce8e55a-67df-49c6-9d33-959bcd5d2294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "gdf_all = gpd.read_file(\"combined_tracts_2024.gpkg\")\n",
    "gdf_all[\"GEOID\"] = gdf_all[\"GEOID\"].astype(str).str.zfill(11)\n",
    "\n",
    "merged = gdf_all.merge(merged_stats, on=\"GEOID\", how=\"inner\")\n",
    "merged = merged.to_crs(epsg=4326)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d39a4de",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72f2434-7ae2-4b74-b35b-d6a1c29ef2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged.columns)\n",
    "print(merged.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2b53f2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d57f45d-aabf-4148-8d17-2481bf0c1160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "os.makedirs(\"svg_maps\", exist_ok=True)\n",
    "\n",
    "for year in sorted(merged['year'].unique()):\n",
    "    for race in sorted(merged['race_label'].unique()):\n",
    "        subset = merged[(merged['year'] == year) & (merged['race_label'] == race)].copy()\n",
    "        if subset.empty:\n",
    "            continue\n",
    "\n",
    "        subset = subset.dropna(subset=[\"approval_rate\"])\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(11, 7))\n",
    "        subset.plot(\n",
    "            column=\"approval_rate\",\n",
    "            cmap=\"viridis\",\n",
    "            linewidth=0,\n",
    "            ax=ax,\n",
    "            legend=True\n",
    "        )\n",
    "        ax.set_title(f\"Approval Rate: {race} - {year}\", fontsize=15)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_xlim([-125, -66])\n",
    "        ax.set_ylim([24, 50])\n",
    "\n",
    "        safe_race = race.replace(\"/\", \"-\")\n",
    "        out_path = f\"svg_maps/{safe_race}_{year}.svg\"\n",
    "        plt.savefig(out_path, format=\"svg\", bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "        print(f\" Saved: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b275d125",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d84522a-ad9e-4097-a268-8a3fa9b19087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "gdf_all = gpd.read_file(\"combined_tracts_2024.gpkg\")\n",
    "gdf_all[\"GEOID\"] = gdf_all[\"GEOID\"].astype(str).str.zfill(11)\n",
    "\n",
    "merged = gdf_all.merge(merged_stats, on=\"GEOID\", how=\"inner\")\n",
    "merged = merged.to_crs(epsg=4326)\n",
    "\n",
    "print(\"Years:\", merged['year'].unique())\n",
    "print(\"Races:\", merged['race_label'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224816be",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aabbf8-1305-44db-a2a3-11df9ef2b23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "os.makedirs(\"svg_maps\", exist_ok=True)\n",
    "\n",
    "print(\"Years:\", merged['year'].unique())\n",
    "print(\"Races:\", merged['race_label'].unique())\n",
    "\n",
    "for year in sorted(merged['year'].unique()):\n",
    "    for race in sorted(merged['race_label'].unique()):\n",
    "        subset = merged[(merged['year'] == year) & (merged['race_label'] == race)].copy()\n",
    "        if subset.empty:\n",
    "            continue\n",
    "\n",
    "        subset = subset.dropna(subset=[\"approval_rate\"])\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(11, 7))\n",
    "        subset.plot(\n",
    "            column=\"approval_rate\",\n",
    "            cmap=\"viridis\",\n",
    "            linewidth=0,\n",
    "            ax=ax,\n",
    "            legend=True\n",
    "        )\n",
    "        ax.set_title(f\"{race} - {year}\", fontsize=15)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        ax.set_xlim([-125, -66])\n",
    "        ax.set_ylim([24, 50])\n",
    "\n",
    "        safe_race = race.replace(\"/\", \"-\").replace(\" \", \"_\")\n",
    "        out_path = f\"svg_maps/{safe_race}_{year}.svg\"\n",
    "        plt.savefig(out_path, format=\"svg\", bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "        print(f\" Saved: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea61cad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b390c37-a8ab-432f-a696-3ec5a0dbc5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "geojson = merged.set_index(\"GEOID\").__geo_interface__\n",
    "\n",
    "years = sorted(merged['year'].unique())\n",
    "races = sorted(merged['race_label'].unique())\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for i, (race, year) in enumerate([(r, y) for r in races for y in years]):\n",
    "    sub = merged[(merged['year'] == year) & (merged['race_label'] == race)]\n",
    "    fig.add_trace(go.Choroplethmapbox(\n",
    "        geojson=geojson,\n",
    "        locations=sub['GEOID'],\n",
    "        z=sub['approval_rate'],\n",
    "        colorscale=\"Viridis\",\n",
    "        zmin=0,\n",
    "        zmax=1,\n",
    "        marker_opacity=0.7,\n",
    "        marker_line_width=0,\n",
    "        name=f\"{race} - {year}\",\n",
    "        visible=(i == 0)  # Only first one visible\n",
    "    ))\n",
    "\n",
    "dropdown_buttons = []\n",
    "for i, (race, year) in enumerate([(r, y) for r in races for y in years]):\n",
    "    visibility = [False] * len(fig.data)\n",
    "    visibility[i] = True\n",
    "    dropdown_buttons.append(dict(\n",
    "        method=\"update\",\n",
    "        label=f\"{race} - {year}\",\n",
    "        args=[{\"visible\": visibility},\n",
    "              {\"title\": f\"Approval Rate: {race} - {year}\"}]\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    mapbox_style=\"carto-positron\",\n",
    "    mapbox_zoom=3,\n",
    "    mapbox_center={\"lat\": 37.8, \"lon\": -96},\n",
    "    height=700,\n",
    "    title=\"Mortgage Approval Rates by Race and Year\",\n",
    "    updatemenus=[{\n",
    "        \"buttons\": dropdown_buttons,\n",
    "        \"direction\": \"down\",\n",
    "        \"showactive\": True,\n",
    "        \"x\": 0.05,\n",
    "        \"xanchor\": \"left\",\n",
    "        \"y\": 1.05,\n",
    "        \"yanchor\": \"top\"\n",
    "    }]\n",
    ")\n",
    "\n",
    "fig.write_html(\"interactive_map_all_in_one.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe580ea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7981349a-8617-4191-ad16-55c43d8f8c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gdf_all['GEOID'].str[:2].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1422af1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de19151b-5f09-43f0-878c-d9a9881546d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_2024 = pd.read_csv(\"unfiltered/2024_hmda.csv\", low_memory=False)\n",
    "\n",
    "df_2024 = df_2024.rename(columns={'applicant_race_1': 'race'})\n",
    "\n",
    "def log_shape(df, label, previous_len):\n",
    "    current_len = len(df)\n",
    "    dropped = previous_len - current_len\n",
    "    print(f\"{label}: {current_len:,} rows (dropped {dropped:,})\")\n",
    "    return current_len\n",
    "\n",
    "step_dfs = []\n",
    "labels = []\n",
    "prev_len = len(df_2024)\n",
    "step_dfs.append(df_2024.copy())\n",
    "labels.append(\"Original\")\n",
    "\n",
    "\n",
    "df = df_2024[df_2024['race'].isin([2, 3, 5])]\n",
    "prev_len = log_shape(df, \"After race filter\", prev_len)\n",
    "step_dfs.append(df.copy())\n",
    "labels.append(\"After race filter\")\n",
    "\n",
    "df = df[df['income'].notna() & (df['income'].astype(float) > 0)]\n",
    "prev_len = log_shape(df, \"After income filter\", prev_len)\n",
    "step_dfs.append(df.copy())\n",
    "labels.append(\"After income filter\")\n",
    "\n",
    "df = df[df['loan_type'] == 1]\n",
    "prev_len = log_shape(df, \"After loan_type == 1\", prev_len)\n",
    "step_dfs.append(df.copy())\n",
    "labels.append(\"After loan_type\")\n",
    "\n",
    "df = df[df['lien_status'] == 1]\n",
    "prev_len = log_shape(df, \"After lien_status == 1\", prev_len)\n",
    "step_dfs.append(df.copy())\n",
    "labels.append(\"After lien_status\")\n",
    "\n",
    "step_dfs.append(df.copy())\n",
    "\n",
    "df = df[df['occupancy_type'] == 1]\n",
    "prev_len = log_shape(df, \"After occupancy_type == 1\", prev_len)\n",
    "step_dfs.append(df.copy())\n",
    "labels.append(\"After occupancy_type\")\n",
    "\n",
    "df = df[df['construction_method'] == 1]\n",
    "prev_len = log_shape(df, \"After construction_method == 1\", prev_len)\n",
    "step_dfs.append(df.copy())\n",
    "labels.append(\"After construction_method\")\n",
    "\n",
    "df['total_units'] = pd.to_numeric(df['total_units'], errors='coerce')\n",
    "df = df[df['total_units'] <= 4]\n",
    "prev_len = log_shape(df, \"After total_units <= 4\", prev_len)\n",
    "step_dfs.append(df.copy())\n",
    "labels.append(\"After total_units\")\n",
    "\n",
    "df = df[df['loan_amount'].notna() & df['property_value'].notna()]\n",
    "prev_len = log_shape(df, \"After loan_amount and property_value present\", prev_len)\n",
    "step_dfs.append(df.copy())\n",
    "labels.append(\"After loan + property_value\")\n",
    "\n",
    "df['debt_to_income_ratio_clean'] = pd.to_numeric(df['debt_to_income_ratio'], errors='coerce')\n",
    "df = df[df['debt_to_income_ratio_clean'].notna()]\n",
    "prev_len = log_shape(df, \"After DTI filter\", prev_len)\n",
    "step_dfs.append(df.copy())\n",
    "labels.append(\"After DTI filter\")\n",
    "\n",
    "for i, step_df in enumerate(step_dfs):\n",
    "    summary = (\n",
    "        step_df.groupby(['state_code', 'race'])\n",
    "        .size()\n",
    "        .reset_index(name='count')\n",
    "        .sort_values(['state_code', 'race'])\n",
    "    )\n",
    "    summary.to_csv(f\"debug_breakdown_step_{i+1:02d}_{labels[i].replace(' ', '_').lower()}.csv\", index=False)\n",
    "\n",
    "print(\" All filter steps completed. CSVs saved with race-state breakdown.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7ba684",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba9344e-8848-4d8a-8b32-1692e2310b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def log_shape(df, label, previous_count=None):\n",
    "    current_count = len(df)\n",
    "    if previous_count is not None:\n",
    "        print(f\"{label}: {current_count:,} rows (dropped {previous_count - current_count:,})\")\n",
    "    else:\n",
    "        print(f\"{label}: {current_count:,} rows\")\n",
    "    return current_count\n",
    "\n",
    "df = pd.read_csv('unfiltered/2024_hmda.csv', low_memory=False)\n",
    "df = df.rename(columns={'applicant_race_1': 'race'})  # unify column name\n",
    "\n",
    "step = 1\n",
    "prev_count = log_shape(df, f\"Step {step}: Original\")\n",
    "\n",
    "df = df[df['race'].isin([2, 3, 5])]\n",
    "step += 1\n",
    "prev_count = log_shape(df, f\"Step {step}: After race filter\", prev_count)\n",
    "\n",
    "df['income'] = pd.to_numeric(df['income'], errors='coerce')\n",
    "df['loan_amount'] = pd.to_numeric(df['loan_amount'], errors='coerce')\n",
    "df['property_value'] = pd.to_numeric(df['property_value'], errors='coerce')\n",
    "df['total_units'] = pd.to_numeric(df['total_units'], errors='coerce')\n",
    "df['debt_to_income_ratio_clean'] = pd.to_numeric(df['debt_to_income_ratio'], errors='coerce')\n",
    "\n",
    "df = df[df['income'].notna() & (df['income'] > 0)]\n",
    "step += 1\n",
    "prev_count = log_shape(df, f\"Step {step}: After income filter\", prev_count)\n",
    "\n",
    "df = df[df['loan_type'] == 1]\n",
    "step += 1\n",
    "prev_count = log_shape(df, f\"Step {step}: After loan_type == 1\", prev_count)\n",
    "\n",
    "df = df[df['lien_status'] == 1]\n",
    "step += 1\n",
    "prev_count = log_shape(df, f\"Step {step}: After lien_status == 1\", prev_count)\n",
    "\n",
    "step += 1\n",
    "\n",
    "df = df[df['occupancy_type'] == 1]\n",
    "step += 1\n",
    "prev_count = log_shape(df, f\"Step {step}: After occupancy_type == 1\", prev_count)\n",
    "\n",
    "df = df[df['construction_method'] == 1]\n",
    "step += 1\n",
    "prev_count = log_shape(df, f\"Step {step}: After construction_method == 1\", prev_count)\n",
    "\n",
    "df = df[df['total_units'].notna() & (df['total_units'] <= 4)]\n",
    "step += 1\n",
    "prev_count = log_shape(df, f\"Step {step}: After total_units <= 4\", prev_count)\n",
    "\n",
    "df = df[df['loan_amount'].notna() & df['property_value'].notna()]\n",
    "step += 1\n",
    "prev_count = log_shape(df, f\"Step {step}: After loan/property present\", prev_count)\n",
    "\n",
    "df = df[df['debt_to_income_ratio_clean'].notna()]\n",
    "step += 1\n",
    "prev_count = log_shape(df, f\"Step {step}: After DTI filter\", prev_count)\n",
    "\n",
    "df.to_csv('2024_filtered_final.csv', index=False)\n",
    "print(\" Final filtered file saved as '2024_filtered_final.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c6fdea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a534a616-2447-4a5e-a251-b4f2b8590e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"2024_filtered_final.csv\")\n",
    "\n",
    "df['race'] = pd.to_numeric(df['race'], errors='coerce')\n",
    "\n",
    "race_map = {2: 'Asian', 3: 'Black', 5: 'White'}\n",
    "expected_races = sorted(race_map.keys())\n",
    "\n",
    "df = df[df['race'].isin(expected_races)]\n",
    "\n",
    "race_counts = (\n",
    "    df.groupby(['state_code', 'race'])\n",
    "    .size()\n",
    "    .reset_index(name='count')\n",
    ")\n",
    "\n",
    "race_pivot = race_counts.pivot(index='state_code', columns='race', values='count').fillna(0).astype(int)\n",
    "race_pivot.columns = [race_map[r] for r in race_pivot.columns]\n",
    "race_pivot = race_pivot.sort_index()\n",
    "\n",
    "def find_missing_races(row):\n",
    "    return [race for race in race_map.values() if race not in row.index or row[race] == 0]\n",
    "\n",
    "race_pivot['missing_races'] = race_pivot.apply(find_missing_races, axis=1)\n",
    "\n",
    "missing_states = race_pivot[race_pivot['missing_races'].map(len) > 0]\n",
    "\n",
    "race_pivot.to_csv(\"2024_race_presence_by_state.csv\")\n",
    "missing_states.to_csv(\"2024_missing_race_by_state.csv\")\n",
    "\n",
    "print(\" Done. Missing races saved to '2024_missing_race_by_state.csv'\")\n",
    "print(missing_states[['missing_races']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7304f5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8fca74-8e35-4568-8187-2d7b20f725f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "input_folder = \"unfiltered\"\n",
    "output_folder = \"race_debug_outputs\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "race_map = {\n",
    "    1: \"Indigenous\",\n",
    "    2: \"Asian Broad\",\n",
    "    3: \"Black\",\n",
    "    4: \"Hawaiian Broad\",\n",
    "    5: \"White\"\n",
    "}\n",
    "race_codes = list(race_map.keys())\n",
    "\n",
    "def log_shape(df, label, logs):\n",
    "    logs.append((label, len(df)))\n",
    "    return df\n",
    "\n",
    "def filter_and_check_races(file_path, year):\n",
    "    print(f\"\\n Processing {year}...\")\n",
    "\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "    logs = []\n",
    "\n",
    "    race_col = \"applicant_race_1\" if year == 2024 else \"applicant_race-1\"\n",
    "    if race_col not in df.columns:\n",
    "        print(f\"️ Skipping {year} – race column '{race_col}' not found.\")\n",
    "        return\n",
    "\n",
    "    df = df.rename(columns={race_col: \"race\"})\n",
    "    df[\"race\"] = pd.to_numeric(df[\"race\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    df = log_shape(df, \"Original\", logs)\n",
    "    df = df[df[\"race\"].isin(race_codes)]\n",
    "    df = log_shape(df, \"After race filter\", logs)\n",
    "\n",
    "    df = df[df[\"income\"].notna()]\n",
    "    df[\"income\"] = pd.to_numeric(df[\"income\"], errors=\"coerce\")\n",
    "    df = df[df[\"income\"] > 0]\n",
    "    df = log_shape(df, \"After income filter\", logs)\n",
    "\n",
    "    filters = [\n",
    "        (\"loan_type\", 1),\n",
    "        (\"lien_status\", 1),\n",
    "        (\"occupancy_type\", 1),\n",
    "        (\"construction_method\", 1)\n",
    "    ]\n",
    "    for col, val in filters:\n",
    "        df = df[df[col] == val]\n",
    "        df = log_shape(df, f\"After {col} == {val}\", logs)\n",
    "\n",
    "    df[\"total_units\"] = pd.to_numeric(df[\"total_units\"], errors=\"coerce\")\n",
    "    df = df[df[\"total_units\"] <= 4]\n",
    "    df = log_shape(df, \"After total_units <= 4\", logs)\n",
    "\n",
    "    df = df[df[\"loan_amount\"].notna() & df[\"property_value\"].notna()]\n",
    "    df = log_shape(df, \"After loan/property present\", logs)\n",
    "\n",
    "    df[\"debt_to_income_ratio_clean\"] = pd.to_numeric(df[\"debt_to_income_ratio\"], errors=\"coerce\")\n",
    "    df = df[df[\"debt_to_income_ratio_clean\"].notna()]\n",
    "    df = log_shape(df, \"After DTI filter\", logs)\n",
    "\n",
    "    race_counts = df.groupby([\"state_code\", \"race\"]).size().reset_index(name=\"count\")\n",
    "    race_pivot = race_counts.pivot(index=\"state_code\", columns=\"race\", values=\"count\").fillna(0).astype(int)\n",
    "    race_pivot.columns = [race_map.get(r, f\"Race_{r}\") for r in race_pivot.columns]\n",
    "\n",
    "    def find_missing_races(row):\n",
    "        return [r for r in race_map.values() if r not in row.index or row[r] == 0]\n",
    "\n",
    "    race_pivot[\"missing_races\"] = race_pivot.apply(find_missing_races, axis=1)\n",
    "    missing_states = race_pivot[race_pivot[\"missing_races\"].map(len) > 0]\n",
    "\n",
    "    race_pivot.to_csv(f\"{output_folder}/{year}_race_presence_by_state.csv\")\n",
    "    missing_states.to_csv(f\"{output_folder}/{year}_missing_race_by_state.csv\")\n",
    "\n",
    "    log_df = pd.DataFrame(logs, columns=[\"step\", \"row_count\"])\n",
    "    log_df.to_csv(f\"{output_folder}/{year}_filter_log.csv\", index=False)\n",
    "\n",
    "    print(f\" {year} done. Missing races in {len(missing_states)} state(s).\")\n",
    "    return missing_states\n",
    "\n",
    "for year in range(2018, 2025):\n",
    "    file_path = os.path.join(input_folder, f\"{year}_hmda.csv\")\n",
    "    if os.path.exists(file_path):\n",
    "        filter_and_check_races(file_path, year)\n",
    "    else:\n",
    "        print(f\" Missing file: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df42c5dd",
   "metadata": {},
   "source": [
    "Load starting from the correct row (skip header metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67588ef5-b273-4189-ac11-6465e7f6458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "folder = 'race_debug_outputs'  # change if needed\n",
    "years = list(range(2018, 2024))\n",
    "dfs = []\n",
    "\n",
    "for year in years:\n",
    "    path = os.path.join(folder, f\"{year}_race_presence_by_state.csv\")\n",
    "    try:\n",
    "        raw = pd.read_csv(path, skiprows=2)\n",
    "        raw.columns = ['state', 'race'] + [f\"{year}_{i:02d}\" for i in range(len(raw.columns) - 2)]\n",
    "\n",
    "        melted = raw.melt(id_vars=['state', 'race'], var_name='step', value_name='presence')\n",
    "        melted['year'] = year\n",
    "        melted = melted[melted['presence'].notna()]\n",
    "        melted['count'] = 1  # presence means count=1 for pivot later\n",
    "        melted = melted[['state', 'race', 'step', 'year', 'count']]\n",
    "        dfs.append(melted)\n",
    "\n",
    "        print(f\" Parsed {year}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Failed for {year}: {e}\")\n",
    "\n",
    "raw = pd.read_csv(os.path.join(folder, \"2024_race_presence_by_state.csv\"), skiprows=2)\n",
    "raw.columns = ['state', 'race'] + [f\"2024_{i:02d}\" for i in range(len(raw.columns) - 2)]\n",
    "\n",
    "melted = raw.melt(id_vars=['state', 'race'], var_name='step', value_name='presence')\n",
    "melted['year'] = 2024\n",
    "melted = melted[melted['presence'].notna()]\n",
    "melted['count'] = 1\n",
    "df_2024 = melted[['state', 'race', 'step', 'year', 'count']]\n",
    "\n",
    "df_2024 = df_2024.rename(columns={'state_code': 'state'})  # standardize\n",
    "df_2024 = df_2024[['state', 'race', 'step', 'year', 'count']]\n",
    "\n",
    "combined = pd.concat(dfs + [df_2024], ignore_index=True)\n",
    "combined.to_csv('combined_race_presence_by_state_and_year.csv', index=False)\n",
    "print(\" Final combined file saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab4009c",
   "metadata": {},
   "source": [
    "Load the combined file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0d250d-ad65-4690-8032-26790d7a345a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"combined_race_presence_by_state_and_year.csv\")\n",
    "\n",
    "df['count'] = pd.to_numeric(df['count'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "df = df[['state', 'race', 'year', 'step', 'count']].copy()\n",
    "df = df.rename(columns={'count': 'present'})\n",
    "\n",
    "race_map = {2.0: \"Asian\", 3.0: \"Black\", 5.0: \"White\"}\n",
    "df['race_label'] = df['race'].map(race_map)\n",
    "\n",
    "df.to_csv(\"race_presence_matrix_by_step.csv\", index=False)\n",
    "print(\" Saved: race_presence_matrix_by_step.csv\")\n",
    "\n",
    "df[df['present'] == 0].to_csv(\"race_step_missing_only.csv\", index=False)\n",
    "print(\" Saved: race_step_missing_only.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3d5e95-67cf-434c-9e84-841ad9659f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"race_presence_matrix_by_step.csv\")\n",
    "\n",
    "step_cols = df.columns[3:]\n",
    "\n",
    "def get_drop_step(row):\n",
    "    for step in step_cols:\n",
    "        if row[step] == 0:\n",
    "            return step\n",
    "    return None  # No drop\n",
    "\n",
    "df['drop_step'] = df.apply(get_drop_step, axis=1)\n",
    "df['dropped'] = df['drop_step'].notna()\n",
    "\n",
    "race_map = {2.0: 'Asian', 3.0: 'Black', 5.0: 'White'}\n",
    "df['race_label'] = df['race'].map(race_map)\n",
    "\n",
    "dropped = df[df['dropped']].copy()\n",
    "\n",
    "dropped.to_csv(\"detected_race_drops_by_state_and_year.csv\", index=False)\n",
    "print(\" Saved: detected_race_drops_by_state_and_year.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca5e1bd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7d2d8f-6ebb-4556-a514-004327b4630e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_folder = \"filtered_data_strict\"\n",
    "years = range(2018, 2025)\n",
    "\n",
    "race_map = {\n",
    "    2.0: 'Asian', 21: 'Asian', 22: 'Asian', 23: 'Asian', 24: 'Asian', 25: 'Asian', 26: 'Asian', 27: 'Asian',\n",
    "    3.0: 'Black',\n",
    "    5.0: 'White',\n",
    "    1.0: 'Indigenous',\n",
    "    4.0: 'Hawaiian', 41.0: 'Hawaiian', 42.0: 'Hawaiian', 43.0: 'Hawaiian', 44.0: 'Hawaiian'\n",
    "}\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for year in years:\n",
    "    file_path = os.path.join(data_folder, f\"{year}_filtered_hmda.csv\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\" Skipping {year}, file not found.\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "    race_col = 'applicant_race-1' if 'applicant_race-1' in df.columns else 'applicant_race_1'\n",
    "\n",
    "    df[race_col] = pd.to_numeric(df[race_col], errors='coerce')\n",
    "    df['race_label'] = df[race_col].map(race_map)\n",
    "\n",
    "    df = df[df['race_label'].notna()]\n",
    "    df['year'] = year\n",
    "\n",
    "    all_data.append(df[['state_code', 'race_label', 'action_taken', 'year']])\n",
    "\n",
    "df_all = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "approved = df_all[df_all['action_taken'].isin([1, 2])]  # approved or originated\n",
    "total = df_all[df_all['action_taken'].isin([1, 2, 3])]   # includes denials\n",
    "\n",
    "approved_counts = (\n",
    "    approved.groupby(['year', 'state_code', 'race_label'])\n",
    "    .size()\n",
    "    .reset_index(name='approved')\n",
    ")\n",
    "total_counts = (\n",
    "    total.groupby(['year', 'state_code', 'race_label'])\n",
    "    .size()\n",
    "    .reset_index(name='total')\n",
    ")\n",
    "\n",
    "merged = pd.merge(total_counts, approved_counts, how='left', on=['year', 'state_code', 'race_label'])\n",
    "merged['approved'] = merged['approved'].fillna(0).astype(int)\n",
    "merged['approval_rate'] = (merged['approved'] / merged['total']).round(3)\n",
    "\n",
    "merged.to_csv(\"race_approval_rates_by_state_year.csv\", index=False)\n",
    "print(\" Saved: race_approval_rates_by_state_year.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e37f547",
   "metadata": {},
   "source": [
    "Load approval rate data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bfb22b-bf1e-4df4-afe1-8b948a3862f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"race_approval_rates_by_state_year.csv\")\n",
    "\n",
    "if \"race_label\" not in df.columns:\n",
    "    race_columns = [\n",
    "        \"white_approval\",\n",
    "        \"black_approval\",\n",
    "        \"asian_approval\",\n",
    "        \"indigenous_approval\",\n",
    "        \"hawaiian_approval\"\n",
    "    ]\n",
    "    race_columns = [col for col in race_columns if col in df.columns]\n",
    "\n",
    "    df = pd.melt(\n",
    "        df,\n",
    "        id_vars=[\"state_code\", \"year\"],\n",
    "        value_vars=race_columns,\n",
    "        var_name=\"race_label\",\n",
    "        value_name=\"approval_rate\"\n",
    "    )\n",
    "    df[\"race_label\"] = df[\"race_label\"].str.replace(\"_approval\", \"\").str.title()\n",
    "\n",
    "if \"state_code\" in df.columns:\n",
    "    df = df.rename(columns={\"state_code\": \"state\"})\n",
    "\n",
    "df[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\", downcast=\"integer\")\n",
    "df[\"approval_rate\"] = pd.to_numeric(df[\"approval_rate\"], errors=\"coerce\")\n",
    "\n",
    "print(\"Available races:\", df[\"race_label\"].unique())\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "g = sns.FacetGrid(\n",
    "    df,\n",
    "    col=\"state\",\n",
    "    col_wrap=4,\n",
    "    height=3.5,\n",
    "    sharey=False\n",
    ")\n",
    "g.map_dataframe(\n",
    "    sns.lineplot,\n",
    "    x=\"year\",\n",
    "    y=\"approval_rate\",\n",
    "    hue=\"race_label\",\n",
    "    marker=\"o\"\n",
    ")\n",
    "\n",
    "g.add_legend(title=\"Race\")\n",
    "g.set_titles(col_template=\"{col_name}\")\n",
    "g.set_axis_labels(\"Year\", \"Approval Rate\")\n",
    "g.fig.subplots_adjust(top=0.92)\n",
    "g.fig.suptitle(\"Mortgage Approval Rates by Race and State (2018–2024)\", fontsize=16)\n",
    "\n",
    "g.savefig(\"approval_rates_by_state_race_all.svg\", format=\"svg\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3749cbe",
   "metadata": {},
   "source": [
    "Check why VI is missing soem races\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3ee412-4573-4586-b677-44eea74b33c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vi = df[df['state'] == 'VI']\n",
    "df_vi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8953acc2-7fe3-42ce-89a2-0df4e5ff4f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped = pd.read_csv(\"filtered_data_strict/2024_dropped_log.csv\")\n",
    "dropped['state_code'] = dropped['state_code'].astype(str).str.zfill(2)\n",
    "print(dropped[dropped['state_code'] == '78']['applicant_race_1'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506da1c8",
   "metadata": {},
   "source": [
    "Fix common column name issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa5c847-d397-43b4-a5cd-607ff50681b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "race_presence_files = glob.glob(\"*_race_presence_by_state.csv\")\n",
    "combined_rows = []\n",
    "\n",
    "for file in race_presence_files:\n",
    "    year = file[:4]\n",
    "    print(f\" Processing {file}\")\n",
    "    \n",
    "    df = pd.read_csv(file)\n",
    "    print(\" Columns:\", df.columns.tolist())\n",
    "\n",
    "    df.columns = [col.strip().lower().replace(' ', '_') for col in df.columns]\n",
    "    \n",
    "    race_col = None\n",
    "    state_col = None\n",
    "\n",
    "    for col in df.columns:\n",
    "        if 'race' in col and 'code' not in col:\n",
    "            race_col = col\n",
    "        if 'state' in col:\n",
    "            state_col = col\n",
    "    \n",
    "    if not race_col or not state_col:\n",
    "        print(f\"️ Skipping {file} (missing race or state column)\")\n",
    "        continue\n",
    "\n",
    "    df = df[[state_col, race_col]].copy()\n",
    "    df.columns = ['state_code', 'race']\n",
    "    df['year'] = int(year)\n",
    "    combined_rows.append(df)\n",
    "\n",
    "if not combined_rows:\n",
    "    raise ValueError(\" No valid files processed. Check column names.\")\n",
    "    \n",
    "all_years_df = pd.concat(combined_rows, ignore_index=True)\n",
    "\n",
    "race_map = {2.0: 'Asian', 3.0: 'Black', 5.0: 'White'}\n",
    "all_years_df['race_label'] = all_years_df['race'].map(race_map)\n",
    "\n",
    "race_state_counts = (\n",
    "    all_years_df\n",
    "    .groupby(['year', 'race_label'])['state_code']\n",
    "    .nunique()\n",
    "    .reset_index(name='num_states')\n",
    ")\n",
    "df_2024 = pd.read_csv(\"2024_race_presence_by_state.csv\")\n",
    "df_2024 = df_2024[['state_code', 'Asian', 'Black', 'White']]\n",
    "df_2024 = df_2024.melt(id_vars='state_code', var_name='race_label', value_name='count')\n",
    "df_2024 = df_2024[df_2024['count'] > 0]\n",
    "df_2024['year'] = 2024\n",
    "\n",
    "race_2024_counts = (\n",
    "    df_2024.groupby('race_label')['state_code']\n",
    "    .nunique()\n",
    "    .reset_index(name='num_states')\n",
    ")\n",
    "race_2024_counts['year'] = 2024\n",
    "\n",
    "final_counts = pd.concat([race_state_counts, race_2024_counts], ignore_index=True)\n",
    "final_pivot = final_counts.pivot(index='year', columns='race_label', values='num_states').reset_index()\n",
    "\n",
    "final_pivot.to_csv(\"combined_race_state_counts_by_year.csv\", index=False)\n",
    "print(\" Saved as 'combined_race_state_counts_by_year.csv'\")\n",
    "print(final_pivot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9588ed",
   "metadata": {},
   "source": [
    "Load all filtered files \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3794d248-5be5-4b68-8d2e-a56b12c4be50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "filtered_folder = \"filtered_data_strict\"\n",
    "years = list(range(2018, 2025))\n",
    "dfs = []\n",
    "\n",
    "for year in years:\n",
    "    path = os.path.join(filtered_folder, f\"{year}_filtered_hmda.csv\")\n",
    "    if not os.path.exists(path):\n",
    "        print(f\" Skipping missing file for {year}\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "    race_col = 'applicant_race_1' if 'applicant_race_1' in df.columns else 'applicant_race-1'\n",
    "    \n",
    "    df['race'] = df[race_col]\n",
    "    df['year'] = year\n",
    "    dfs.append(df[['year', 'lei', 'state_code', 'race', 'action_taken']])\n",
    "\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df_all['approved'] = df_all['action_taken'] == 1\n",
    "\n",
    "agg = df_all.groupby(['year', 'lei', 'state_code', 'race']).agg(\n",
    "    total_applications=('action_taken', 'count'),\n",
    "    approved_applications=('approved', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "agg.to_csv(\"aggregated_lei_race_state.csv\", index=False)\n",
    "print(\" Saved: aggregated_lei_race_state.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccef5f4",
   "metadata": {},
   "source": [
    "Load the aggregated file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82bc3d8-8d25-4521-871c-663baf267f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"aggregated_lei_race_state.csv\")\n",
    "\n",
    "race_map = {\n",
    "    2.0: 'Asian', 21: 'Asian', 22: 'Asian', 23: 'Asian', 24: 'Asian',\n",
    "    25: 'Asian', 26: 'Asian', 27: 'Asian',\n",
    "    3.0: 'Black',\n",
    "    5.0: 'White'\n",
    "}\n",
    "df['race_label'] = df['race'].map(race_map)\n",
    "\n",
    "df['approval_rate'] = df['approved_applications'] / df['total_applications']\n",
    "\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Missing values per column:\\n\", df.isnull().sum())\n",
    "\n",
    "print(\"\\nUnique states:\", sorted(df['state_code'].dropna().unique()))\n",
    "print(\"\\nUnique race labels:\", df['race_label'].unique())\n",
    "print(\"\\nApproval rate min/max:\", df['approval_rate'].min(), df['approval_rate'].max())\n",
    "\n",
    "df = df[df['approval_rate'].between(0, 1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fc2b71",
   "metadata": {},
   "source": [
    "code cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec96f636-b72f-4b57-a1ad-a3f6404ff759",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'year': 'activity_year'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e12f5c",
   "metadata": {},
   "source": [
    "Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e58487-765c-4136-8bff-70b351cf4205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"race_approval_rates_by_state_year.csv\")\n",
    "\n",
    "excluded_states = ['PR', 'VI', 'GU', 'MP', 'AS', 'UM']\n",
    "df = df[~df['state_code'].isin(excluded_states)]\n",
    "\n",
    "df['race_label'] = df['race_label'].str.strip().str.title()\n",
    "\n",
    "df = df[df['year'].between(2020, 2024)]\n",
    "\n",
    "all_main_races = ['White', 'Black', 'Asian', 'Indigenous', 'Hawaiian']\n",
    "present_races = [r for r in all_main_races if r in df['race_label'].unique()]\n",
    "print(\" Races found in data:\", present_races)\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "g = sns.FacetGrid(\n",
    "    df,\n",
    "    col='state_code',\n",
    "    col_wrap=6,\n",
    "    height=3.5,\n",
    "    aspect=1.3,\n",
    "    sharey=True\n",
    ")\n",
    "\n",
    "def custom_lineplot(data, **kwargs):\n",
    "    sns.lineplot(\n",
    "        data=data,\n",
    "        x='year', \n",
    "        y='approval_rate',\n",
    "        hue='race_label',\n",
    "        hue_order=present_races,\n",
    "        marker='o',\n",
    "        ci=None,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "g.map_dataframe(custom_lineplot)\n",
    "\n",
    "g.set_axis_labels(\"Year\", \"Approval Rate\")\n",
    "g.set_titles(\"{col_name}\")\n",
    "g.set(ylim=(0.4, 1.0))\n",
    "g.add_legend(title='Race')\n",
    "\n",
    "for ax in g.axes.flatten():\n",
    "    ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "\n",
    "plt.subplots_adjust(top=1.0)\n",
    "g.fig.suptitle(\n",
    "    \"Mortgage Approval Rates by Race (2020–2024)\",\n",
    "    fontsize=16,\n",
    "    fontweight='bold'\n",
    ")\n",
    "\n",
    "g.savefig(\"approval_rates_by_state_all_races_2020_2024.png\", format='png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03fbc5c",
   "metadata": {},
   "source": [
    "Debugging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dc006c-32c3-4a2d-9f8d-4f678594ef7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89d27462",
   "metadata": {},
   "source": [
    "Set up ===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430b01f1-67ff-40a9-93ea-8b771da6cac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "unfiltered_folder = \"unfiltered\"\n",
    "years = list(range(2018, 2025))\n",
    "race_colnames = {2024: \"applicant_race_1\"}  # Adjust as needed\n",
    "default_race_col = \"applicant_race-1\"\n",
    "\n",
    "main_races = {\n",
    "    1.0: \"American Indian or Alaska Native\",   # Indigenous\n",
    "    2.0: \"Asian\",\n",
    "    3.0: \"Black or African American\",\n",
    "    4.0: \"Native Hawaiian or Pacific Islander\",\n",
    "    5.0: \"White\"\n",
    "}\n",
    "\n",
    "states_to_check = [\n",
    "    'AK', 'HI', 'ID', 'MT', 'ND', 'NH', 'RI', 'SD', 'VT', 'WV', 'WY'\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for year in years:\n",
    "    file_path = os.path.join(unfiltered_folder, f\"{year}_hmda.csv\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"️ Missing file: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    print(f\" Checking {year}...\")\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "    race_col = race_colnames.get(year, default_race_col)\n",
    "    if race_col not in df.columns or 'state_code' not in df.columns:\n",
    "        print(f\" Required columns missing in {year}\")\n",
    "        continue\n",
    "\n",
    "    for state in states_to_check:\n",
    "        for race_code, race_label in main_races.items():\n",
    "            count = df[(df['state_code'] == state) & (df[race_col] == race_code)].shape[0]\n",
    "            results.append({\n",
    "                'year': year,\n",
    "                'state_code': state,\n",
    "                'race_code': race_code,\n",
    "                'race_label': race_label,\n",
    "                'count': count\n",
    "            })\n",
    "\n",
    "df_check = pd.DataFrame(results)\n",
    "df_check = df_check.sort_values(by=['state_code', 'race_code', 'year'])\n",
    "\n",
    "df_check.to_csv(\"checked_main_race_state_counts.csv\", index=False)\n",
    "print(\" Saved check results to 'checked_main_race_state_counts.csv'\")\n",
    "display(df_check)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6f5a2f",
   "metadata": {},
   "source": [
    "Set file path and race codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471b684f-1301-499f-bf54-bd3b0a01e5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "year = 2024\n",
    "file_path = f'unfiltered/{year}_hmda.csv'\n",
    "race_col = 'applicant_race_1'  # Correct column name for 2024\n",
    "target_states = ['HI', 'ID', 'NH', 'AK', 'ND', 'WY', 'MT', 'VT', 'SD', 'RI', 'WV']\n",
    "target_races = {\n",
    "    'Black or African American': 3,\n",
    "    'Asian': 2\n",
    "}\n",
    "\n",
    "df = pd.read_csv(file_path, low_memory=False)\n",
    "required_columns = ['state_code', race_col]\n",
    "if not all(col in df.columns for col in required_columns):\n",
    "    print(f\" Required columns missing in {year}\")\n",
    "else:\n",
    "    df = df[df['state_code'].isin(target_states)].copy()\n",
    "    df = df[[race_col, 'state_code']].dropna()\n",
    "    df[race_col] = pd.to_numeric(df[race_col], errors='coerce')\n",
    "\n",
    "    results = []\n",
    "    for race_label, race_code in target_races.items():\n",
    "        for state in target_states:\n",
    "            count = df[\n",
    "                (df[race_col] == race_code) &\n",
    "                (df['state_code'] == state)\n",
    "            ].shape[0]\n",
    "            results.append({\n",
    "                'year': year,\n",
    "                'state_code': state,\n",
    "                'race_label': race_label,\n",
    "                'count': count\n",
    "            })\n",
    "\n",
    "    df_out = pd.DataFrame(results)\n",
    "    df_out.to_csv('checked_2024_missing_race_state_counts.csv', index=False)\n",
    "    print(\" Saved 2024 check to 'checked_2024_missing_race_state_counts.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6652c8b9",
   "metadata": {},
   "source": [
    "Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b31d75c-e386-407d-8475-bb5daccedff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "df = pd.read_csv(\"race_action_breakdown_by_year_state_lei.csv\")\n",
    "\n",
    "grouped = (\n",
    "    df.groupby([\"activity_year\", \"Race\", \"Decision\"])[\"count\"]\n",
    "    .sum()\n",
    "    .unstack(fill_value=0)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "grouped[\"total\"] = grouped[\"Approved\"] + grouped[\"Denied\"]\n",
    "grouped[\"approval_rate\"] = grouped[\"Approved\"] / grouped[\"total\"]\n",
    "\n",
    "main_races = [\n",
    "    \"White\",\n",
    "    \"Black or African American\",\n",
    "    \"Asian\",\n",
    "    \"American Indian or Alaska Native\",\n",
    "    \"Native Hawaiian or Pacific Islander\"\n",
    "]\n",
    "grouped = grouped[grouped[\"Race\"].isin(main_races)]\n",
    "\n",
    "grouped[\"Period\"] = grouped[\"activity_year\"].apply(lambda x: \"Pre-2020\" if x <= 2020 else \"Post-2020\")\n",
    "\n",
    "period_summary = (\n",
    "    grouped.groupby([\"Race\", \"Period\"])[[\"Approved\", \"Denied\"]]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "period_summary[\"total\"] = period_summary[\"Approved\"] + period_summary[\"Denied\"]\n",
    "period_summary[\"approval_rate\"] = period_summary[\"Approved\"] / period_summary[\"total\"]\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    data=period_summary,\n",
    "    x=\"Race\",\n",
    "    y=\"approval_rate\",\n",
    "    hue=\"Period\"\n",
    ")\n",
    "plt.title(\"Pre- vs Post-2020 Mortgage Approval Rates by Race (National)\")\n",
    "plt.ylabel(\"Approval Rate\")\n",
    "plt.xlabel(\"Race\")\n",
    "plt.xticks(rotation=30)\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"approval_rates_pre_post_2020_by_race.svg\", format=\"svg\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b02108-a07c-4cdf-b9cc-2c262e14baf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "df = pd.read_csv(\"race_action_breakdown_by_year_state_lei.csv\")\n",
    "\n",
    "core_races = [\"White\", \"Black or African American\", \"Asian\", \"American Indian or Alaska Native\", \"Native Hawaiian or Pacific Islander\"]\n",
    "df = df[df[\"Race\"].isin(core_races)]\n",
    "\n",
    "df = df[df[\"activity_year\"].between(2018, 2024)]\n",
    "df[\"Period\"] = df[\"activity_year\"].apply(lambda x: \"Pre-2020\" if x <= 2020 else \"Post-2020\")\n",
    "\n",
    "pivot = df.pivot_table(\n",
    "    index=[\"lei\", \"Race\", \"Period\"],\n",
    "    columns=\"Decision\",\n",
    "    values=\"count\",\n",
    "    aggfunc=\"sum\",\n",
    "    fill_value=0\n",
    ").reset_index()\n",
    "\n",
    "pivot[\"total\"] = pivot[\"Approved\"] + pivot[\"Denied\"]\n",
    "pivot[\"approval_rate\"] = pivot[\"Approved\"] / pivot[\"total\"]\n",
    "\n",
    "pivot_wide = pivot.pivot_table(\n",
    "    index=[\"lei\", \"Period\"],\n",
    "    columns=\"Race\",\n",
    "    values=\"approval_rate\"\n",
    ").reset_index()\n",
    "\n",
    "for race in [\"Black or African American\", \"Asian\", \"American Indian or Alaska Native\", \"Native Hawaiian or Pacific Islander\"]:\n",
    "    pivot_wide[f\"{race}_gap\"] = pivot_wide[race] - pivot_wide[\"White\"]\n",
    "\n",
    "gap_cols = [c for c in pivot_wide.columns if \"_gap\" in c]\n",
    "summary = pivot_wide.groupby(\"lei\")[gap_cols].mean().reset_index()\n",
    "\n",
    "plot_df = pivot_wide.copy()\n",
    "plot_df = plot_df[[\"lei\", \"Period\", \"Black or African American_gap\"]].dropna()\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=plot_df, x=\"Period\", y=\"Black or African American_gap\")\n",
    "plt.axhline(0, color='gray', linestyle='--')\n",
    "plt.ylabel(\"Approval Gap (Black - White)\")\n",
    "plt.title(\"Change in Approval Gap: Black vs. White by Bank (LEI)\")\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"black_white_gap_pre_post_2020.svg\", format=\"svg\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbf9fc6-62e1-4a7f-9525-e27c5715184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"race_action_breakdown_by_year_state_lei.csv\")\n",
    "lei_dict = pd.read_csv(\"lei_info/bloomberg_scraped_structured.csv\")\n",
    "lei_dict = lei_dict.rename(columns={\n",
    "    \"lei_number\": \"lei\",\n",
    "    \"Legal Name\": \"bank_name\"\n",
    "}).drop_duplicates(\"lei\")\n",
    "\n",
    "df = df[df[\"Race\"].isin([\"White\", \"Black or African American\"])]\n",
    "df = df[df[\"activity_year\"].between(2018, 2024)]\n",
    "\n",
    "df[\"Period\"] = df[\"activity_year\"].apply(lambda x: \"Pre-2020\" if x <= 2020 else \"Post-2020\")\n",
    "df[\"is_approved\"] = df[\"Decision\"] == \"Approved\"\n",
    "\n",
    "agg = df.groupby([\"lei\", \"Race\", \"Period\"]).agg(\n",
    "    approved=(\"is_approved\", \"sum\"),\n",
    "    total=(\"is_approved\", \"count\")\n",
    ").reset_index()\n",
    "agg[\"approval_rate\"] = agg[\"approved\"] / agg[\"total\"]\n",
    "\n",
    "wide = agg.pivot(index=\"lei\", columns=[\"Race\", \"Period\"], values=\"approval_rate\")\n",
    "wide.columns = [f\"{race}_{period}\" for race, period in wide.columns]\n",
    "wide = wide.reset_index()\n",
    "\n",
    "wide[\"white_rate_change\"] = wide[\"White_Post-2020\"] - wide[\"White_Pre-2020\"]\n",
    "wide[\"black_rate_change\"] = wide[\"Black or African American_Post-2020\"] - wide[\"Black or African American_Pre-2020\"]\n",
    "wide[\"gap_change\"] = (wide[\"Black or African American_Post-2020\"] - wide[\"White_Post-2020\"]) - \\\n",
    "                     (wide[\"Black or African American_Pre-2020\"] - wide[\"White_Pre-2020\"])\n",
    "\n",
    "wide = wide.merge(lei_dict, on=\"lei\", how=\"left\")\n",
    "\n",
    "wide_sorted = wide.sort_values(by=\"gap_change\", ascending=False)\n",
    "wide_sorted.to_csv(\"ranked_bank_equity_shifts_with_names.csv\", index=False)\n",
    "print(\" Saved to 'ranked_bank_equity_shifts_with_names.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9cc843",
   "metadata": {},
   "source": [
    "print(lei_dict.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ce9feb-a2f3-48df-9751-9e0e335f500d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a70abdf8",
   "metadata": {},
   "source": [
    "Load and Clean Input Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8354bfa6-f7ad-445a-8aa8-91766181ad43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv(\"race_action_breakdown_by_year_state_lei.csv\")\n",
    "\n",
    "lei_dict = pd.read_csv(\"lei_info/bloomberg_scraped_structured.csv\")[[\n",
    "    \"lei_number\", \"Legal Name\"\n",
    "]]\n",
    "lei_dict.columns = [\"lei\", \"bank_name\"]\n",
    "\n",
    "df[\"lei\"] = df[\"lei\"].astype(str).str.strip()\n",
    "lei_dict[\"lei\"] = lei_dict[\"lei\"].astype(str).str.strip()\n",
    "\n",
    "main_races = [\n",
    "    \"White\",\n",
    "    \"Black or African American\",\n",
    "    \"Asian\",\n",
    "    \"Native Hawaiian or Pacific Islander\",\n",
    "    \"American Indian or Alaska Native\"\n",
    "]\n",
    "df = df[df[\"Race\"].isin(main_races)]\n",
    "df = df[df[\"Decision\"].isin([\"Approved\", \"Denied\"])]\n",
    "df[\"is_approved\"] = df[\"Decision\"] == \"Approved\"\n",
    "\n",
    "df[\"Period\"] = df[\"activity_year\"].apply(lambda x: \"Pre2020\" if x < 2020 else \"Post2020\")\n",
    "\n",
    "grouped = df.groupby([\"lei\", \"Race\", \"Period\"])[\"is_approved\"].agg(\n",
    "    approvals=\"sum\",\n",
    "    total=\"count\"\n",
    ").reset_index()\n",
    "grouped[\"approval_rate\"] = grouped[\"approvals\"] / grouped[\"total\"]\n",
    "\n",
    "wide = grouped.pivot(index=\"lei\", columns=[\"Race\", \"Period\"], values=\"approval_rate\")\n",
    "wide.columns = [f\"{race}_{period}\" for race, period in wide.columns]\n",
    "wide = wide.reset_index()\n",
    "\n",
    "for race in main_races:\n",
    "    pre = f\"{race}_Pre2020\"\n",
    "    post = f\"{race}_Post2020\"\n",
    "    if pre in wide.columns and post in wide.columns:\n",
    "        wide[f\"{race}_shift\"] = wide[post] - wide[pre]\n",
    "\n",
    "shift_cols = [col for col in wide.columns if \"_shift\" in col]\n",
    "X = wide[shift_cols].fillna(0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=\"auto\")\n",
    "wide[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "wide[\"lei\"] = wide[\"lei\"].astype(str).str.strip()\n",
    "lei_dict[\"lei\"] = lei_dict[\"lei\"].astype(str).str.strip()\n",
    "wide = wide.merge(lei_dict, on=\"lei\", how=\"left\")\n",
    "\n",
    "wide.to_csv(\"bank_equity_clusters.csv\", index=False)\n",
    "print(\" Final clusters saved to 'bank_equity_clusters.csv'\")\n",
    "\n",
    "print(wide[[\"lei\", \"bank_name\", \"Cluster\"] + shift_cols].head())\n",
    "df = pd.read_csv(\"bank_equity_clusters.csv\")\n",
    "\n",
    "centroids = df.groupby(\"Cluster\")[[col for col in df.columns if \"_shift\" in col]].mean()\n",
    "print(centroids)\n",
    "\n",
    "label_map = {\n",
    "    0: \"Improved for Minorities\",\n",
    "    1: \"Worsened for Minorities\",\n",
    "    2: \"Improved for All\",\n",
    "    3: \"No Change or Mixed\"\n",
    "}\n",
    "df[\"Cluster_Label\"] = df[\"Cluster\"].map(label_map)\n",
    "\n",
    "df.to_csv(\"bank_equity_clusters_labeled.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b73822",
   "metadata": {},
   "source": [
    "Load the clustered output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb096e2-c0a9-4bc2-9308-d67896ee4622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"bank_equity_clusters.csv\")\n",
    "\n",
    "shift_cols = [col for col in df.columns if col.endswith(\"_shift\")]\n",
    "\n",
    "minority_prefixes = [\n",
    "    \"Black or African American\",\n",
    "    \"Asian\",\n",
    "    \"Hispanic or Latino\",\n",
    "    \"Native Hawaiian or Pacific Islander\",\n",
    "    \"American Indian or Alaska Native\"\n",
    "]\n",
    "\n",
    "minority_cols = [col for col in shift_cols if any(col.startswith(race) for race in minority_prefixes)]\n",
    "\n",
    "cluster_means = df.groupby(\"Cluster\")[shift_cols].mean()\n",
    "cluster_means[\"minority_avg_shift\"] = cluster_means[minority_cols].mean(axis=1)\n",
    "\n",
    "best_cluster_id = cluster_means[\"minority_avg_shift\"].idxmax()\n",
    "print(f\" Best cluster (Improved for Minorities): {best_cluster_id}\")\n",
    "\n",
    "top_banks = df[df[\"Cluster\"] == best_cluster_id]\n",
    "\n",
    "priority_sort = \"Black or African American_shift\"\n",
    "sort_col = priority_sort if priority_sort in top_banks.columns else minority_cols[0]\n",
    "top_banks = top_banks.sort_values(by=sort_col, ascending=False)\n",
    "\n",
    "output_cols = [\"lei\", \"bank_name\", \"Cluster\"] + shift_cols\n",
    "top_banks = top_banks[output_cols]\n",
    "\n",
    "top_banks.to_csv(\"top_banks_improved_for_minorities.csv\", index=False)\n",
    "print(\" Saved: top_banks_improved_for_minorities.csv\")\n",
    "\n",
    "print(top_banks.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d066a3",
   "metadata": {},
   "source": [
    "Load clustered output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370279b4-6d18-40fd-8840-d9c3751844a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_clusters = pd.read_csv(\"bank_equity_clusters.csv\")\n",
    "\n",
    "df_counts = pd.read_csv(\"race_action_breakdown_by_year_state_lei.csv\")\n",
    "\n",
    "black = df_counts[df_counts[\"Race\"] == \"Black or African American\"].copy()\n",
    "black[\"Period\"] = black[\"activity_year\"].apply(lambda x: \"Pre2020\" if x < 2020 else \"Post2020\")\n",
    "\n",
    "volumes = (\n",
    "    black.groupby([\"lei\", \"Period\"])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    "    .rename(columns={\"Pre2020\": \"black_pre\", \"Post2020\": \"black_post\"})\n",
    ")\n",
    "\n",
    "MIN_APPS = 15\n",
    "\n",
    "valid_leis = volumes[(volumes[\"black_pre\"] >= MIN_APPS) & (volumes[\"black_post\"] >= MIN_APPS)].index\n",
    "\n",
    "filtered = df_clusters[df_clusters[\"lei\"].isin(valid_leis)]\n",
    "\n",
    "shift_cols = [col for col in filtered.columns if col.endswith(\"_shift\")]\n",
    "minority_cols = [c for c in shift_cols if not c.startswith(\"White\")]\n",
    "\n",
    "cluster_means = filtered.groupby(\"Cluster\")[shift_cols].mean()\n",
    "cluster_means[\"minority_avg_shift\"] = cluster_means[minority_cols].mean(axis=1)\n",
    "\n",
    "best_cluster_id = cluster_means[\"minority_avg_shift\"].idxmax()\n",
    "print(f\" Best cluster (Improved for Minorities): {best_cluster_id}\")\n",
    "\n",
    "top_banks = filtered[filtered[\"Cluster\"] == best_cluster_id]\n",
    "sort_col = \"Black or African American_shift\" if \"Black or African American_shift\" in top_banks.columns else minority_cols[0]\n",
    "top_banks = top_banks.sort_values(by=sort_col, ascending=False)\n",
    "\n",
    "output_cols = [\"lei\", \"bank_name\", \"Cluster\"] + shift_cols\n",
    "top_banks = top_banks[output_cols]\n",
    "\n",
    "top_banks.head(10).to_csv(\"top_banks_improved_for_minorities_filtered.csv\", index=False)\n",
    "print(\" Saved: top_banks_improved_for_minorities_filtered.csv\")\n",
    "print(top_banks.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84066f4b-7d9f-4b87-b109-fc103e5a6c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of valid LEIs: {len(valid_leis)}\")\n",
    "print(f\"Number of banks in top cluster: {top_banks.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d099194",
   "metadata": {},
   "source": [
    " Load clustered output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae49530-adc3-4e16-a48e-cfa2e97a46ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_clusters = pd.read_csv(\"bank_equity_clusters.csv\")\n",
    "\n",
    "df_counts = pd.read_csv(\"race_action_breakdown_by_year_state_lei.csv\")\n",
    "\n",
    "black = df_counts[df_counts[\"Race\"] == \"Black or African American\"].copy()\n",
    "black[\"Period\"] = black[\"activity_year\"].apply(lambda x: \"Pre2020\" if x < 2020 else \"Post2020\")\n",
    "\n",
    "volumes = (\n",
    "    black.groupby([\"lei\", \"Period\"])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    "    .rename(columns={\"Pre2020\": \"black_pre\", \"Post2020\": \"black_post\"})\n",
    ")\n",
    "\n",
    "MIN_APPS = 15\n",
    "\n",
    "valid_leis = volumes[(volumes[\"black_pre\"] <= MIN_APPS) & (volumes[\"black_post\"] <= MIN_APPS)].index\n",
    "\n",
    "filtered = df_clusters[df_clusters[\"lei\"].isin(valid_leis)]\n",
    "\n",
    "shift_cols = [col for col in filtered.columns if col.endswith(\"_shift\")]\n",
    "minority_cols = [c for c in shift_cols if not c.startswith(\"White\")]\n",
    "\n",
    "cluster_means = filtered.groupby(\"Cluster\")[shift_cols].mean()\n",
    "cluster_means[\"minority_avg_shift\"] = cluster_means[minority_cols].mean(axis=1)\n",
    "\n",
    "worst_cluster_id = cluster_means[\"minority_avg_shift\"].idxmin()\n",
    "print(f\"️ Worst cluster (Worsened for Minorities): {worst_cluster_id}\")\n",
    "\n",
    "worst_banks = filtered[filtered[\"Cluster\"] == worst_cluster_id]\n",
    "sort_col = \"Black or African American_shift\" if \"Black or African American_shift\" in worst_banks.columns else minority_cols[0]\n",
    "worst_banks = worst_banks.sort_values(by=sort_col, ascending=True)\n",
    "\n",
    "output_cols = [\"lei\", \"bank_name\", \"Cluster\"] + shift_cols\n",
    "worst_banks = worst_banks[output_cols]\n",
    "\n",
    "worst_banks.head(10).to_csv(\"worst_banks_for_minorities_filtered.csv\", index=False)\n",
    "print(\" Saved: worst_banks_for_minorities_filtered.csv\")\n",
    "\n",
    "print(worst_banks.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaad969",
   "metadata": {},
   "source": [
    "unmatched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672a957e-bca5-4a75-8b4d-98fddd378afc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96ec69eb",
   "metadata": {},
   "source": [
    "Load both\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41821000-6873-4188-bdfb-98f5b29bb790",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = pd.read_csv(\"bank_equity_clusters.csv\")\n",
    "lei_dict = pd.read_csv(\"lei_info/bloomberg_scraped_structured.csv\")\n",
    "lei_dict.columns = lei_dict.columns.str.lower()\n",
    "lei_dict[\"lei_number\"] = lei_dict[\"lei_number\"].astype(str).str.strip()\n",
    "\n",
    "merged = clusters.merge(lei_dict[[\"lei_number\", \"legal name\"]], left_on=\"lei\", right_on=\"lei_number\", how=\"left\")\n",
    "missing = merged[merged[\"legal name\"].isna()]\n",
    "print(f\"️Missing bank names for {len(missing)} out of {len(clusters)} LEIs\")\n",
    "print(missing[\"lei\"].value_counts().head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965b17f6",
   "metadata": {},
   "source": [
    "Merck employee\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee176afd-816e-4201-8fa9-5d985ceb8ebd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7749e4e",
   "metadata": {},
   "source": [
    "Load the full dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63f5c24-4d17-4ebe-ac41-d6b2f07a85b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"race_action_breakdown_by_year_state_lei.csv\")\n",
    "\n",
    "target_lei = \"549300RKVDUINI2A8Y77\"\n",
    "df_lei = df[df[\"lei\"] == target_lei]\n",
    "\n",
    "df_lei = df_lei[df_lei[\"Decision\"].isin([\"Approved\", \"Denied\"])]\n",
    "\n",
    "df_lei[\"is_approved\"] = df_lei[\"Decision\"] == \"Approved\"\n",
    "\n",
    "summary = df_lei.groupby([\"activity_year\", \"Race\"]).agg(\n",
    "    total_applications=(\"is_approved\", \"count\"),\n",
    "    approvals=(\"is_approved\", \"sum\")\n",
    ").reset_index()\n",
    "\n",
    "summary[\"approval_rate\"] = summary[\"approvals\"] / summary[\"total_applications\"]\n",
    "\n",
    "summary.to_csv(\"lei_race_year_summary_549300RKVDUINI2A8Y77.csv\", index=False)\n",
    "print(\" Summary saved to 'lei_race_year_summary_549300RKVDUINI2A8Y77.csv'\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928fd486",
   "metadata": {},
   "source": [
    "Load cluster results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937d7829-e2af-4513-ba59-05b9670437af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from us import states\n",
    "\n",
    "clusters = pd.read_csv(\"bank_equity_clusters.csv\")\n",
    "\n",
    "lei_info = pd.read_csv(\"lei_info/bloomberg_scraped_structured.csv\")[[\n",
    "    \"lei_number\", \"Legal Name\", \"Headquarters Address (English)\"\n",
    "]].drop_duplicates()\n",
    "lei_info.columns = [\"lei\", \"bank_name\", \"address\"]\n",
    "\n",
    "def extract_state(address):\n",
    "    for s in states.STATES:\n",
    "        if s.abbr in str(address):\n",
    "            return s.abbr\n",
    "    return None\n",
    "\n",
    "lei_info[\"state_code\"] = lei_info[\"address\"].apply(extract_state)\n",
    "lei_info = lei_info.dropna(subset=[\"state_code\"])\n",
    "\n",
    "merged = clusters.merge(lei_info, on=\"lei\", how=\"left\")\n",
    "merged = merged.dropna(subset=[\"state_code\"])\n",
    "\n",
    "\n",
    "shapefile_path = \"shapes/ne_50m_admin_1_states_provinces.shp\"\n",
    "us_states = gpd.read_file(shapefile_path)\n",
    "\n",
    "us_states = us_states[us_states[\"admin\"] == \"United States of America\"]\n",
    "us_states[\"state_code\"] = us_states[\"postal\"]\n",
    "us_states = us_states[[\"state_code\", \"geometry\"]]\n",
    "\n",
    "us_states[\"geometry\"] = us_states[\"geometry\"].centroid\n",
    "\n",
    "merged = merged.merge(us_states, on=\"state_code\", how=\"left\")\n",
    "gdf = gpd.GeoDataFrame(merged, geometry=\"geometry\", crs=\"EPSG:4326\").to_crs(epsg=3857)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 9))\n",
    "gdf.plot(column=\"Cluster\", cmap=\"tab10\", legend=True, markersize=60, edgecolor=\"black\", alpha=0.85, ax=ax)\n",
    "\n",
    "ax.set_title(\"U.S. Banks by Equity Shift Cluster (State HQ Approximation)\", fontsize=16)\n",
    "ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"bank_clusters_by_state_map.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dcf5f4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1adb65b-1184-4c75-8f8c-028a2332b913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"bank_equity_clusters.csv\")\n",
    "\n",
    "shift_cols = [col for col in df.columns if col.endswith(\"_shift\")]\n",
    "\n",
    "minority_prefixes = [\n",
    "    \"Black or African American\",\n",
    "    \"Asian\",\n",
    "    \"Hispanic or Latino\",\n",
    "    \"Native Hawaiian or Pacific Islander\",\n",
    "    \"American Indian or Alaska Native\"\n",
    "]\n",
    "\n",
    "minority_cols = [col for col in shift_cols if any(col.startswith(race) for race in minority_prefixes)]\n",
    "\n",
    "cluster_means = df.groupby(\"Cluster\")[shift_cols].mean()\n",
    "cluster_means[\"minority_avg_shift\"] = cluster_means[minority_cols].mean(axis=1)\n",
    "\n",
    "best_cluster_id = cluster_means[\"minority_avg_shift\"].idxmax()\n",
    "worst_cluster_id = cluster_means[\"minority_avg_shift\"].idxmin()\n",
    "\n",
    "print(f\" Best cluster (Improved for Minorities): {best_cluster_id}\")\n",
    "print(f\"️ Worst cluster (Worsened or Least Improved for Minorities): {worst_cluster_id}\")\n",
    "\n",
    "top_banks = df[df[\"Cluster\"] == best_cluster_id].copy()\n",
    "sort_col = \"Black or African American_shift\" if \"Black or African American_shift\" in top_banks.columns else minority_cols[0]\n",
    "top_banks = top_banks.sort_values(by=sort_col, ascending=False)\n",
    "top_banks = top_banks[[\"lei\", \"bank_name\", \"Cluster\"] + shift_cols]\n",
    "top_banks.to_csv(\"top_banks_improved_for_minorities.csv\", index=False)\n",
    "print(\" Saved: top_banks_improved_for_minorities.csv\")\n",
    "\n",
    "worst_banks = df[df[\"Cluster\"] == worst_cluster_id].copy()\n",
    "worst_banks = worst_banks.sort_values(by=sort_col, ascending=True)\n",
    "worst_banks = worst_banks[[\"lei\", \"bank_name\", \"Cluster\"] + shift_cols]\n",
    "worst_banks.to_csv(\"worst_banks_for_minorities.csv\", index=False)\n",
    "print(\" Saved: worst_banks_for_minorities.csv\")\n",
    "\n",
    "print(\"\\n Top 5 Best Banks:\")\n",
    "print(top_banks.head())\n",
    "\n",
    "print(\"\\n Top 5 Worst Banks:\")\n",
    "print(worst_banks.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422c1e3f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7672b4e4-a212-4aa0-ba7e-179d55b8cafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "df = pd.read_csv(\"approval_rates_by_lei_race_year_state.csv\")\n",
    "\n",
    "main_races = [\n",
    "    \"Asian\",\n",
    "    \"Black\",\n",
    "    \"White\",\n",
    "    \"Indigenous\",  # usually short for \"American Indian or Alaska Native\"\n",
    "    \"Hawaiian/Pacific Islander\"\n",
    "]\n",
    "df = df[df['race_label'].isin(main_races)]\n",
    "\n",
    "island_codes = ['PR', 'GU', 'VI', 'AS', 'MP']\n",
    "df = df[~df['state_code'].isin(island_codes)]\n",
    "\n",
    "df = df[df['total_applications'] >= 10]\n",
    "\n",
    "agg = df.groupby(['state_code', 'year', 'race_label']).agg(\n",
    "    total_apps=('total_applications', 'sum'),\n",
    "    approved_apps=('approved_count', 'sum')\n",
    ").reset_index()\n",
    "agg['approval_rate'] = agg['approved_apps'] / agg['total_apps']\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "g = sns.FacetGrid(\n",
    "    agg,\n",
    "    col='state_code',\n",
    "    col_wrap=6,\n",
    "    height=3.5,\n",
    "    aspect=1.3,\n",
    "    sharey=True\n",
    ")\n",
    "g.map_dataframe(\n",
    "    sns.lineplot,\n",
    "    x='year',\n",
    "    y='approval_rate',\n",
    "    hue='race_label',\n",
    "    marker='o'\n",
    ")\n",
    "g.set_axis_labels(\"Year\", \"Approval Rate\")\n",
    "g.set_titles(\"{col_name}\")\n",
    "g.add_legend(title='Race')\n",
    "g.set(ylim=(0.0, 1.0))\n",
    "\n",
    "for ax in g.axes.flatten():\n",
    "    ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "\n",
    "plt.subplots_adjust(top=0.92)\n",
    "g.fig.suptitle(\"Mortgage Approval Rates by Race Over Time (by State)\", fontsize=16, fontweight='bold')\n",
    "\n",
    "g.savefig(\"approval_rates_by_state.svg\", format='svg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b95b71",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61948969-4fb6-41a9-b112-775abfdf66d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print([f for f in os.listdir() if f.endswith('.csv')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246c2b88",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227edf4b-aefd-4ece-a12c-88672d6520c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd2a8da",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f5be8d-3311-497a-9479-a4954322eab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"approval_rates_by_lei_race_year_state.csv\")\n",
    "\n",
    "counts = df.groupby(['state_code', 'year', 'race_label']).size().reset_index(name='count')\n",
    "print(counts.sort_values(by='count').head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba6f15b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0690bc3d-b683-4301-8f19-dbd2cf7b617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_2024 = pd.read_csv(\"filtered_data_strict/2024_filtered_hmda.csv\", low_memory=False)\n",
    "\n",
    "race_col = 'applicant_race_1' if 'applicant_race_1' in df_2024.columns else 'applicant_race-1'\n",
    "\n",
    "asian_race_codes = [2.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0]\n",
    "df_vermont_asian = df_2024[\n",
    "    (df_2024['state_code'] == 'VT') &\n",
    "    (df_2024[race_col].isin(asian_race_codes))\n",
    "]\n",
    "\n",
    "print(f\"Found {len(df_vermont_asian)} Asian loans in Vermont (2024):\")\n",
    "df_vermont_asian.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689fbd26",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f08971-f916-4200-b8a1-7cbb66273767",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_2024.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6824dd31",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7f66ff-9d57-4251-badf-207562ed3366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_2024 = pd.read_csv(\"filtered_data_strict/2024_filtered_hmda.csv\", low_memory=False)\n",
    "\n",
    "race_col = 'applicant_race_1'\n",
    "\n",
    "asian_race_codes = [2.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0]\n",
    "\n",
    "df_vermont_asian = df_2024[\n",
    "    (df_2024['state_code'] == 'VT') &\n",
    "    (df_2024[race_col].astype(float).isin(asian_race_codes))\n",
    "]\n",
    "\n",
    "print(f\" Found {len(df_vermont_asian)} Asian loans in Vermont (2024):\")\n",
    "df_vermont_asian.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa20b24",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2351d947-db9a-4f0f-85ca-5647aab27079",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4ac36a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03f9428-06ac-4080-9138-d8689d31fa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['race_label'].notna()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bee043",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9551949b-ba7a-4228-89eb-a7f634665971",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9280320a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7105780-c534-401f-8425-d831b5019438",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['denied'] = df['total_applications'] - df['approved_count']\n",
    "\n",
    "lei_summary = df.groupby('lei').agg({\n",
    "    'approved_count': 'sum',\n",
    "    'denied': 'sum',\n",
    "    'total_applications': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "lei_summary.rename(columns={\n",
    "    'approved_count': 'accepted',\n",
    "    'denied': 'denied',\n",
    "    'total_applications': 'total'\n",
    "}, inplace=True)\n",
    "\n",
    "lei_summary = lei_summary.sort_values(by='total', ascending=False)\n",
    "\n",
    "lei_summary.to_csv('lei_accept_denied_summary_2024.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a77727a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07368dae-716d-4a13-b2c2-9e7456d2ead5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece0e6d3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eb1fb6-1f71-4aa7-b997-83cc85e9396a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "lei_summary = pd.read_csv('lei_accept_denied_summary_2024.csv')\n",
    "\n",
    "total_all = df.groupby('lei').size().reset_index(name='total_all')\n",
    "\n",
    "lei_summary = pd.merge(lei_summary, total_all, on='lei', how='left')\n",
    "\n",
    "lei_summary.to_csv('lei_accept_denied_summary_2024.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f90b0e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781cf040-3bdb-4c18-b792-9b87f873accb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "years = [2018, 2019, 2020, 2021, 2022, 2023, 2024]\n",
    "\n",
    "data_dir = './unfiltered'\n",
    "output_dir = './filtered_outputs'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "required_columns = [\n",
    "    'total_units', 'debt_to_income_ratio', 'action_taken', 'income',\n",
    "    'construction_method', 'loan_amount', 'property_value',\n",
    "    'activity_year'\n",
    "]\n",
    "\n",
    "race_column_legacy = 'applicant_race-1'\n",
    "race_column_2024 = 'applicant_race_1'\n",
    "\n",
    "for year in years:\n",
    "    input_file = os.path.join(data_dir, f'{year}_hmda.csv')\n",
    "    output_file = os.path.join(output_dir, f'filtered_{year}.csv')\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(input_file, low_memory=True)\n",
    "\n",
    "        if race_column_legacy in df.columns:\n",
    "            df.rename(columns={race_column_legacy: 'race'}, inplace=True)\n",
    "        elif race_column_2024 in df.columns:\n",
    "            df.rename(columns={race_column_2024: 'race'}, inplace=True)\n",
    "        else:\n",
    "            print(f\"️ Skipping {year}: Missing race column\")\n",
    "            continue\n",
    "\n",
    "        missing = [col for col in required_columns if col not in df.columns]\n",
    "        if missing:\n",
    "            print(f\"️ Skipping {year}: Missing columns {missing}\")\n",
    "            continue\n",
    "\n",
    "        df['total_units'] = pd.to_numeric(df['total_units'], errors='coerce')\n",
    "        df['debt_to_income_ratio_clean'] = df['debt_to_income_ratio'].apply(\n",
    "            lambda x: pd.to_numeric(\n",
    "                str(x).replace('%', '')\n",
    "                      .replace('>', '')\n",
    "                      .replace('<', '')\n",
    "                      .replace('NA', '')\n",
    "                      .replace('Exempt', '')\n",
    "                      .replace(' ', '')\n",
    "                      .replace('--', '')\n",
    "                      .replace('n/a', '')\n",
    "                      .replace('N/A', '')\n",
    "                      .replace('Not Applicable', ''),\n",
    "                errors='coerce'\n",
    "            )\n",
    "        )\n",
    "\n",
    "        df_filtered = df[\n",
    "            df['action_taken'].isin([1, 3]) &\n",
    "            df['income'].notna() & (df['income'] > 0) &\n",
    "            (df['loan_type'] == 1) &\n",
    "            (df['lien_status'] == 1) &\n",
    "            (df['occupancy_type'] == 1) &\n",
    "            (df['construction_method'] == 1) &\n",
    "            (df['total_units'] <= 4) &\n",
    "            df['loan_amount'].notna() &\n",
    "            df['property_value'].notna() &\n",
    "            df['debt_to_income_ratio_clean'].notna() &\n",
    "            df['race'].isin([2.0, 3.0, 4.0, 5.0]) &\n",
    "            df['activity_year'].isin([2018, 2019, 2020, 2021, 2022, 2023, 2024])\n",
    "        ]\n",
    "\n",
    "        df_filtered.to_csv(output_file, index=False)\n",
    "        print(f\" Processed {year}: {len(df_filtered)} rows\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\" File not found: {input_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Error processing {year}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b1917d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41093ffe-353a-476c-bca8-5d5f04a7674b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "unfiltered_folder = \"unfiltered\"\n",
    "filtered_folder = \"filtered_data_strict\"\n",
    "\n",
    "unfiltered_files = [\n",
    "    \"2018_hmda.csv\", \"2019_hmda.csv\", \"2020_hmda.csv\",\n",
    "    \"2021_hmda.csv\", \"2022_hmda.csv\", \"2023_hmda.csv\", \"2024_hmda.csv\"\n",
    "]\n",
    "filtered_files = [\n",
    "    \"2018_filtered_hmda.csv\", \"2019_filtered_hmda.csv\", \"2020_filtered_hmda.csv\",\n",
    "    \"2021_filtered_hmda.csv\", \"2022_filtered_hmda.csv\", \"2023_filtered_hmda.csv\",\n",
    "    \"2024_filtered_hmda.csv\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for i in range(len(unfiltered_files)):\n",
    "    year = 2018 + i\n",
    "    print(f\"Processing {year}...\")\n",
    "\n",
    "    df_unfiltered = pd.read_csv(os.path.join(unfiltered_folder, unfiltered_files[i]), low_memory=False)\n",
    "    total_all = df_unfiltered.groupby('lei').size().reset_index(name='total_all')\n",
    "\n",
    "    df_filtered = pd.read_csv(os.path.join(filtered_folder, filtered_files[i]), low_memory=False)\n",
    "\n",
    "    race_col = 'applicant_race-1' if 'applicant_race-1' in df_filtered.columns else 'applicant_race_1'\n",
    "\n",
    "    accepted = df_filtered[df_filtered['action_taken'].isin([1, 2])].groupby('lei').size().reset_index(name='accepted')\n",
    "    denied = df_filtered[df_filtered['action_taken'] == 3].groupby('lei').size().reset_index(name='denied')\n",
    "\n",
    "    def race_group(race_code, action_codes, name):\n",
    "        return (\n",
    "            df_filtered[(df_filtered[race_col] == race_code) & (df_filtered['action_taken'].isin(action_codes))]\n",
    "            .groupby('lei').size().reset_index(name=name)\n",
    "        )\n",
    "\n",
    "    white_accepted = race_group(5, [1, 2], 'white_accepted')\n",
    "    white_denied = race_group(5, [3], 'white_denied')\n",
    "    black_accepted = race_group(3, [1, 2], 'black_accepted')\n",
    "    black_denied = race_group(3, [3], 'black_denied')\n",
    "    asian_accepted = race_group(2, [1, 2], 'asian_accepted')\n",
    "    asian_denied = race_group(2, [3], 'asian_denied')\n",
    "\n",
    "    merged = total_all \\\n",
    "        .merge(accepted, on='lei', how='outer') \\\n",
    "        .merge(denied, on='lei', how='outer') \\\n",
    "        .merge(white_accepted, on='lei', how='outer') \\\n",
    "        .merge(white_denied, on='lei', how='outer') \\\n",
    "        .merge(black_accepted, on='lei', how='outer') \\\n",
    "        .merge(black_denied, on='lei', how='outer') \\\n",
    "        .merge(asian_accepted, on='lei', how='outer') \\\n",
    "        .merge(asian_denied, on='lei', how='outer')\n",
    "\n",
    "    merged = merged.fillna(0)\n",
    "    merged['total'] = merged['accepted'] + merged['denied']\n",
    "    merged['year'] = year\n",
    "\n",
    "    results.append(merged)\n",
    "\n",
    "full_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "output_path = \"lei_summary_2018_2024.csv\"\n",
    "full_df.to_csv(output_path, index=False)\n",
    "print(f\" Summary saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9469f40",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93cde1a-be35-4fae-9e84-78b0484498b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"lei_summary_2018_2024.csv\")\n",
    "\n",
    "lei_map = {\n",
    "    \"B4TYDEB6GKMZO031MB27\": \"Bank of America\",\n",
    "    \"7H6GLXDRUGQFU57RNE97\": \"JPMorgan Chase\",\n",
    "    \"KB1H1DSPRFMYMCUFXT09\": \"Wells Fargo\",\n",
    "    \"E57ODZWZ7FF32TWEFA76\": \"Citibank\"\n",
    "}\n",
    "\n",
    "df_four = df[df[\"lei\"].isin(lei_map.keys())].copy()\n",
    "df_four[\"bank_name\"] = df_four[\"lei\"].map(lei_map)\n",
    "\n",
    "df_melted = df_four.melt(\n",
    "    id_vars=[\"year\", \"bank_name\"],\n",
    "    value_vars=[\"total_all\", \"white_accepted\", \"black_accepted\", \"asian_accepted\"],\n",
    "    var_name=\"category\",\n",
    "    value_name=\"count\"\n",
    ")\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
    "g = sns.FacetGrid(df_melted, col=\"bank_name\", col_wrap=2, height=4, sharey=False)\n",
    "g.map_dataframe(sns.lineplot, x=\"year\", y=\"count\", hue=\"category\", marker=\"o\")\n",
    "\n",
    "g.set_titles(\"{col_name}\")\n",
    "g.set_axis_labels(\"Year\", \"Application Count\")\n",
    "g.add_legend()\n",
    "plt.subplots_adjust(top=0.9)\n",
    "g.fig.suptitle(\"Mortgage Applications by Race and Total: Top 4 Banks (2018–2024)\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15294b4d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9119123-d80a-4bb4-930e-c1de0393333d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"lei_summary_2018_2024.csv\")\n",
    "\n",
    "lei_map = {\n",
    "    \"B4TYDEB6GKMZO031MB27\": \"Bank of America\",\n",
    "    \"7H6GLXDRUGQFU57RNE97\": \"JPMorgan Chase\",\n",
    "    \"KB1H1DSPRFMYMCUFXT09\": \"Wells Fargo\",\n",
    "    \"E57ODZWZ7FF32TWEFA76\": \"Citibank\"\n",
    "}\n",
    "\n",
    "df_filtered = df[df[\"lei\"].isin(lei_map)].copy()\n",
    "df_filtered[\"bank_name\"] = df_filtered[\"lei\"].map(lei_map)\n",
    "\n",
    "df_plot = df_filtered[[\"year\", \"bank_name\", \"total_all\"]]\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
    "g = sns.FacetGrid(df_plot, col=\"bank_name\", col_wrap=2, height=4, sharey=False)\n",
    "g.map_dataframe(sns.lineplot, x=\"year\", y=\"total_all\", marker=\"o\", color=\"steelblue\")\n",
    "\n",
    "g.set_titles(\"{col_name}\")\n",
    "g.set_axis_labels(\"Year\", \"Total Applications\")\n",
    "plt.subplots_adjust(top=0.9)\n",
    "g.fig.suptitle(\"Total Mortgage Applications by Bank (2018–2024)\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7079f1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375a8e42-87d1-47e2-9a0a-b5dd61b03e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('year').size()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e169d7e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfebbe69-84e8-4c72-bd3d-d9aea89d4759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_folder = \"filtered_data_strict\"\n",
    "output_csv = \"lei_state_summary.csv\"\n",
    "years = range(2018, 2025)\n",
    "\n",
    "lei_map = {\n",
    "    \"B4TYDEB6GKMZO031MB27\": \"Bank of America\",\n",
    "    \"7H6GLXDRUGQFU57RNE97\": \"JP Morgan\",\n",
    "    \"KB1H1DSPRFMYMCUFXT09\": \"Wells Fargo\",\n",
    "    \"E57ODZWZ7FF32TWEFA76\": \"Citi Bank\"\n",
    "}\n",
    "\n",
    "race_map = {\n",
    "    2.0: 'Asian', 21: 'Asian', 22: 'Asian', 23: 'Asian', 24: 'Asian', 25: 'Asian', 26: 'Asian', 27: 'Asian',\n",
    "    3.0: 'Black',\n",
    "    5.0: 'White',\n",
    "    1.0: 'Indigenous',\n",
    "    4.0: 'Hawaiian/Pacific Islander', 41: 'Hawaiian/Pacific Islander', 42: 'Hawaiian/Pacific Islander',\n",
    "    43: 'Hawaiian/Pacific Islander', 44: 'Hawaiian/Pacific Islander'\n",
    "}\n",
    "\n",
    "records = []\n",
    "\n",
    "for year in years:\n",
    "    file_path = os.path.join(data_folder, f\"{year}_filtered_hmda.csv\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"️ Missing: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "    race_col = 'applicant_race-1' if 'applicant_race-1' in df.columns else 'applicant_race_1'\n",
    "    df['race_label'] = df[race_col].map(race_map)\n",
    "    df = df[df['race_label'].notna()]\n",
    "\n",
    "    df = df[df['lei'].isin(lei_map.keys())]\n",
    "    df = df[df['action_taken'].isin([1, 2])]  # Approved only\n",
    "\n",
    "    df_grouped = df.groupby(['lei', 'state_code', 'race_label']).size().reset_index(name='count')\n",
    "    df_grouped['year'] = year\n",
    "\n",
    "    records.append(df_grouped)\n",
    "\n",
    "df_all = pd.concat(records, ignore_index=True)\n",
    "\n",
    "summary = (\n",
    "    df_all.pivot_table(\n",
    "        index=['lei', 'state_code', 'year'],\n",
    "        columns='race_label',\n",
    "        values='count',\n",
    "        fill_value=0\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename_axis(None, axis=1)\n",
    ")\n",
    "\n",
    "for col in ['White', 'Black', 'Asian', 'Indigenous', 'Hawaiian/Pacific Islander']:\n",
    "    if col not in summary.columns:\n",
    "        summary[col] = 0\n",
    "\n",
    "summary = summary.rename(columns={\n",
    "    'White': 'white_accepted',\n",
    "    'Black': 'black_accepted',\n",
    "    'Asian': 'asian_accepted',\n",
    "    'Indigenous': 'indigenous_accepted',\n",
    "    'Hawaiian/Pacific Islander': 'hawaiian_accepted'\n",
    "})\n",
    "\n",
    "summary.to_csv(output_csv, index=False)\n",
    "print(f\" Saved summary to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eba9ba",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2771d5-bf07-49da-8329-18a59fe41a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "unfiltered_folder = \"unfiltered\"\n",
    "filtered_folder = \"filtered_data_strict\"\n",
    "\n",
    "years = list(range(2018, 2025))\n",
    "unfiltered_files = [f\"{y}_hmda.csv\" for y in years]\n",
    "filtered_files = [f\"{y}_filtered_hmda.csv\" for y in years]\n",
    "\n",
    "lei_results = []\n",
    "state_results = []\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(f\"\\n Processing {year}...\")\n",
    "\n",
    "    unf_path = os.path.join(unfiltered_folder, unfiltered_files[i])\n",
    "    if not os.path.exists(unf_path):\n",
    "        print(f\" Missing unfiltered file: {unf_path}\")\n",
    "        continue\n",
    "    df_unfiltered = pd.read_csv(unf_path, low_memory=False)\n",
    "    total_all = df_unfiltered.groupby('lei').size().reset_index(name='total_all')\n",
    "\n",
    "    filt_path = os.path.join(filtered_folder, filtered_files[i])\n",
    "    if not os.path.exists(filt_path):\n",
    "        print(f\" Missing filtered file: {filt_path}\")\n",
    "        continue\n",
    "    df_filtered = pd.read_csv(filt_path, low_memory=False)\n",
    "\n",
    "    race_col = 'applicant_race-1' if 'applicant_race-1' in df_filtered.columns else 'applicant_race_1'\n",
    "\n",
    "    accepted = df_filtered[df_filtered['action_taken'].isin([1, 2])].groupby('lei').size().reset_index(name='accepted')\n",
    "    denied = df_filtered[df_filtered['action_taken'] == 3].groupby('lei').size().reset_index(name='denied')\n",
    "\n",
    "    def race_group(code, action_taken, label):\n",
    "        return df_filtered[(df_filtered[race_col] == code) & (df_filtered['action_taken'].isin(action_taken))] \\\n",
    "            .groupby('lei').size().reset_index(name=label)\n",
    "\n",
    "    merged_lei = total_all \\\n",
    "        .merge(accepted, on='lei', how='outer') \\\n",
    "        .merge(denied, on='lei', how='outer') \\\n",
    "        .merge(race_group(5, [1, 2], 'white_accepted'), on='lei', how='outer') \\\n",
    "        .merge(race_group(5, [3], 'white_denied'), on='lei', how='outer') \\\n",
    "        .merge(race_group(3, [1, 2], 'black_accepted'), on='lei', how='outer') \\\n",
    "        .merge(race_group(3, [3], 'black_denied'), on='lei', how='outer') \\\n",
    "        .merge(race_group(2, [1, 2], 'asian_accepted'), on='lei', how='outer') \\\n",
    "        .merge(race_group(2, [3], 'asian_denied'), on='lei', how='outer') \\\n",
    "        .merge(race_group(1, [1, 2], 'indigenous_accepted'), on='lei', how='outer') \\\n",
    "        .merge(race_group(1, [3], 'indigenous_denied'), on='lei', how='outer') \\\n",
    "        .merge(race_group(4, [1, 2], 'hawaiian_accepted'), on='lei', how='outer') \\\n",
    "        .merge(race_group(4, [3], 'hawaiian_denied'), on='lei', how='outer')\n",
    "\n",
    "    merged_lei = merged_lei.fillna(0)\n",
    "    merged_lei['total'] = merged_lei['accepted'] + merged_lei['denied']\n",
    "    merged_lei['year'] = year\n",
    "    lei_results.append(merged_lei)\n",
    "\n",
    "    df_filtered = df_filtered[df_filtered['action_taken'].isin([1, 2, 3])]\n",
    "    grouped = df_filtered.groupby(['state_code', 'activity_year'])\n",
    "\n",
    "    def count_by_race(df, race_code, actions):\n",
    "        return ((df[race_col] == race_code) & (df['action_taken'].isin(actions))).sum()\n",
    "\n",
    "    state_summary = grouped.apply(lambda g: pd.Series({\n",
    "        'total_all': len(g),\n",
    "        'white_accepted': count_by_race(g, 5, [1, 2]),\n",
    "        'black_accepted': count_by_race(g, 3, [1, 2]),\n",
    "        'asian_accepted': count_by_race(g, 2, [1, 2]),\n",
    "        'indigenous_accepted': count_by_race(g, 1, [1, 2]),\n",
    "        'hawaiian_accepted': count_by_race(g, 4, [1, 2]),\n",
    "        'white_denied': count_by_race(g, 5, [3]),\n",
    "        'black_denied': count_by_race(g, 3, [3]),\n",
    "        'asian_denied': count_by_race(g, 2, [3]),\n",
    "        'indigenous_denied': count_by_race(g, 1, [3]),\n",
    "        'hawaiian_denied': count_by_race(g, 4, [3])\n",
    "    })).reset_index()\n",
    "\n",
    "    state_results.append(state_summary)\n",
    "\n",
    "if lei_results:\n",
    "    pd.concat(lei_results).to_csv(\"lei_summary_2018_2024.csv\", index=False)\n",
    "    print(\" Saved: lei_summary_2018_2024.csv\")\n",
    "\n",
    "if state_results:\n",
    "    pd.concat(state_results).to_csv(\"state_year_mortgage_summary.csv\", index=False)\n",
    "    print(\" Saved: state_year_mortgage_summary.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55256ed9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2064c125-99a5-4d57-9a18-9514e5e24f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "unfiltered_folder = \"unfiltered\"\n",
    "filtered_folder = \"filtered_data_strict\"\n",
    "years = list(range(2018, 2025))\n",
    "\n",
    "unfiltered_files = [f\"{y}_hmda.csv\" for y in years]\n",
    "filtered_files = [f\"{y}_filtered_hmda.csv\" for y in years]\n",
    "\n",
    "lei_results = []\n",
    "lei_state_results = []\n",
    "state_results = []\n",
    "\n",
    "def count_by_race(df, race_code, actions):\n",
    "    return ((df[race_col] == race_code) & (df['action_taken'].isin(actions))).sum()\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(f\" Processing {year}...\")\n",
    "\n",
    "    unf_path = os.path.join(unfiltered_folder, unfiltered_files[i])\n",
    "    if not os.path.exists(unf_path):\n",
    "        print(f\" Missing: {unf_path}\")\n",
    "        continue\n",
    "    df_unfiltered = pd.read_csv(unf_path, low_memory=False)\n",
    "    total_all = df_unfiltered.groupby('lei').size().reset_index(name='total_all')\n",
    "\n",
    "    filt_path = os.path.join(filtered_folder, filtered_files[i])\n",
    "    if not os.path.exists(filt_path):\n",
    "        print(f\" Missing: {filt_path}\")\n",
    "        continue\n",
    "    df_filtered = pd.read_csv(filt_path, low_memory=False)\n",
    "    race_col = 'applicant_race-1' if 'applicant_race-1' in df_filtered.columns else 'applicant_race_1'\n",
    "\n",
    "    actions = df_filtered['action_taken']\n",
    "    accepted = df_filtered[actions.isin([1, 2])].groupby('lei').size().reset_index(name='accepted')\n",
    "    denied = df_filtered[actions == 3].groupby('lei').size().reset_index(name='denied')\n",
    "\n",
    "    def race_group(code, actions, name):\n",
    "        return df_filtered[(df_filtered[race_col] == code) & (df_filtered['action_taken'].isin(actions))] \\\n",
    "            .groupby('lei').size().reset_index(name=name)\n",
    "\n",
    "    merged_lei = total_all \\\n",
    "        .merge(accepted, on='lei', how='outer') \\\n",
    "        .merge(denied, on='lei', how='outer') \\\n",
    "        .merge(race_group(5, [1, 2], 'white_accepted'), on='lei', how='outer') \\\n",
    "        .merge(race_group(5, [3], 'white_denied'), on='lei', how='outer') \\\n",
    "        .merge(race_group(3, [1, 2], 'black_accepted'), on='lei', how='outer') \\\n",
    "        .merge(race_group(3, [3], 'black_denied'), on='lei', how='outer') \\\n",
    "        .merge(race_group(2, [1, 2], 'asian_accepted'), on='lei', how='outer') \\\n",
    "        .merge(race_group(2, [3], 'asian_denied'), on='lei', how='outer') \\\n",
    "        .merge(race_group(1, [1, 2], 'indigenous_accepted'), on='lei', how='outer') \\\n",
    "        .merge(race_group(1, [3], 'indigenous_denied'), on='lei', how='outer') \\\n",
    "        .merge(race_group(4, [1, 2], 'hawaiian_accepted'), on='lei', how='outer') \\\n",
    "        .merge(race_group(4, [3], 'hawaiian_denied'), on='lei', how='outer')\n",
    "\n",
    "    merged_lei = merged_lei.fillna(0)\n",
    "    merged_lei['total'] = merged_lei['accepted'] + merged_lei['denied']\n",
    "    merged_lei['year'] = year\n",
    "    lei_results.append(merged_lei)\n",
    "\n",
    "    filtered = df_filtered[df_filtered['action_taken'].isin([1, 2, 3])]\n",
    "    lei_state_summary = (\n",
    "        filtered.groupby(['lei', 'state_code'])\n",
    "        .apply(lambda g: pd.Series({\n",
    "            'total_all': len(g),\n",
    "            'white_accepted': count_by_race(g, 5, [1, 2]),\n",
    "            'black_accepted': count_by_race(g, 3, [1, 2]),\n",
    "            'asian_accepted': count_by_race(g, 2, [1, 2]),\n",
    "            'indigenous_accepted': count_by_race(g, 1, [1, 2]),\n",
    "            'hawaiian_accepted': count_by_race(g, 4, [1, 2]),\n",
    "            'white_denied': count_by_race(g, 5, [3]),\n",
    "            'black_denied': count_by_race(g, 3, [3]),\n",
    "            'asian_denied': count_by_race(g, 2, [3]),\n",
    "            'indigenous_denied': count_by_race(g, 1, [3]),\n",
    "            'hawaiian_denied': count_by_race(g, 4, [3])\n",
    "        }))\n",
    "        .reset_index()\n",
    "    )\n",
    "    lei_state_summary['year'] = year\n",
    "    lei_state_results.append(lei_state_summary)\n",
    "\n",
    "    grouped = filtered.groupby(['state_code', 'activity_year'])\n",
    "    state_summary = grouped.apply(lambda g: pd.Series({\n",
    "        'total_all': len(g),\n",
    "        'white_accepted': count_by_race(g, 5, [1, 2]),\n",
    "        'black_accepted': count_by_race(g, 3, [1, 2]),\n",
    "        'asian_accepted': count_by_race(g, 2, [1, 2]),\n",
    "        'indigenous_accepted': count_by_race(g, 1, [1, 2]),\n",
    "        'hawaiian_accepted': count_by_race(g, 4, [1, 2]),\n",
    "        'white_denied': count_by_race(g, 5, [3]),\n",
    "        'black_denied': count_by_race(g, 3, [3]),\n",
    "        'asian_denied': count_by_race(g, 2, [3]),\n",
    "        'indigenous_denied': count_by_race(g, 1, [3]),\n",
    "        'hawaiian_denied': count_by_race(g, 4, [3])\n",
    "    })).reset_index()\n",
    "    state_results.append(state_summary)\n",
    "\n",
    "if lei_results:\n",
    "    pd.concat(lei_results).to_csv(\"lei_summary_2018_2024.csv\", index=False)\n",
    "    print(\" Saved: lei_summary_2018_2024.csv\")\n",
    "\n",
    "if lei_state_results:\n",
    "    pd.concat(lei_state_results).to_csv(\"lei_state_summary.csv\", index=False)\n",
    "    print(\" Saved: lei_state_summary.csv\")\n",
    "\n",
    "if state_results:\n",
    "    pd.concat(state_results).to_csv(\"state_year_mortgage_summary.csv\", index=False)\n",
    "    print(\" Saved: state_year_mortgage_summary.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5910937",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac8d1ac-cc89-460f-9bd4-f7d086457822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4596443",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e6270a-2391-467d-a205-3a75b5364989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"state_year_mortgage_summary.csv\")\n",
    "\n",
    "df['activity_year'] = df['activity_year'].astype(str)\n",
    "\n",
    "df_melted = df.melt(\n",
    "    id_vars=['state_code', 'activity_year'],\n",
    "    value_vars=[\n",
    "        'white_accepted',\n",
    "        'black_accepted',\n",
    "        'asian_accepted',\n",
    "        'hawaiian_accepted',\n",
    "        'indigenous_accepted'\n",
    "    ],\n",
    "    var_name='race',\n",
    "    value_name='accepted'\n",
    ")\n",
    "\n",
    "race_label_map = {\n",
    "    'white_accepted': 'White',\n",
    "    'black_accepted': 'Black',\n",
    "    'asian_accepted': 'Asian',\n",
    "    'hawaiian_accepted': 'Hawaiian/Pacific Islander',\n",
    "    'indigenous_accepted': 'Indigenous'\n",
    "}\n",
    "df_melted['race'] = df_melted['race'].map(race_label_map)\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "g = sns.FacetGrid(\n",
    "    df_melted,\n",
    "    col='state_code',\n",
    "    col_wrap=6,\n",
    "    hue='race',\n",
    "    sharey=False,\n",
    "    height=3,\n",
    "    aspect=1.4\n",
    ")\n",
    "\n",
    "g.map(sns.lineplot, 'activity_year', 'accepted', marker='o')\n",
    "g.add_legend(title='Race')\n",
    "\n",
    "for ax in g.axes.flatten():\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "g.fig.subplots_adjust(top=0.95)\n",
    "g.fig.suptitle(\"Accepted Mortgage Applications by Race and State (2018–2024)\", fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fcbc59",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c99c6b-f0ab-402c-b667-366307d49124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"state_year_mortgage_summary.csv\")\n",
    "\n",
    "df['activity_year'] = df['activity_year'].astype(str)\n",
    "\n",
    "df['total_accepted'] = df['white_accepted'] + df['black_accepted'] + df['asian_accepted']\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "g = sns.FacetGrid(\n",
    "    df,\n",
    "    col=\"state_code\",\n",
    "    col_wrap=6,\n",
    "    sharey=False,\n",
    "    height=3,\n",
    "    aspect=1.4\n",
    ")\n",
    "\n",
    "g.map_dataframe(\n",
    "    sns.lineplot,\n",
    "    x=\"activity_year\",\n",
    "    y=\"total_accepted\",\n",
    "    marker=\"o\"\n",
    ")\n",
    "\n",
    "for ax in g.axes.flatten():\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "g.fig.subplots_adjust(top=0.92)\n",
    "g.fig.suptitle(\"Total Accepted Mortgage Applications by State (2018–2024)\", fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig(\"accepted_by_race_state.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7962e71b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62aad20-c2c5-4ce2-baeb-f59a53f950dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"state_year_mortgage_summary.csv\")\n",
    "\n",
    "df['activity_year'] = df['activity_year'].astype(str)\n",
    "\n",
    "df_long = pd.melt(\n",
    "    df,\n",
    "    id_vars=[\"state_code\", \"activity_year\"],\n",
    "    value_vars=[\n",
    "        \"white_accepted\",\n",
    "        \"black_accepted\",\n",
    "        \"asian_accepted\",\n",
    "        \"hawaiian_accepted\",\n",
    "        \"indigenous_accepted\"\n",
    "    ],\n",
    "    var_name=\"Race\",\n",
    "    value_name=\"Accepted Applications\"\n",
    ")\n",
    "\n",
    "df_long[\"Race\"] = (\n",
    "    df_long[\"Race\"]\n",
    "    .str.replace(\"_accepted\", \"\")\n",
    "    .str.replace(\"hawaiian\", \"Hawaiian/Pacific Islander\", case=False)\n",
    "    .str.replace(\"indigenous\", \"Indigenous\", case=False)\n",
    "    .str.title()\n",
    ")\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "g = sns.FacetGrid(\n",
    "    df_long,\n",
    "    col=\"state_code\",\n",
    "    col_wrap=6,\n",
    "    height=3,\n",
    "    aspect=1.4,\n",
    "    sharey=False,\n",
    "    margin_titles=True\n",
    ")\n",
    "\n",
    "g.map_dataframe(\n",
    "    sns.lineplot,\n",
    "    x=\"activity_year\",\n",
    "    y=\"Accepted Applications\",\n",
    "    hue=\"Race\",\n",
    "    marker=\"o\"\n",
    ")\n",
    "\n",
    "g.add_legend(title=\"Race\")\n",
    "for ax in g.axes.flatten():\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "g.fig.subplots_adjust(top=0.92)\n",
    "g.fig.suptitle(\"Accepted Mortgage Applications by Race and State (2018–2024)\", fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"accepted_by_race_state_all_breakdown.svg\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ecefb3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e24f34-63ef-4bc5-bf80-0113ff599f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"state_year_mortgage_summary.csv\")\n",
    "\n",
    "df['activity_year'] = df['activity_year'].astype(str)\n",
    "\n",
    "df_long = pd.melt(\n",
    "    df,\n",
    "    id_vars=[\"state_code\", \"activity_year\"],\n",
    "    value_vars=[\n",
    "        \"white_accepted\",\n",
    "        \"black_accepted\",\n",
    "        \"asian_accepted\",\n",
    "        \"indigenous_accepted\",\n",
    "        \"hawaiian_accepted\"\n",
    "    ],\n",
    "    var_name=\"Race\",\n",
    "    value_name=\"Accepted Applications\"\n",
    ")\n",
    "\n",
    "race_label_map = {\n",
    "    \"white_accepted\": \"White\",\n",
    "    \"black_accepted\": \"Black\",\n",
    "    \"asian_accepted\": \"Asian\",\n",
    "    \"indigenous_accepted\": \"Indigenous\",\n",
    "    \"hawaiian_accepted\": \"Hawaiian/Pacific Islander\"\n",
    "}\n",
    "df_long[\"Race\"] = df_long[\"Race\"].map(race_label_map)\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "g = sns.FacetGrid(\n",
    "    df_long,\n",
    "    col=\"state_code\",\n",
    "    col_wrap=6,\n",
    "    height=3,\n",
    "    aspect=1.4,\n",
    "    sharey=False,\n",
    "    margin_titles=True\n",
    ")\n",
    "\n",
    "g.map_dataframe(\n",
    "    sns.lineplot,\n",
    "    x=\"activity_year\",\n",
    "    y=\"Accepted Applications\",\n",
    "    hue=\"Race\",\n",
    "    marker=\"o\"\n",
    ")\n",
    "\n",
    "g.add_legend(title=\"Race\")\n",
    "for ax in g.axes.flatten():\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "g.fig.subplots_adjust(top=0.92)\n",
    "g.fig.suptitle(\"Accepted Mortgage Applications by Race and State (2018–2024)\", fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"accepted_by_race_state_all_breakdown.svg\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f8095c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c653f841-97c0-458f-af40-236307e2b3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "unfiltered_folder = \"unfiltered\"\n",
    "years = list(range(2018, 2025))\n",
    "unfiltered_files = [f\"{y}_hmda.csv\" for y in years]\n",
    "\n",
    "race_labels = {\n",
    "    1: \"American Indian or Alaska Native\",\n",
    "    2: \"Asian\", 21: \"Asian\", 22: \"Asian\", 23: \"Asian\", 24: \"Asian\", 25: \"Asian\", 26: \"Asian\", 27: \"Asian\",\n",
    "    3: \"Black or African American\",\n",
    "    4: \"Native Hawaiian or Pacific Islander\", 41: \"Native Hawaiian or Pacific Islander\",\n",
    "    42: \"Native Hawaiian or Pacific Islander\", 43: \"Native Hawaiian or Pacific Islander\", 44: \"Native Hawaiian or Pacific Islander\",\n",
    "    5: \"White\",\n",
    "    6: \"Info Not Provided\",\n",
    "    7: \"Not Applicable\"\n",
    "}\n",
    "\n",
    "all_years = []\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    file_path = os.path.join(unfiltered_folder, unfiltered_files[i])\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"️ Missing file for {year}: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing {year}...\")\n",
    "\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "    \n",
    "    race_col = 'applicant_race-1' if 'applicant_race-1' in df.columns else 'applicant_race_1'\n",
    "    \n",
    "    df = df[df['action_taken'].isin([1, 2, 3])]\n",
    "    \n",
    "    df[\"Race\"] = df[race_col].map(race_labels).fillna(\"Other/Unknown\")\n",
    "    df[\"activity_year\"] = year\n",
    "\n",
    "    df[\"Decision\"] = df[\"action_taken\"].map({1: \"Approved\", 2: \"Approved\", 3: \"Denied\"})\n",
    "\n",
    "    grouped = (\n",
    "        df.groupby([\"activity_year\", \"state_code\", \"lei\", \"Race\", \"Decision\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"count\")\n",
    "    )\n",
    "\n",
    "    all_years.append(grouped)\n",
    "\n",
    "if all_years:\n",
    "    final_df = pd.concat(all_years, ignore_index=True)\n",
    "    final_df.to_csv(\"race_action_breakdown_by_year_state_lei.csv\", index=False)\n",
    "    print(\" Exported: race_action_breakdown_by_year_state_lei.csv\")\n",
    "else:\n",
    "    print(\" No data processed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f130936d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31146007-beee-4dde-a7a7-4026fbd86d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"race_action_breakdown_by_year_state_lei.csv\")\n",
    "\n",
    "df = df[df[\"Decision\"] == \"Denied\"]\n",
    "\n",
    "df_grouped = df.groupby([\"activity_year\", \"state_code\", \"Race\"])[\"count\"].sum().reset_index()\n",
    "\n",
    "df_grouped[\"activity_year\"] = df_grouped[\"activity_year\"].astype(str)\n",
    "\n",
    "df_grouped[\"Race\"] = df_grouped[\"Race\"].replace({\n",
    "    \"Black or African American\": \"Black\",\n",
    "    \"American Indian or Alaska Native\": \"Indigenous\",\n",
    "    \"Native Hawaiian or Pacific Islander\": \"Hawaiian/Pacific Islander\"\n",
    "})\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "g = sns.FacetGrid(\n",
    "    df_grouped,\n",
    "    col=\"state_code\",\n",
    "    col_wrap=6,\n",
    "    height=3,\n",
    "    aspect=1.4,\n",
    "    sharey=False,\n",
    "    margin_titles=True\n",
    ")\n",
    "\n",
    "g.map_dataframe(\n",
    "    sns.lineplot,\n",
    "    x=\"activity_year\",\n",
    "    y=\"count\",\n",
    "    hue=\"Race\",\n",
    "    marker=\"o\"\n",
    ")\n",
    "\n",
    "g.add_legend(title=\"Race\")\n",
    "for ax in g.axes.flatten():\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "g.fig.subplots_adjust(top=0.92)\n",
    "g.fig.suptitle(\"Denied Mortgage Applications by Race and State (2018–2024)\", fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"denied_by_race_state_all_breakdown.svg\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6824d2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9cddf0-795b-49d7-90c0-74525ea72f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"lei_state_summary.csv\")\n",
    "\n",
    "lei_dict = {\n",
    "    \"B4TYDEB6GKMZO031MB27\": \"Bank of America\",\n",
    "    \"7H6GLXDRUGQFU57RNE97\": \"JP Morgan\",\n",
    "    \"KB1H1DSPRFMYMCUFXT09\": \"Wells Fargo\",\n",
    "    \"E57ODZWZ7FF32TWEFA76\": \"Citi Bank\"\n",
    "}\n",
    "\n",
    "df_filtered = df[df['lei'].isin(lei_dict.keys())].copy()\n",
    "\n",
    "df_filtered[\"Bank\"] = df_filtered[\"lei\"].map(lei_dict)\n",
    "\n",
    "df_filtered['year'] = df_filtered['year'].astype(str)\n",
    "\n",
    "df_long = pd.melt(\n",
    "    df_filtered,\n",
    "    id_vars=[\"Bank\", \"state_code\", \"year\"],\n",
    "    value_vars=[\n",
    "        \"white_accepted\",\n",
    "        \"black_accepted\",\n",
    "        \"asian_accepted\",\n",
    "        \"hawaiian_accepted\",\n",
    "        \"indigenous_accepted\"\n",
    "    ],\n",
    "    var_name=\"Race\",\n",
    "    value_name=\"Accepted Applications\"\n",
    ")\n",
    "\n",
    "df_long[\"Race\"] = (\n",
    "    df_long[\"Race\"]\n",
    "    .str.replace(\"_accepted\", \"\", case=False)\n",
    "    .str.replace(\"hawaiian\", \"Hawaiian/Pacific Islander\", case=False)\n",
    "    .str.replace(\"indigenous\", \"Indigenous\", case=False)\n",
    "    .str.title()\n",
    ")\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "g = sns.FacetGrid(\n",
    "    df_long,\n",
    "    col=\"Bank\",\n",
    "    col_wrap=2,\n",
    "    height=4,\n",
    "    aspect=1.2,\n",
    "    sharey=False,\n",
    "    margin_titles=True\n",
    ")\n",
    "\n",
    "g.map_dataframe(\n",
    "    sns.lineplot,\n",
    "    x=\"year\",\n",
    "    y=\"Accepted Applications\",\n",
    "    hue=\"Race\",\n",
    "    marker=\"o\",\n",
    "    ci=None\n",
    ")\n",
    "\n",
    "g.add_legend(title=\"Race\")\n",
    "for ax in g.axes.flatten():\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "g.fig.subplots_adjust(top=0.9)\n",
    "g.fig.suptitle(\"Accepted Applications by Race for Top 4 Banks (2018–2024)\", fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"top4banks_race_acceptance_all.svg\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d93318",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f02df3-0d78-4cba-ac58-0d8c5893e91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"lei_state_summary.csv\")\n",
    "\n",
    "lei_dict = {\n",
    "    \"B4TYDEB6GKMZO031MB27\": \"Bank of America\",\n",
    "    \"7H6GLXDRUGQFU57RNE97\": \"JP Morgan\",\n",
    "    \"KB1H1DSPRFMYMCUFXT09\": \"Wells Fargo\",\n",
    "    \"E57ODZWZ7FF32TWEFA76\": \"Citi Bank\"\n",
    "}\n",
    "\n",
    "df[\"Bank\"] = df[\"lei\"].map(lei_dict)\n",
    "df = df[df[\"Bank\"].notna()].copy()\n",
    "\n",
    "df[\"year\"] = df[\"year\"].astype(int)\n",
    "\n",
    "epsilon = 1e-6\n",
    "\n",
    "df[\"white_total_apps\"] = df[\"white_accepted\"] + df[\"white_denied\"]\n",
    "df[\"black_total_apps\"] = df[\"black_accepted\"] + df[\"black_denied\"]\n",
    "\n",
    "df[\"white_rate\"] = df[\"white_accepted\"] / (df[\"white_total_apps\"] + epsilon)\n",
    "df[\"black_rate\"] = df[\"black_accepted\"] / (df[\"black_total_apps\"] + epsilon)\n",
    "\n",
    "agg_df = df.groupby([\"Bank\", \"year\"])[[\"white_rate\", \"black_rate\"]].mean().reset_index()\n",
    "\n",
    "banks = agg_df[\"Bank\"].unique()\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, bank in enumerate(banks):\n",
    "    data = agg_df[agg_df[\"Bank\"] == bank].sort_values(\"year\")\n",
    "    \n",
    "    years = data[\"year\"]\n",
    "    white = data[\"white_rate\"] * 100  # convert to %\n",
    "    black = data[\"black_rate\"] * 100\n",
    "    \n",
    "    ax = axes[i]\n",
    "    \n",
    "    ax.fill_between(years, white, black, where=(white > black), color=\"#fde0dd\", label=\"Gap\", alpha=0.6)\n",
    "    \n",
    "    ax.plot(years, white, marker='o', color=\"gray\", label=\"White Approval Rate\")\n",
    "    ax.plot(years, black, marker='o', color=\"black\", label=\"Black Approval Rate\")\n",
    "    \n",
    "    ax.set_title(bank)\n",
    "    ax.set_xlabel(\"Year\")\n",
    "    ax.set_ylabel(\"Approval Rate (%)\")\n",
    "    ax.set_ylim(50, 100)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.legend()\n",
    "\n",
    "fig.suptitle(\"Black–White Mortgage Approval Rate Gap by Bank (2018–2024)\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.savefig(\"approval_rate_gap_by_bank.svg\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dd49e1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bf9d3b-66ec-4424-a29a-23085acec87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"lei_state_summary.csv\")\n",
    "\n",
    "lei_dict = {\n",
    "    \"B4TYDEB6GKMZO031MB27\": \"Bank of America\",\n",
    "    \"7H6GLXDRUGQFU57RNE97\": \"JP Morgan\", \n",
    "    \"KB1H1DSPRFMYMCUFXT09\": \"Wells Fargo\",\n",
    "    \"E57ODZWZ7FF32TWEFA76\": \"Citi Bank\",\n",
    "    \"549300FGXN1K3HLB1R50\": \"Rocket\",        # Added Rocket\n",
    "    \"549300HW662MN1WU8550\": \"UWM\"            # Added UWM\n",
    "}\n",
    "\n",
    "df[\"Bank\"] = df[\"lei\"].map(lei_dict)\n",
    "df = df[df[\"Bank\"].notna()].copy()\n",
    "\n",
    "print(f\"Banks found in data: {sorted(df['Bank'].unique())}\")\n",
    "print(f\"Years available: {sorted(df['year'].unique())}\")\n",
    "print(f\"Total records: {len(df)}\")\n",
    "\n",
    "df[\"year\"] = df[\"year\"].astype(int)\n",
    "\n",
    "epsilon = 1e-6\n",
    "\n",
    "df[\"white_total_apps\"] = df[\"white_accepted\"] + df[\"white_denied\"]\n",
    "df[\"black_total_apps\"] = df[\"black_accepted\"] + df[\"black_denied\"]\n",
    "\n",
    "agg_df = df.groupby([\"Bank\", \"year\"]).apply(lambda g: pd.Series({\n",
    "    \"white_accepted_sum\": g[\"white_accepted\"].sum(),\n",
    "    \"white_total_sum\": g[\"white_total_apps\"].sum(),\n",
    "    \"black_accepted_sum\": g[\"black_accepted\"].sum(),\n",
    "    \"black_total_sum\": g[\"black_total_apps\"].sum(),\n",
    "})).reset_index()\n",
    "\n",
    "agg_df[\"white_rate\"] = agg_df[\"white_accepted_sum\"] / (agg_df[\"white_total_sum\"] + epsilon)\n",
    "agg_df[\"black_rate\"] = agg_df[\"black_accepted_sum\"] / (agg_df[\"black_total_sum\"] + epsilon)\n",
    "\n",
    "print(f\"\\nData availability by bank:\")\n",
    "for bank in sorted(agg_df[\"Bank\"].unique()):\n",
    "    bank_data = agg_df[agg_df[\"Bank\"] == bank]\n",
    "    years = sorted(bank_data[\"year\"].unique())\n",
    "    print(f\"  {bank}: {years}\")\n",
    "\n",
    "banks = sorted(agg_df[\"Bank\"].unique())\n",
    "n_banks = len(banks)\n",
    "\n",
    "if n_banks <= 4:\n",
    "    ncols = 2\n",
    "    nrows = (n_banks + 1) // 2\n",
    "elif n_banks <= 6:\n",
    "    ncols = 3\n",
    "    nrows = (n_banks + 2) // 3\n",
    "else:\n",
    "    ncols = 3\n",
    "    nrows = (n_banks + 2) // 3\n",
    "\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(5*ncols, 4*nrows))\n",
    "\n",
    "if n_banks == 1:\n",
    "    axes = [axes]\n",
    "elif nrows == 1:\n",
    "    axes = axes if hasattr(axes, '__len__') else [axes]\n",
    "else:\n",
    "    axes = axes.flatten()\n",
    "\n",
    "while len(axes) < n_banks:\n",
    "    axes.append(None)\n",
    "\n",
    "for i, bank in enumerate(banks):\n",
    "    data = agg_df[agg_df[\"Bank\"] == bank].sort_values(\"year\")\n",
    "    \n",
    "    if data.empty:\n",
    "        continue\n",
    "        \n",
    "    years = data[\"year\"]\n",
    "    white = data[\"white_rate\"] * 100  # convert to %\n",
    "    black = data[\"black_rate\"] * 100\n",
    "    \n",
    "    ax = axes[i]\n",
    "    \n",
    "    ax.fill_between(years, white, black, where=(white > black), \n",
    "                    color=\"#fde0dd\", label=\"Gap\", alpha=0.6)\n",
    "    \n",
    "    ax.plot(years, white, marker='o', color=\"gray\", label=\"White Approval Rate\", linewidth=2)\n",
    "    ax.plot(years, black, marker='o', color=\"black\", label=\"Black Approval Rate\", linewidth=2)\n",
    "    \n",
    "    ax.set_title(bank, fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel(\"Year\")\n",
    "    ax.set_ylabel(\"Approval Rate (%)\")\n",
    "    ax.set_ylim(50, 100)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    if len(data) > 0:\n",
    "        latest_year = data.iloc[-1]\n",
    "        gap = (latest_year[\"white_rate\"] - latest_year[\"black_rate\"]) * 100\n",
    "        ax.text(0.05, 0.95, f'Latest Gap: {gap:.1f}%', \n",
    "                transform=ax.transAxes, fontsize=9, \n",
    "                verticalalignment='top', \n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "for j in range(n_banks, len(axes)):\n",
    "    if axes[j] is not None:\n",
    "        axes[j].set_visible(False)\n",
    "\n",
    "fig.suptitle(\"Black–White Mortgage Approval Rate Gap by Bank (2018–2024)\", \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "plt.savefig(\"approval_rate_gap_by_bank_updated.svg\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n SUMMARY STATISTICS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "latest_year = agg_df[\"year\"].max()\n",
    "latest_data = agg_df[agg_df[\"year\"] == latest_year]\n",
    "\n",
    "print(f\"\\nLatest year ({latest_year}) approval rates and gaps:\")\n",
    "for _, row in latest_data.iterrows():\n",
    "    white_pct = row[\"white_rate\"] * 100\n",
    "    black_pct = row[\"black_rate\"] * 100\n",
    "    gap = white_pct - black_pct\n",
    "    print(f\"  {row['Bank']}:\")\n",
    "    print(f\"    White: {white_pct:.1f}% | Black: {black_pct:.1f}% | Gap: {gap:.1f}%\")\n",
    "\n",
    "print(f\"\\nOverall statistics across all years:\")\n",
    "for bank in sorted(banks):\n",
    "    bank_data = agg_df[agg_df[\"Bank\"] == bank]\n",
    "    avg_white = bank_data[\"white_rate\"].mean() * 100\n",
    "    avg_black = bank_data[\"black_rate\"].mean() * 100\n",
    "    avg_gap = avg_white - avg_black\n",
    "    print(f\"  {bank}: Avg White: {avg_white:.1f}% | Avg Black: {avg_black:.1f}% | Avg Gap: {avg_gap:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28de55d3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60105cb-3c5c-43a7-9cde-b131e8ce227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_folder = \"filtered_data_strict\"  # path to your filtered HMDA CSVs\n",
    "years = range(2018, 2025)\n",
    "\n",
    "lei_to_name = {\n",
    "    \"549300HW662MN1WU8550\": \"United Wholesale Mortgage\",\n",
    "    \"549300FGXN1K3HLB1R50\": \"Rocket Mortgage\",\n",
    "    \"KB1H1DSPRFMYMCUFXT09\": \"Wells Fargo\",\n",
    "    \"549300MGPZBLQDIL7538\": \"Fairway Mortgage\",\n",
    "    \"7H6GLXDRUGQFU57RNE97\": \"JPMorgan Chase\",\n",
    "    \"B4TYDEB6GKMZO031MB27\": \"Bank of America\",\n",
    "    \"549300VZVN841I2ILS84\": \"CrossCountry Mortgage\",\n",
    "    \"549300AG64NHILB7ZP05\": \"loanDepot\",\n",
    "    \"549300U3721PJGQZYY68\": \"Guaranteed Rate\",\n",
    "    \"549300J7XKT2BI5WX213\": \"Caliber Home Loans\"\n",
    "}\n",
    "\n",
    "target_leis = list(lei_to_name.keys())\n",
    "race_map = {3.0: 'Black', 5.0: 'White'}\n",
    "epsilon = 1e-6\n",
    "results = []\n",
    "\n",
    "for year in years:\n",
    "    filepath = os.path.join(data_folder, f\"{year}_filtered_hmda.csv\")\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Missing: {filepath}\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(filepath, low_memory=False)\n",
    "    race_col = 'applicant_race-1' if 'applicant_race-1' in df.columns else 'applicant_race_1'\n",
    "\n",
    "    df = df[df['lei'].isin(target_leis)]\n",
    "    df = df[df[race_col].isin(race_map.keys())]\n",
    "    df['race'] = df[race_col].map(race_map)\n",
    "\n",
    "    df = df[df['action_taken'].isin([1, 2, 3])]\n",
    "\n",
    "    grouped = df.groupby(['lei', 'race'])['action_taken'].value_counts().unstack(fill_value=0)\n",
    "    grouped['total'] = grouped.sum(axis=1)\n",
    "    grouped['approved'] = grouped.get(1, 0) + grouped.get(2, 0)\n",
    "    grouped['approval_rate'] = grouped['approved'] / (grouped['total'] + epsilon)\n",
    "    grouped = grouped.reset_index()\n",
    "    grouped['year'] = year\n",
    "    results.append(grouped)\n",
    "\n",
    "all_df = pd.concat(results)\n",
    "pivoted = all_df.pivot_table(index=['lei', 'year'], columns='race', values='approval_rate').reset_index()\n",
    "pivoted['gap'] = pivoted['White'] - pivoted['Black']\n",
    "pivoted['Lender'] = pivoted['lei'].map(lei_to_name)\n",
    "\n",
    "ncols = 3\n",
    "nrows = -(-len(target_leis) // ncols)\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(16, 4 * nrows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (lei, lender) in enumerate(lei_to_name.items()):\n",
    "    data = pivoted[pivoted['lei'] == lei].sort_values('year')\n",
    "    if data.empty:\n",
    "        continue\n",
    "\n",
    "    years = data['year']\n",
    "    white = data['White'] * 100\n",
    "    black = data['Black'] * 100\n",
    "\n",
    "    ax = axes[i]\n",
    "    ax.fill_between(years, white, black, where=(white > black), color=\"#fde0dd\", label=\"Gap\", alpha=0.6)\n",
    "    ax.plot(years, white, marker='o', color='gray', label='White Approval Rate')\n",
    "    ax.plot(years, black, marker='o', color='black', label='Black Approval Rate')\n",
    "\n",
    "    ax.set_title(lender, fontsize=10)\n",
    "    ax.set_ylim(50, 100)\n",
    "    ax.set_xlabel(\"Year\")\n",
    "    ax.set_ylabel(\"Approval Rate (%)\")\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.legend(fontsize=7)\n",
    "\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "fig.suptitle(\"Black–White Mortgage Approval Rate Gap by Top Lenders (2018–2024)\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.savefig(\"approval_gap_top_lenders.svg\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3117ab5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b0fd70-25de-42b8-92b9-cd3dc662f336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "774a370d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f947475-c662-4d80-b624-2778948c302b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c494198c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d98fe4-9e0b-4d76-abc3-bc11e142a264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b1803c4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0eb5e5-8f20-4d32-9ecb-8c29c0e66ceb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "613dc0ac",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff01fca-cdf3-4da9-b56a-f100c801cbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"lei_state_summary.csv\")\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b8c0ee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb82938-febf-4683-a257-b5b861ba94b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "lei_dict = {\n",
    "    \"B4TYDEB6GKMZO031MB27\": \"Bank of America\",\n",
    "    \"7H6GLXDRUGQFU57RNE97\": \"JP Morgan\",\n",
    "    \"KB1H1DSPRFMYMCUFXT09\": \"Wells Fargo\",\n",
    "    \"E57ODZWZ7FF32TWEFA76\": \"Citi Bank\"\n",
    "}\n",
    "\n",
    "unfiltered_folder = \"unfiltered\"\n",
    "\n",
    "years = list(range(2018, 2025))\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for year in years:\n",
    "    file_path = os.path.join(unfiltered_folder, f\"{year}_hmda.csv\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\" File not found: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "    \n",
    "    df = df[df['lei'].isin(lei_dict.keys())].copy()\n",
    "    df['Bank'] = df['lei'].map(lei_dict)\n",
    "    df['year'] = str(year)\n",
    "\n",
    "    summary = (\n",
    "        df.groupby(\"Bank\")\n",
    "        .size()\n",
    "        .reset_index(name=\"Total Applications\")\n",
    "    )\n",
    "    summary['year'] = str(year)\n",
    "\n",
    "    all_data.append(summary)\n",
    "\n",
    "df_totals = pd.concat(all_data)\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "g = sns.FacetGrid(\n",
    "    df_totals,\n",
    "    col=\"Bank\",\n",
    "    col_wrap=2,\n",
    "    height=4,\n",
    "    aspect=1.2,\n",
    "    sharey=False\n",
    ")\n",
    "\n",
    "g.map_dataframe(\n",
    "    sns.lineplot,\n",
    "    x=\"year\",\n",
    "    y=\"Total Applications\",\n",
    "    marker=\"o\",\n",
    "    color=\"darkgreen\"\n",
    ")\n",
    "\n",
    "for ax in g.axes.flatten():\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "g.fig.subplots_adjust(top=0.88)\n",
    "g.fig.suptitle(\"Total Mortgage Applications by Bank (2018–2024)\", fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"top4banks_total_applications_by_year.svg\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cd5e78",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7274dd70-3f28-46c3-83f6-edf8d8d31adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2bc4676c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15459a6-47f5-4e77-b6da-2927e8ae4a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "e2016 = pd.read_csv(\"election results/2016_US_County_Level_Presidential_Results.csv\", dtype={'county_fips':str})\n",
    "e2020 = pd.read_csv(\"election results/2020_US_County_Level_Presidential_Results.csv\", dtype={'county_fips':str})\n",
    "\n",
    "e2016 = e2016.loc[:, ~e2016.columns.str.contains('^Unnamed')]\n",
    "e2016['per_point_diff'] = e2016['per_point_diff'].str.replace('%','').astype(float)\n",
    "e2016.drop_duplicates(subset='county_fips', inplace=True)\n",
    "e2016['year'] = 2016\n",
    "e2016 = e2016.rename(columns={\n",
    "    'state_abbr': 'state_code',\n",
    "    'per_point_diff': 'gop_margin_pct'\n",
    "})\n",
    "\n",
    "e2020['year'] = 2020\n",
    "e2020 = e2020.rename(columns={\n",
    "    'state_name': 'state',\n",
    "    'per_point_diff': 'gop_margin_pct'\n",
    "})\n",
    "e2020['state_code'] = e2020['state']  # assuming state codes match abbreviations\n",
    "\n",
    "keep = ['county_fips', 'state_code', 'votes_dem', 'votes_gop', 'gop_margin_pct', 'year']\n",
    "e2016 = e2016[keep]\n",
    "e2020 = e2020[keep]\n",
    "\n",
    "election = pd.concat([e2016, e2020], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554d8ebe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9b1e73-fab0-4bc1-97b2-2c469f88a9a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14170809",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320ea16b-19f1-462d-be96-533c726db305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_2016 = pd.read_csv(\"election results/2016_US_County_Level_Presidential_Results.csv\", dtype={\"combined_fips\": str})\n",
    "df_2020 = pd.read_csv(\"election results/2020_US_County_Level_Presidential_Results.csv\", dtype={\"county_fips\": str})\n",
    "\n",
    "df_2016 = df_2016.rename(columns={\"combined_fips\": \"county_fips\", \"per_dem\": \"per_democrat\", \"per_gop\": \"per_republican\"})\n",
    "df_2020 = df_2020.rename(columns={\"county_fips\": \"county_fips\", \"per_dem\": \"per_democrat\", \"per_gop\": \"per_republican\"})\n",
    "\n",
    "df_2016[\"election_year\"] = 2016\n",
    "df_2020[\"election_year\"] = 2020\n",
    "\n",
    "df_2016[\"mortgage_years\"] = [[2018, 2019, 2020]] * len(df_2016)\n",
    "df_2020[\"mortgage_years\"] = [[2021, 2022, 2023, 2024]] * len(df_2020)\n",
    "\n",
    "df_combined = pd.concat([df_2016, df_2020], ignore_index=True)\n",
    "df_exploded = df_combined.explode(\"mortgage_years\").rename(columns={\"mortgage_years\": \"year\"})\n",
    "\n",
    "df_final = df_exploded[[\"county_fips\", \"year\", \"per_democrat\", \"per_republican\"]]\n",
    "\n",
    "df_final.to_csv(\"merged_election_results.csv\", index=False)\n",
    "print(\" Saved merged_election_results.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84253e60",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da052173-1a46-4ce7-bb40-ab69a8bb0a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "approval = pd.read_csv(\"tract_level_approval_rates.csv\", dtype=str)\n",
    "approval.columns = approval.columns.str.strip()\n",
    "approval['county_fips'] = approval['tract_fips'].str[:5]  # Extract county-level FIPS\n",
    "\n",
    "election = pd.read_csv(\"merged_election_results.csv\", dtype=str)\n",
    "election.columns = election.columns.str.strip()\n",
    "\n",
    "merged = approval.merge(election, on=['county_fips', 'year'], how='left')\n",
    "\n",
    "merged.to_csv(\"tract_approval_election_merged.csv\", index=False)\n",
    "print(\" Saved tract_approval_election_merged.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829d88dd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751450bb-cbe1-4710-8856-b44209c73f21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae47f749",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b49c4b-6eb5-405e-a368-0f0d721b66f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"tract_approval_election_merged.csv\")\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8722571b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8945f2-a718-43d8-a554-68356e01a16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"tract_approval_election_merged.csv\", dtype={\"county_fips\": str})\n",
    "\n",
    "df[\"state_fips\"] = df[\"county_fips\"].str[:2]\n",
    "\n",
    "state_race_rates = (\n",
    "    df.groupby([\"state_fips\", \"race_label\"])\n",
    "      .agg(approval_rate=('approval_rate', 'mean'),\n",
    "           total_applications=('total', 'sum'))\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "state_race_rates.to_csv(\"approval_rates_by_state_race_with_election.csv\", index=False)\n",
    "print(\" Saved: approval_rates_by_state_race.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbc1dc3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0122fe0-fcf7-4ef7-852c-0af0a6b4779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"tract_approval_election_merged.csv\", dtype={'county_fips': str})\n",
    "\n",
    "print(\"Non-null race_label counts by year:\")\n",
    "print(df[df['race_label'].notna()].groupby('year')['race_label'].count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de828112",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1041d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"filtered_data_strict/2018_filtered_hmda.csv\", low_memory=False)\n",
    "\n",
    "if 'applicant_race-1' in df.columns:\n",
    "    race_col = 'applicant_race-1'\n",
    "elif 'applicant_race_1' in df.columns:\n",
    "    race_col = 'applicant_race_1'\n",
    "else:\n",
    "    raise ValueError(\" Race column not found in the dataset\")\n",
    "\n",
    "race_groups = {\n",
    "    'White': [5.0],\n",
    "    'Black': [3.0],\n",
    "    'Indigenous': [1.0],  # American Indian or Alaska Native\n",
    "    'Asian': [2.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0],\n",
    "    'Hawaiian': [4.0, 41.0, 42.0, 43.0, 44.0],  # Native Hawaiian or Other Pacific Islander\n",
    "    'Unknown': [6.0, 7.0]\n",
    "}\n",
    "\n",
    "def map_race(code):\n",
    "    for label, codes in race_groups.items():\n",
    "        if code in codes:\n",
    "            return label\n",
    "    return 'Other'\n",
    "\n",
    "df[race_col] = pd.to_numeric(df[race_col], errors='coerce')\n",
    "df['race_label'] = df[race_col].apply(map_race)\n",
    "\n",
    "print(\" Race label breakdown:\")\n",
    "print(df['race_label'].value_counts())\n",
    "\n",
    "df.to_csv(\"2018_filtered_race_mapped.csv\", index=False)\n",
    "print(\" Saved: 2018_filtered_race_mapped.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282b6727",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2c96a5-b829-4653-bf53-e7a7f4549473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_folder = \"filtered_data_strict\"\n",
    "\n",
    "years = list(range(2018, 2025))\n",
    "\n",
    "bank_leis = {\n",
    "    \"B4TYDEB6GKMZO031MB27\": \"Bank of America\",\n",
    "    \"7H6GLXDRUGQFU57RNE97\": \"JPMorgan\",\n",
    "    \"KB1H1DSPRFMYMCUFXT09\": \"Wells Fargo\",\n",
    "    \"E57ODZWZ7FF32TWEFA76\": \"Citi\"\n",
    "}\n",
    "\n",
    "def classify_institution(lei):\n",
    "    return \"Bank\" if lei in bank_leis else \"Non-Bank\"\n",
    "\n",
    "summary_records = []\n",
    "\n",
    "for year in years:\n",
    "    file_path = os.path.join(data_folder, f\"{year}_filtered_hmda.csv\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"️ File not found: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing {year}…\")\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "    race_col = \"applicant_race-1\" if \"applicant_race-1\" in df.columns else \"applicant_race_1\"\n",
    "    lei_col = \"lei\" if \"lei\" in df.columns else \"respondent_id\"\n",
    "\n",
    "    race_map = {3.0: \"Black\", 5.0: \"White\"}\n",
    "    df = df[df[race_col].isin(race_map.keys())]\n",
    "    df[\"race_label\"] = df[race_col].map(race_map)\n",
    "\n",
    "    df[\"institution_type\"] = df[lei_col].apply(classify_institution)\n",
    "\n",
    "    approved_actions = [1]\n",
    "    df[\"is_approved\"] = df[\"action_taken\"].isin(approved_actions)\n",
    "\n",
    "    grouped = (\n",
    "        df.groupby([\"activity_year\", \"institution_type\", \"race_label\"])\n",
    "          .agg(total_applications=(\"is_approved\", \"size\"),\n",
    "               approvals=(\"is_approved\", \"sum\"))\n",
    "          .reset_index()\n",
    "    )\n",
    "    grouped[\"approval_rate\"] = grouped[\"approvals\"] / grouped[\"total_applications\"]\n",
    "\n",
    "    summary_records.append(grouped)\n",
    "\n",
    "summary_df = pd.concat(summary_records, ignore_index=True)\n",
    "\n",
    "output_path = \"nonbank_summary.csv\"\n",
    "summary_df.to_csv(output_path, index=False)\n",
    "print(f\"Done. Summary saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773cdcfe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a622e3-7b30-42b0-a67f-6b530ddcb81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_folder = \"filtered_data_strict\"\n",
    "years = list(range(2018, 2025))\n",
    "\n",
    "bank_leis = {\n",
    "    \"B4TYDEB6GKMZO031MB27\": \"Bank of America\",\n",
    "    \"7H6GLXDRUGQFU57RNE97\": \"JPMorgan Chase\",\n",
    "    \"KB1H1DSPRFMYMCUFXT09\": \"Wells Fargo\",\n",
    "    \"E57ODZWZ7FF32TWEFA76\": \"Citibank\",\n",
    "}\n",
    "\n",
    "race_map = {\n",
    "    1.0: \"Native American\",\n",
    "    2.0: \"Asian\",\n",
    "    3.0: \"Black\",\n",
    "    4.0: \"Pacific Islander\",\n",
    "    5.0: \"White\",\n",
    "    7.0: \"Two or More Races\"\n",
    "}\n",
    "\n",
    "summary = []\n",
    "\n",
    "for year in years:\n",
    "    path = os.path.join(data_folder, f\"{year}_filtered_hmda.csv\")\n",
    "    if not os.path.exists(path):\n",
    "        continue\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "\n",
    "    race_col = \"applicant_race-1\" if \"applicant_race-1\" in df.columns else \"applicant_race_1\"\n",
    "\n",
    "    df = df[df[race_col].isin(race_map.keys())]\n",
    "    df[\"race_label\"] = df[race_col].map(race_map)\n",
    "\n",
    "    df = df[df[\"lei\"].isin(bank_leis.keys())]\n",
    "    df[\"bank_name\"] = df[\"lei\"].map(bank_leis)\n",
    "\n",
    "    df[\"is_approved\"] = df[\"action_taken\"] == 1\n",
    "\n",
    "    group = (\n",
    "        df.groupby([\"activity_year\", \"bank_name\", \"race_label\"])\n",
    "          .agg(total_applications=(\"is_approved\", \"size\"),\n",
    "               approvals=(\"is_approved\", \"sum\"))\n",
    "          .reset_index()\n",
    "    )\n",
    "    group[\"approval_rate\"] = group[\"approvals\"] / group[\"total_applications\"]\n",
    "    summary.append(group)\n",
    "\n",
    "bank_race_summary = pd.concat(summary, ignore_index=True)\n",
    "bank_race_summary.to_csv(\"bank_race_summary.csv\", index=False)\n",
    "print(\" Created bank_race_summary.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fe73bf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f903502-362c-4648-9fe5-11bd16c23a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"tract_approval_election_merged.csv\", dtype={\"county_fips\": str})\n",
    "\n",
    "df[\"state_fips\"] = df[\"county_fips\"].str[:2]\n",
    "state_fips_to_code = {\n",
    "    \"01\": \"AL\", \"02\": \"AK\", \"04\": \"AZ\", \"05\": \"AR\", \"06\": \"CA\", \"08\": \"CO\",\n",
    "    \"09\": \"CT\", \"10\": \"DE\", \"11\": \"DC\", \"12\": \"FL\", \"13\": \"GA\", \"15\": \"HI\",\n",
    "    \"16\": \"ID\", \"17\": \"IL\", \"18\": \"IN\", \"19\": \"IA\", \"20\": \"KS\", \"21\": \"KY\",\n",
    "    \"22\": \"LA\", \"23\": \"ME\", \"24\": \"MD\", \"25\": \"MA\", \"26\": \"MI\", \"27\": \"MN\",\n",
    "    \"28\": \"MS\", \"29\": \"MO\", \"30\": \"MT\", \"31\": \"NE\", \"32\": \"NV\", \"33\": \"NH\",\n",
    "    \"34\": \"NJ\", \"35\": \"NM\", \"36\": \"NY\", \"37\": \"NC\", \"38\": \"ND\", \"39\": \"OH\",\n",
    "    \"40\": \"OK\", \"41\": \"OR\", \"42\": \"PA\", \"44\": \"RI\", \"45\": \"SC\", \"46\": \"SD\",\n",
    "    \"47\": \"TN\", \"48\": \"TX\", \"49\": \"UT\", \"50\": \"VT\", \"51\": \"VA\", \"53\": \"WA\",\n",
    "    \"54\": \"WV\", \"55\": \"WI\", \"56\": \"WY\"\n",
    "}\n",
    "df[\"state_code\"] = df[\"state_fips\"].map(state_fips_to_code)\n",
    "\n",
    "df[\"political_lean\"] = df.apply(\n",
    "    lambda row: \"Republican\" if row[\"per_republican\"] > row[\"per_democrat\"] else \"Democrat\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df = df.dropna(subset=[\"approval_rate\", \"race_label\", \"state_code\"])\n",
    "\n",
    "group = (\n",
    "    df.groupby([\"year\", \"state_code\", \"political_lean\", \"race_label\"])\n",
    "      .agg(avg_approval_rate=(\"approval_rate\", \"mean\"),\n",
    "           total_applications=(\"total\", \"sum\"))\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "group.to_csv(\"election_race_summary.csv\", index=False)\n",
    "print(\" Created election_race_summary.csv with state_code and political_lean\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3124236e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b18399-23df-4d15-bd05-48c4aafc2426",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"tract_approval_election_merged.csv\", dtype={\"county_fips\": str})\n",
    "\n",
    "df[\"election_result\"] = df.apply(\n",
    "    lambda row: \"Trump\" if row[\"per_republican\"] > row[\"per_democrat\"] else \"Biden\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "group = (\n",
    "    df.groupby([\"year\", \"election_result\", \"race_label\"])\n",
    "      .agg(avg_approval_rate=(\"approval_rate\", \"mean\"),\n",
    "           total_applications=(\"total\", \"sum\"))\n",
    "      .reset_index()\n",
    ")\n",
    "group.to_csv(\"election_race_summary.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f130ff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ab396d-f00d-4fe1-8bb8-6376603cb353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dbdbe79b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9c0f46-f2ff-478e-b087-5ec6e580fdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "state_df = pd.read_csv('state_race_summary.csv')\n",
    "\n",
    "sub = state_df[state_df['race_label'].isin(['Black', 'White'])]\n",
    "\n",
    "pivot = sub.pivot_table(index=['activity_year', 'state_code'],\n",
    "                        columns='race_label',\n",
    "                        values='approval_rate').reset_index()\n",
    "pivot['gap'] = pivot['White'] - pivot['Black']\n",
    "\n",
    "avg_df = pivot.groupby('state_code')['gap'].mean().reset_index()\n",
    "\n",
    "fig = px.choropleth(\n",
    "    avg_df,\n",
    "    locations='state_code',\n",
    "    locationmode='USA-states',\n",
    "    color='gap',\n",
    "    scope='usa',\n",
    "    color_continuous_scale='RdBu',\n",
    "    range_color=(-0.1, 0.2),\n",
    "    labels={'gap': 'Average Gap (White – Black)'},\n",
    "    title='Average White vs Black Approval Rate Gap by State (2018–2024)'\n",
    ")\n",
    "fig.update_layout(margin=dict(l=0, r=0, t=50, b=0))\n",
    "fig.write_html('avg_gap_map.html')  # Save as interactive HTML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5359a5f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e7979e-cfa1-4b14-ba50-40698c0a9bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2018  # replace with the year you want\n",
    "df_year = pivot[pivot['activity_year'] == year]\n",
    "fig = px.choropleth(\n",
    "    df_year,\n",
    "    locations='state_code',\n",
    "    locationmode='USA-states',\n",
    "    color='gap',\n",
    "    scope='usa',\n",
    "    color_continuous_scale='RdBu_r',\n",
    "    range_color=(-0.1, 0.2),\n",
    "    labels={'gap': f'Gap (White – Black) in {year}'},\n",
    "    title=f'White vs Black Approval Rate Gap by State – {year}'\n",
    ")\n",
    "fig.write_html(f'state_gap_{year}.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d6d55b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359e7795-dda9-492d-b503-e6c256b24cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"election_race_summary.csv\")\n",
    "\n",
    "print(\"Available races:\", df[\"race_label\"].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e8c962",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f2b3f8-5079-409c-a956-86efd0edff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "df = pd.read_csv(\"election_race_summary.csv\")\n",
    "\n",
    "if \"state_code\" not in df.columns:\n",
    "    raise ValueError(\"Missing 'state_code' column in your data.\")\n",
    "\n",
    "df = df[df[\"race_label\"].isin([\"Black\", \"White\"])]\n",
    "\n",
    "pivot = df.pivot(index=[\"year\", \"state_code\", \"political_lean\"],\n",
    "                 columns=\"race_label\",\n",
    "                 values=\"avg_approval_rate\").reset_index()\n",
    "\n",
    "pivot = pivot.dropna(subset=[\"White\", \"Black\"])\n",
    "\n",
    "pivot[\"gap\"] = pivot[\"White\"] - pivot[\"Black\"]\n",
    "\n",
    "year = 2024\n",
    "df_year = pivot[pivot[\"year\"] == year].copy()\n",
    "\n",
    "def plot_gap_map(df_subset, title, filename):\n",
    "    fig = px.choropleth(\n",
    "        df_subset,\n",
    "        locations=\"state_code\",\n",
    "        locationmode=\"USA-states\",\n",
    "        color=\"gap\",\n",
    "        scope=\"usa\",\n",
    "        color_continuous_scale=\"RdBu_r\",  # blue to red gradient\n",
    "        range_color=(-0.1, 0.2),\n",
    "        labels={\"gap\": \"Approval Gap (White – Black)\"},\n",
    "        title=title,\n",
    "        hover_name=\"state_code\",\n",
    "    )\n",
    "    fig.update_layout(margin=dict(l=0, r=0, t=50, b=0))\n",
    "    fig.write_html(filename)\n",
    "    print(f\" Saved: {filename}\")\n",
    "\n",
    "df_dem = df_year[df_year[\"political_lean\"] == \"Democrat\"]\n",
    "df_rep = df_year[df_year[\"political_lean\"] == \"Republican\"]\n",
    "\n",
    "plot_gap_map(\n",
    "    df_dem,\n",
    "    title=f\"White–Black Mortgage Approval Gap – Democrat States ({year})\",\n",
    "    filename=\"gap_map_democrat.html\"\n",
    ")\n",
    "\n",
    "plot_gap_map(\n",
    "    df_rep,\n",
    "    title=f\"White–Black Mortgage Approval Gap – Republican States ({year})\",\n",
    "    filename=\"gap_map_republican.html\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf754aa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c2c64b-1d70-4ba3-ae68-6d1665019649",
   "metadata": {},
   "outputs": [],
   "source": [
    "KeyError: ['White', 'Black']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74d4a63",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b106cea-f8c9-4632-afc5-4c705dbec3f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db82f333",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276e89f4-c215-4fbf-ad66-60b712c417c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7724ddf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a25f3c-0149-4d80-913e-49166a7115c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03ce55bb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bd86cf-1fd7-4524-88e0-149347a52d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import os\n",
    "\n",
    "tract_data_path = \"tract_level_approval_rates.csv\"\n",
    "tract_shapefile = \"shapefiles_2024/us_tracts_combined.shp\"\n",
    "output_dir = \"svg_maps\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(tract_data_path)\n",
    "\n",
    "df['tract_fips'] = df['tract_fips'].astype(str).str[-11:]\n",
    "\n",
    "df = df[df['race_label'].isin(['Black', 'White'])]\n",
    "\n",
    "pivot = df.pivot_table(index=['tract_fips', 'year'], columns='race_label', values='approval_rate').reset_index()\n",
    "pivot['gap'] = pivot['White'] - pivot['Black']\n",
    "\n",
    "tracts = gpd.read_file(tract_shapefile)\n",
    "tracts = tracts.rename(columns={'GEOID': 'tract_fips'}) if 'GEOID' in tracts.columns else tracts\n",
    "tracts['tract_fips'] = tracts['tract_fips'].astype(str).str.zfill(11)\n",
    "tracts = tracts[['tract_fips', 'geometry']]\n",
    "\n",
    "merged = tracts.merge(pivot, on='tract_fips', how='inner')\n",
    "merged = merged.to_crs(\"EPSG:3857\")  # Web Mercator for better rendering\n",
    "\n",
    "years = sorted(merged['year'].dropna().unique())\n",
    "\n",
    "cmap = plt.get_cmap(\"bwr\")  # Blue-White-Red\n",
    "norm = mcolors.TwoSlopeNorm(vmin=-0.5, vcenter=0, vmax=0.5)\n",
    "\n",
    "for year in years:\n",
    "    print(f\" Rendering year {year}...\")\n",
    "\n",
    "    yearly = merged[merged['year'] == year]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title(f\"Mortgage Approval Gap (White − Black) – {year}\", fontsize=16)\n",
    "\n",
    "    yearly.plot(\n",
    "        column=\"gap\",\n",
    "        cmap=cmap,\n",
    "        norm=norm,\n",
    "        linewidth=0,\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    output_file = os.path.join(output_dir, f\"approval_gap_{year}.svg\")\n",
    "    fig.savefig(output_file, format='svg', bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "print(f\" Export complete. Maps saved to folder: {output_dir}\")\n",
    "merged = tracts.merge(pivot, on='tract_fips', how='inner')\n",
    "print(\" Merged rows:\", len(merged))\n",
    "print(\" Total tracts in shapefile:\", len(tracts))\n",
    "print(\" Unique tract_fips in approval data:\", df['tract_fips'].nunique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c9f603",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af4cba1-bc7f-42ea-ba2b-2a4a920f3046",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29c2030",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d6349f-00b6-4b73-aa11-6b175986ee5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('tract_level_approval_rates.csv')\n",
    "\n",
    "df = df[df['race_label'].isin(['Black','White'])]\n",
    "\n",
    "pivot = df.pivot_table(index=['tract_fips','year'], columns='race_label', values='approval_rate').reset_index()\n",
    "pivot['gap'] = pivot['White'] - pivot['Black']\n",
    "\n",
    "summary = pivot[['tract_fips','year','gap']]\n",
    "summary.to_csv('tract_gap_summary.csv', index=False)\n",
    "print('Saved tract_gap_summary.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd3323f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b4049e-0987-4b4d-89ac-3321b48e5d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "import json\n",
    "import re\n",
    "\n",
    "gap_csv = \"tract_gap_summary.csv\"\n",
    "tract_shapefile = \"tracts_shapefiles_2024/us_tracts_combined.shp\"\n",
    "metro_shapefile = \"metro_area_shapefile/tl_2023_us_cbsa.shp\"\n",
    "\n",
    "state_fips = {\n",
    "    'AL': '01', 'AK': '02', 'AZ': '04', 'AR': '05', 'CA': '06', 'CO': '08', 'CT': '09', 'DE': '10',\n",
    "    'FL': '12', 'GA': '13', 'HI': '15', 'ID': '16', 'IL': '17', 'IN': '18', 'IA': '19', 'KS': '20',\n",
    "    'KY': '21', 'LA': '22', 'ME': '23', 'MD': '24', 'MA': '25', 'MI': '26', 'MN': '27', 'MS': '28',\n",
    "    'MO': '29', 'MT': '30', 'NE': '31', 'NV': '32', 'NH': '33', 'NJ': '34', 'NM': '35', 'NY': '36',\n",
    "    'NC': '37', 'ND': '38', 'OH': '39', 'OK': '40', 'OR': '41', 'PA': '42', 'RI': '44', 'SC': '45',\n",
    "    'SD': '46', 'TN': '47', 'TX': '48', 'UT': '49', 'VT': '50', 'VA': '51', 'WA': '53', 'WV': '54',\n",
    "    'WI': '55', 'WY': '56', 'DC': '11'\n",
    "}\n",
    "\n",
    "def analyze_tract_formats(gap_df, sample_size=20):\n",
    "    \"\"\"Analyze the different tract FIPS formats to improve conversion\"\"\"\n",
    "    print(\"=== Analyzing tract FIPS formats ===\")\n",
    "    samples = gap_df['tract_fips'].head(sample_size).tolist()\n",
    "    \n",
    "    for i, tract_id in enumerate(samples):\n",
    "        print(f\"{i+1:2d}: {tract_id} (length: {len(str(tract_id))})\")\n",
    "    \n",
    "    lengths = gap_df['tract_fips'].astype(str).str.len().value_counts().sort_index()\n",
    "    print(f\"\\nLength distribution:\")\n",
    "    for length, count in lengths.items():\n",
    "        print(f\"  Length {length}: {count:,} records\")\n",
    "    \n",
    "    prefixes = gap_df['tract_fips'].astype(str).str[:2].value_counts().head(10)\n",
    "    print(f\"\\nTop 10 prefixes:\")\n",
    "    for prefix, count in prefixes.items():\n",
    "        print(f\"  '{prefix}': {count:,} records\")\n",
    "\n",
    "def improved_convert_tract_fips(tract_id):\n",
    "    \"\"\"Improved tract FIPS conversion with multiple strategies\"\"\"\n",
    "    tract_id = str(tract_id).strip()\n",
    "    original_id = tract_id\n",
    "    \n",
    "    tract_id = tract_id.replace('.', '').replace(' ', '')\n",
    "    \n",
    "    if tract_id.isdigit() and len(tract_id) == 11:\n",
    "        return tract_id\n",
    "    \n",
    "    if len(tract_id) >= 13 and tract_id[:2] in state_fips:\n",
    "        state_abbr = tract_id[:2]\n",
    "        state_code = state_fips[state_abbr]\n",
    "        remaining = tract_id[2:]\n",
    "        \n",
    "        if len(remaining) >= 9:\n",
    "            if len(remaining) == 9:\n",
    "                county = remaining[:3]\n",
    "                tract = remaining[3:]\n",
    "                return f\"{state_code}{county}{tract}\"\n",
    "            elif len(remaining) >= 11:\n",
    "                county_tract = remaining[-9:]\n",
    "                county = county_tract[:3]\n",
    "                tract = county_tract[3:]\n",
    "                return f\"{state_code}{county}{tract}\"\n",
    "    \n",
    "    digit_sequences = re.findall(r'\\d+', tract_id)\n",
    "    \n",
    "    for seq in digit_sequences:\n",
    "        if len(seq) == 11:\n",
    "            return seq\n",
    "        elif len(seq) > 11:\n",
    "            if seq[:11].startswith(('01', '02', '04', '05', '06')):  # Common state codes\n",
    "                return seq[:11]\n",
    "            elif seq[-11:].startswith(('01', '02', '04', '05', '06')):\n",
    "                return seq[-11:]\n",
    "    \n",
    "    for state_abbr, state_code in state_fips.items():\n",
    "        if state_abbr in tract_id:\n",
    "            pos = tract_id.find(state_abbr)\n",
    "            after_state = tract_id[pos + 2:]\n",
    "            digits_after = re.findall(r'\\d+', after_state)\n",
    "            \n",
    "            if digits_after:\n",
    "                longest_digit = max(digits_after, key=len)\n",
    "                if len(longest_digit) >= 9:\n",
    "                    county_tract = longest_digit[:9]\n",
    "                    return f\"{state_code}{county_tract}\"\n",
    "    \n",
    "    state_codes = list(state_fips.values())\n",
    "    for code in state_codes:\n",
    "        if code in tract_id:\n",
    "            pos = tract_id.find(code)\n",
    "            after_code = tract_id[pos + 2:]\n",
    "            digits = re.findall(r'\\d+', after_code)\n",
    "            if digits and len(digits[0]) >= 9:\n",
    "                return f\"{code}{digits[0][:9]}\"\n",
    "    \n",
    "    if digit_sequences:\n",
    "        longest = max(digit_sequences, key=len)\n",
    "        if len(longest) >= 11:\n",
    "            for start in range(len(longest) - 10):\n",
    "                candidate = longest[start:start+11]\n",
    "                if candidate[:2] in state_codes:\n",
    "                    return candidate\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_color(gap_value):\n",
    "    \"\"\"Get color for gap value with enhanced visibility\"\"\"\n",
    "    if pd.isna(gap_value):\n",
    "        return '#cccccc'  # Light gray for missing\n",
    "    \n",
    "    if gap_value >= 0:\n",
    "        intensity = min(gap_value * 1.5, 1.0)\n",
    "        red = int(255)\n",
    "        green_blue = int(255 * (1 - intensity))\n",
    "        return f'#{red:02x}{green_blue:02x}{green_blue:02x}'\n",
    "    else:\n",
    "        intensity = min(abs(gap_value) * 1.5, 1.0)\n",
    "        blue = int(255)\n",
    "        red_green = int(255 * (1 - intensity))\n",
    "        return f'#{red_green:02x}{red_green:02x}{blue:02x}'\n",
    "\n",
    "print(\"=== Loading gap data ===\")\n",
    "gap_df = pd.read_csv(gap_csv)\n",
    "gap_df['year'] = gap_df['year'].astype(int)\n",
    "\n",
    "analyze_tract_formats(gap_df, sample_size=30)\n",
    "\n",
    "print(\"\\n=== Testing conversion on sample ===\")\n",
    "sample_ids = gap_df['tract_fips'].head(10).tolist()\n",
    "for tract_id in sample_ids:\n",
    "    converted = improved_convert_tract_fips(tract_id)\n",
    "    print(f\"'{tract_id}' -> '{converted}'\")\n",
    "\n",
    "print(\"\\n=== Applying conversion to all data ===\")\n",
    "gap_df['tract_fips_converted'] = gap_df['tract_fips'].apply(improved_convert_tract_fips)\n",
    "\n",
    "total_records = len(gap_df)\n",
    "successful_conversions = gap_df['tract_fips_converted'].notna().sum()\n",
    "print(f\"Conversion success: {successful_conversions:,} / {total_records:,} ({successful_conversions/total_records*100:.1f}%)\")\n",
    "\n",
    "gap_df_clean = gap_df.dropna(subset=['tract_fips_converted', 'gap']).copy()\n",
    "gap_df_clean['tract_fips'] = gap_df_clean['tract_fips_converted']\n",
    "gap_df_clean = gap_df_clean.drop('tract_fips_converted', axis=1)\n",
    "print(f\"Records after cleaning: {len(gap_df_clean):,}\")\n",
    "\n",
    "print(\"\\n=== Loading shapefile ===\")\n",
    "tracts_gdf = gpd.read_file(tract_shapefile)[['GEOID', 'geometry']]\n",
    "tracts_gdf = tracts_gdf.rename(columns={'GEOID': 'tract_fips'})\n",
    "tracts_gdf = tracts_gdf.to_crs(epsg=4326)\n",
    "print(f\"Shapefile records: {len(tracts_gdf):,}\")\n",
    "\n",
    "gap_tracts = set(gap_df_clean['tract_fips'].unique())\n",
    "shp_tracts = set(tracts_gdf['tract_fips'].unique())\n",
    "overlap = gap_tracts.intersection(shp_tracts)\n",
    "print(f\"Overlapping tracts: {len(overlap):,}\")\n",
    "\n",
    "if len(overlap) < 1000:\n",
    "    print(\"Still low overlap. Let's examine some examples:\")\n",
    "    print(\"Sample gap tract IDs:\", list(gap_tracts)[:10])\n",
    "    print(\"Sample shapefile tract IDs:\", list(shp_tracts)[:10])\n",
    "\n",
    "merged = tracts_gdf.merge(gap_df_clean, on='tract_fips', how='inner')\n",
    "print(f\"Final merged data: {len(merged):,} records\")\n",
    "print(f\"Years available: {sorted(merged['year'].unique())}\")\n",
    "\n",
    "if len(merged) == 0:\n",
    "    print(\" Still no merged data. Need to examine tract formats more closely.\")\n",
    "    exit()\n",
    "\n",
    "unique_years = sorted(merged['year'].unique())\n",
    "print(f\"\\n=== Creating map with {len(merged)} records ===\")\n",
    "\n",
    "m = folium.Map(location=[39.8, -98.6], zoom_start=4, tiles='cartodbpositron')\n",
    "\n",
    "year_data = {}\n",
    "for year in unique_years:\n",
    "    year_data[year] = merged[merged['year'] == year].copy()\n",
    "    print(f\"Year {year}: {len(year_data[year])} tracts\")\n",
    "\n",
    "for year in unique_years:\n",
    "    layer_group = folium.FeatureGroup(name=f\"Year {year}\")\n",
    "    \n",
    "    data = year_data[year]\n",
    "    for idx, row in data.iterrows():\n",
    "        color = get_color(row['gap'])\n",
    "        \n",
    "        folium.GeoJson(\n",
    "            row['geometry'].__geo_interface__,\n",
    "            style_function=lambda x, color=color: {\n",
    "                'fillColor': color,\n",
    "                'color': 'black',\n",
    "                'weight': 0.3,\n",
    "                'fillOpacity': 0.8\n",
    "            },\n",
    "            popup=folium.Popup(\n",
    "                f\"<b>Census Tract:</b> {row['tract_fips']}<br>\"\n",
    "                f\"<b>Year:</b> {year}<br>\"\n",
    "                f\"<b>Gap Value:</b> {row['gap']:.3f}\",\n",
    "                max_width=200\n",
    "            ),\n",
    "            tooltip=f\"Tract {row['tract_fips']}: {row['gap']:.3f}\"\n",
    "        ).add_to(layer_group)\n",
    "    \n",
    "    layer_group.add_to(m)\n",
    "\n",
    "try:\n",
    "    metro = gpd.read_file(metro_shapefile).to_crs(epsg=4326)\n",
    "    metro_layer = folium.FeatureGroup(name=\"Metro Areas\")\n",
    "    folium.GeoJson(\n",
    "        metro,\n",
    "        style_function=lambda x: {\n",
    "            'color': 'gray', \n",
    "            'weight': 1, \n",
    "            'fillOpacity': 0,\n",
    "            'opacity': 0.4\n",
    "        }\n",
    "    ).add_to(metro_layer)\n",
    "    metro_layer.add_to(m)\n",
    "except Exception as e:\n",
    "    print(f\"Could not load metro areas: {e}\")\n",
    "\n",
    "control_html = f'''\n",
    "<div id=\"year-controls\" style=\"\n",
    "    position: fixed;\n",
    "    top: 10px;\n",
    "    left: 50px;\n",
    "    z-index: 1000;\n",
    "    background: white;\n",
    "    padding: 10px;\n",
    "    border: 2px solid #ccc;\n",
    "    border-radius: 5px;\n",
    "    box-shadow: 0 2px 5px rgba(0,0,0,0.2);\n",
    "\">\n",
    "    <div style=\"margin-bottom: 10px; font-weight: bold;\">Select Year ({len(merged):,} tracts):</div>\n",
    "    <div id=\"year-buttons\">\n",
    "        {\" \".join([f'<button onclick=\"showYear({year})\" id=\"btn-{year}\" class=\"year-btn\" style=\"margin: 2px; padding: 5px 10px; border: 1px solid #ccc; background: #f0f0f0; cursor: pointer;\">{year}</button>' for year in unique_years])}\n",
    "    </div>\n",
    "    <div style=\"margin-top: 10px; font-size: 12px;\">\n",
    "        <div><span style=\"color: #ff0000;\">●</span> Positive Gap</div>\n",
    "        <div><span style=\"color: #0000ff;\">●</span> Negative Gap</div>\n",
    "        <div><span style=\"color: #cccccc;\">●</span> No Data</div>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<script>\n",
    "var currentYear = {unique_years[0]};\n",
    "var yearLayers = {{}};\n",
    "\n",
    "function showYear(year) {{\n",
    "    // Hide all layers first\n",
    "    document.querySelectorAll('.leaflet-control-layers-overlays input').forEach(input => {{\n",
    "        if (input.checked && input.nextSibling.textContent.includes('Year')) {{\n",
    "            input.click();\n",
    "        }}\n",
    "    }});\n",
    "    \n",
    "    // Show selected year layer\n",
    "    document.querySelectorAll('.leaflet-control-layers-overlays input').forEach(input => {{\n",
    "        if (input.nextSibling.textContent.includes('Year ' + year)) {{\n",
    "            if (!input.checked) {{\n",
    "                input.click();\n",
    "            }}\n",
    "        }}\n",
    "    }});\n",
    "    \n",
    "    currentYear = year;\n",
    "    \n",
    "    // Update button styles\n",
    "    document.querySelectorAll('.year-btn').forEach(btn => {{\n",
    "        btn.style.background = '#f0f0f0';\n",
    "        btn.style.fontWeight = 'normal';\n",
    "        btn.style.color = 'black';\n",
    "    }});\n",
    "    document.getElementById('btn-' + year).style.background = '#007cba';\n",
    "    document.getElementById('btn-' + year).style.color = 'white';\n",
    "    document.getElementById('btn-' + year).style.fontWeight = 'bold';\n",
    "}}\n",
    "\n",
    "// Initialize with first year\n",
    "window.addEventListener('load', function() {{\n",
    "    setTimeout(() => showYear({unique_years[0]}), 1000);\n",
    "}});\n",
    "</script>\n",
    "\n",
    "<style>\n",
    ".year-btn:hover {{\n",
    "    background: #e0e0e0 !important;\n",
    "}}\n",
    "</style>\n",
    "'''\n",
    "\n",
    "m.get_root().html.add_child(folium.Element(control_html))\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "output_file = 'tract_gap_improved_coverage.html'\n",
    "m.save(output_file)\n",
    "print(f\" Improved map saved to {output_file}\")\n",
    "print(f\"This version should have much more data coverage!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830ea774",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc31e28-37fa-47ee-8f0a-9a1919bcd8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "lei_jpm = \"7H6GLXDRUGQFU57RNE97\"\n",
    "data_folder = \"filtered_data_strict\"\n",
    "years = range(2018, 2025)\n",
    "race_map = {\n",
    "    3.0: 'Black',\n",
    "    5.0: 'White'\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for year in years:\n",
    "    file_path = os.path.join(data_folder, f\"{year}_filtered_hmda.csv\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\" Missing file: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "    race_col = 'applicant_race-1' if 'applicant_race-1' in df.columns else 'applicant_race_1'\n",
    "\n",
    "    df = df[df['lei'] == lei_jpm]\n",
    "    df = df[df[race_col].isin([3.0, 5.0])]\n",
    "    df['race'] = df[race_col].map(race_map)\n",
    "\n",
    "    df = df[df['action_taken'].isin([1, 2, 3])]\n",
    "\n",
    "    summary = df.groupby('race')['action_taken'].value_counts().unstack(fill_value=0)\n",
    "    summary['total'] = summary.sum(axis=1)\n",
    "    summary['approved'] = summary.get(1, 0) + summary.get(2, 0)\n",
    "    summary['approval_rate'] = summary['approved'] / summary['total']\n",
    "\n",
    "    if 'White' in summary.index and 'Black' in summary.index:\n",
    "        gap = summary.loc['White', 'approval_rate'] - summary.loc['Black', 'approval_rate']\n",
    "    else:\n",
    "        gap = None\n",
    "\n",
    "    results.append({\n",
    "        'year': year,\n",
    "        'white_approval_rate': summary.loc['White', 'approval_rate'] if 'White' in summary.index else None,\n",
    "        'black_approval_rate': summary.loc['Black', 'approval_rate'] if 'Black' in summary.index else None,\n",
    "        'approval_gap': gap\n",
    "    })\n",
    "\n",
    "gap_df = pd.DataFrame(results)\n",
    "gap_df = gap_df.sort_values(by='year')\n",
    "\n",
    "print(gap_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca25a517",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24e8377-a385-4b26-aec0-e03c2d12d714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "unfiltered_folder = \"unfiltered\"\n",
    "\n",
    "years = range(2018, 2025)\n",
    "\n",
    "file_names = [f\"{year}_hmda.csv\" for year in years]\n",
    "\n",
    "total_rows = 0\n",
    "\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(unfiltered_folder, file_name)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\" File not found: {file_path}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\" Reading {file_path} ...\")\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "    num_rows = len(df)\n",
    "    total_rows += num_rows\n",
    "    print(f\" {file_name}: {num_rows:,} rows\")\n",
    "\n",
    "print(f\"\\n Total rows across all unfiltered files: {total_rows:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ad03e9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e02161-60df-4b0a-8312-fc2cb4289449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "filtered_folder = \"filtered_data_strict\"\n",
    "\n",
    "years = range(2018, 2025)\n",
    "\n",
    "file_names = [f\"{year}_filtered_hmda.csv\" for year in years]\n",
    "\n",
    "total_rows = 0\n",
    "\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(filtered_folder, file_name)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\" File not found: {file_path}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\" Reading {file_path} ...\")\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "    num_rows = len(df)\n",
    "    total_rows += num_rows\n",
    "    print(f\" {file_name}: {num_rows:,} rows\")\n",
    "\n",
    "print(f\"\\n Total rows across all filtered files: {total_rows:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e838666",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738ee59b-f76b-4add-8d1b-9b70de4f7531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_dir = \"filtered_data_strict\"\n",
    "years = range(2018, 2025)\n",
    "race_map = {\n",
    "    1: \"American Indian or Alaska Native\",\n",
    "    2: \"Asian\",\n",
    "    3: \"Black\",\n",
    "    4: \"Native Hawaiian\",\n",
    "    5: \"White\"\n",
    "}\n",
    "\n",
    "bloomberg = pd.read_csv(\"bloomberg_scraped_structured.csv\")\n",
    "bloomberg[\"Entity Legal Form Code\"] = bloomberg[\"Entity Legal Form Code\"].fillna(\"\").str.lower()\n",
    "bloomberg[\"is_bank\"] = bloomberg[\"Entity Legal Form Code\"].str.contains(\"bank|credit union\", case=False)\n",
    "bank_leis = set(bloomberg[bloomberg[\"is_bank\"]][\"lei_number\"].dropna().unique())\n",
    "\n",
    "print(f\" Loaded {len(bank_leis)} bank LEIs from Bloomberg\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for year in years:\n",
    "    file_path = os.path.join(data_dir, f\"{year}_filtered_hmda.csv\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\" Missing file: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(file_path, dtype={\"lei\": str})\n",
    "    print(f\"\\n Year: {year} — {len(df)} rows\")\n",
    "\n",
    "    race_col = \"applicant_race_1\" if year == 2024 else \"applicant_race-1\"\n",
    "    if race_col not in df.columns:\n",
    "        print(f\"️ Missing race column: {race_col}\")\n",
    "        continue\n",
    "\n",
    "    if \"lei\" not in df.columns:\n",
    "        print(f\"️ Missing LEI column\")\n",
    "        continue\n",
    "\n",
    "    df = df[df[\"action_taken\"].isin([1, 3])]\n",
    "    if df.empty:\n",
    "        print(\"️ No loans with action_taken 1 or 3\")\n",
    "        continue\n",
    "\n",
    "    df = df[df[race_col].isin(race_map.keys())]\n",
    "    if df.empty:\n",
    "        print(\"️ No rows with valid race codes\")\n",
    "        continue\n",
    "\n",
    "    if \"lei\" in df.columns:\n",
    "        df[\"lei\"] = df[\"lei\"].astype(str)\n",
    "        df[\"lender_type\"] = df[\"lei\"].apply(lambda x: \"Bank\" if x in bank_leis else \"Non-Bank\")\n",
    "    else:\n",
    "        print(f\"️ Skipping {year}: Missing 'lei' column\")\n",
    "        continue\n",
    "\n",
    "    df[\"approved\"] = (df[\"action_taken\"] == 1).astype(int)\n",
    "    df[\"race\"] = df[race_col].map(race_map)\n",
    "\n",
    "    grouped = (\n",
    "        df.groupby([\"lender_type\", \"race\"])[\"approved\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .assign(year=year)\n",
    "    )\n",
    "    print(grouped)\n",
    "    results.append(grouped)\n",
    "\n",
    "if not results:\n",
    "    print(\" No usable data found across any year.\")\n",
    "else:\n",
    "    final_df = pd.concat(results, ignore_index=True)\n",
    "    print(\" Final combined data preview:\")\n",
    "    print(final_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a562e1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8093e2-22fb-4e9d-9e86-d4b843e8ef20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "bank_df = final_df[final_df[\"lender_type\"] == \"Bank\"]\n",
    "nonbank_df = final_df[final_df[\"lender_type\"] == \"Non-Bank\"]\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10), sharey=True)\n",
    "\n",
    "sns.barplot(\n",
    "    data=bank_df,\n",
    "    x=\"year\",\n",
    "    y=\"approved\",  #  use correct column\n",
    "    hue=\"race\",\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title(\"Mortgage Approval Rates by Race — Banks (2018–2024)\", fontsize=14)\n",
    "axes[0].set_ylabel(\"Approval Rate\")\n",
    "axes[0].set_xlabel(\"\")\n",
    "axes[0].legend(title=\"Race\", bbox_to_anchor=(1.01, 1), loc=\"upper left\")\n",
    "\n",
    "sns.barplot(\n",
    "    data=nonbank_df,\n",
    "    x=\"year\",\n",
    "    y=\"approved\",  #  use correct column\n",
    "    hue=\"race\",\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title(\"Mortgage Approval Rates by Race — Non-Banks (2018–2024)\", fontsize=14)\n",
    "axes[1].set_ylabel(\"Approval Rate\")\n",
    "axes[1].set_xlabel(\"Year\")\n",
    "axes[1].legend(title=\"Race\", bbox_to_anchor=(1.01, 1), loc=\"upper left\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_ylim(0.75, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"bank_vs_nonbank_chart.svg\", format=\"svg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23ea149",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfaf24d-f077-463f-8df6-8de7105efb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bank rows:\", bank_df.shape[0])\n",
    "print(\"Non-Bank rows:\", nonbank_df.shape[0])\n",
    "print(\"Races in Bank data:\", bank_df[\"race\"].unique())\n",
    "print(\"Races in Non-Bank data:\", nonbank_df[\"race\"].unique())\n",
    "print(\"Sample of Bank data:\")\n",
    "print(bank_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46f2735",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11baa57f-11ba-4825-8586-292d236e4d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73af7bb1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc80e8a-4ee2-4965-96f9-5b0bd8b8993e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d2b3257",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98afbcd8-5f8d-450c-b177-302b9a0bc23e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6832c452",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b55093d-1547-4c77-b5c3-58b58dd733c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "770c6b17",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229d9d41-8427-4bdb-b373-6854dbe4b78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "hmda_folder = \"filtered_data_strict\"\n",
    "output_file = \"approval_by_year_and_election.csv\"\n",
    "bank_leis_file = \"bloomberg_scraped_structured.csv\"\n",
    "\n",
    "def load_bank_leis():\n",
    "    try:\n",
    "        bloomberg = pd.read_csv(bank_leis_file)\n",
    "        print(f\" Bloomberg file columns: {list(bloomberg.columns)}\")\n",
    "        \n",
    "        lei_columns = ['lei', 'LEI', 'Lei', 'lei_number', 'legal_entity_identifier']\n",
    "        lei_col = None\n",
    "        \n",
    "        for col in lei_columns:\n",
    "            if col in bloomberg.columns:\n",
    "                lei_col = col\n",
    "                break\n",
    "        \n",
    "        if lei_col is None:\n",
    "            print(f\" No LEI column found. Available columns: {list(bloomberg.columns)}\")\n",
    "            return set()\n",
    "            \n",
    "        bank_leis = set(bloomberg[lei_col].dropna().astype(str).unique())\n",
    "        print(f\" Loaded {len(bank_leis)} bank LEIs from column '{lei_col}'\")\n",
    "        return bank_leis\n",
    "    except Exception as e:\n",
    "        print(f\" Error loading LEI file: {e}\")\n",
    "        return set()\n",
    "\n",
    "def load_election_results(filepath):\n",
    "    \"\"\"Load election results and ensure proper FIPS formatting\"\"\"\n",
    "    print(f\" Loading election data from {filepath}\")\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    df[\"county_fips\"] = df[\"county_fips\"].astype(str).str.zfill(5)\n",
    "    df[\"election_winner\"] = df[\"per_gop\"] > df[\"per_dem\"]\n",
    "    \n",
    "    print(f\"    Loaded {len(df)} counties\")\n",
    "    print(f\"    Sample FIPS: {df['county_fips'].head().tolist()}\")\n",
    "    \n",
    "    return df[[\"county_fips\", \"election_winner\"]]\n",
    "\n",
    "def create_fips_code(state_code, county_code):\n",
    "    \"\"\"Create FIPS code with robust error handling\"\"\"\n",
    "    try:\n",
    "        state_str = str(state_code).strip()\n",
    "        county_str = str(county_code).strip()\n",
    "        \n",
    "        if len(state_str) > 10:  # Likely contains full FIPS like 'TN47043.0'\n",
    "            full_match = pd.Series([state_str]).str.extract(r'[A-Z]{2}(\\d{5})')[0].iloc[0]\n",
    "            if not pd.isna(full_match):\n",
    "                return str(full_match)\n",
    "        \n",
    "        state_clean = state_str.replace('.0', '').strip()\n",
    "        county_clean = county_str.replace('.0', '').strip()\n",
    "        \n",
    "        state_match = pd.Series([state_clean]).str.extract(r'(\\d+)')[0].iloc[0]\n",
    "        county_match = pd.Series([county_clean]).str.extract(r'(\\d+)')[0].iloc[0]\n",
    "        \n",
    "        if pd.isna(state_match) or pd.isna(county_match):\n",
    "            return np.nan\n",
    "            \n",
    "        fips = str(state_match).zfill(2) + str(county_match).zfill(3)\n",
    "        return fips\n",
    "        \n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def process_hmda_year(year, hmda_folder, bank_leis, election_df):\n",
    "    \"\"\"Process a single year of HMDA data\"\"\"\n",
    "    file_path = os.path.join(hmda_folder, f\"{year}_filtered_hmda.csv\")\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\" Skipping {year}: File not found\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\n Processing {year}...\")\n",
    "\n",
    "    try:\n",
    "        print(f\"    Reading file: {file_path}\")\n",
    "        df = pd.read_csv(file_path, dtype={\n",
    "            \"county_code\": str, \n",
    "            \"state_code\": str,\n",
    "            \"lei\": str\n",
    "        }, low_memory=False)\n",
    "        print(f\"    Loaded {len(df)} rows\")\n",
    "        print(f\"    Columns available: {list(df.columns)[:10]}...\")  # Show first 10 columns\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error reading {year} data: {e}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"    Raw state_code sample: {df['state_code'].dropna().head().tolist()}\")\n",
    "    print(f\"    Raw county_code sample: {df['county_code'].dropna().head().tolist()}\")\n",
    "\n",
    "    print(f\"    Creating FIPS codes...\")\n",
    "    \n",
    "    sample_county = str(df['county_code'].dropna().iloc[0]) if len(df) > 0 else \"\"\n",
    "    sample_state = str(df['state_code'].dropna().iloc[0]) if len(df) > 0 else \"\"\n",
    "    \n",
    "    print(f\"    Sample state_code: '{sample_state}', county_code: '{sample_county}'\")\n",
    "    \n",
    "    if len(sample_county.replace('.0', '')) == 5 and sample_county.replace('.0', '').isdigit():\n",
    "        print(f\"    Detected county_code contains full FIPS\")\n",
    "        df[\"county_fips\"] = df[\"county_code\"].astype(str).str.replace('.0', '', regex=False)\n",
    "        \n",
    "    elif len(sample_state) > 5:  # Likely format like 'TN47043.0'\n",
    "        print(f\"    Detected consolidated FIPS format in state_code: {sample_state}\")\n",
    "        df[\"county_fips\"] = df[\"state_code\"].astype(str).str.extract(r'[A-Z]{2}(\\d{5})')[0]\n",
    "        df[\"county_fips\"] = df[\"county_fips\"].astype(str).str.replace('.0', '', regex=False)\n",
    "        \n",
    "    else:\n",
    "        print(f\"    Using separate state/county codes - building FIPS from parts\")\n",
    "        df[\"state_clean\"] = df[\"state_code\"].astype(str).str.replace('.0', '', regex=False).str.extract(r'(\\d+)')[0]\n",
    "        df[\"county_clean\"] = df[\"county_code\"].astype(str).str.replace('.0', '', regex=False).str.extract(r'(\\d+)')[0]\n",
    "        \n",
    "        df[\"county_fips\"] = df[\"state_clean\"].str.zfill(2) + df[\"county_clean\"].str.zfill(3)\n",
    "        \n",
    "        df.drop([\"state_clean\", \"county_clean\"], axis=1, inplace=True)\n",
    "    \n",
    "    initial_count = len(df)\n",
    "    df = df.dropna(subset=[\"county_fips\"])\n",
    "    df = df[df[\"county_fips\"].str.len() == 5]\n",
    "    df = df[df[\"county_fips\"].str.match(r'^\\d{5}$')]\n",
    "    \n",
    "    print(f\"    Valid FIPS codes: {len(df)} of {initial_count} rows ({len(df)/initial_count*100:.1f}%)\")\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(f\"    No valid FIPS codes created for {year}\")\n",
    "        return None\n",
    "        \n",
    "    print(f\"    Sample HMDA FIPS: {df['county_fips'].unique()[:5].tolist()}\")\n",
    "\n",
    "    race_col = \"applicant_race_1\" if year == 2024 else \"applicant_race-1\"\n",
    "    if race_col not in df.columns:\n",
    "        print(f\"️ Skipping {year}: Missing race column {race_col}\")\n",
    "        return None\n",
    "\n",
    "    df[race_col] = pd.to_numeric(df[race_col], errors=\"coerce\")\n",
    "    race_map = {\n",
    "        1: \"White\",\n",
    "        2: \"Black\", \n",
    "        3: \"Asian\",\n",
    "        4: \"Native Hawaiian\",\n",
    "        5: \"American Indian or Alaska Native\"\n",
    "    }\n",
    "    df[\"race\"] = df[race_col].map(race_map)\n",
    "    print(f\"    Race distribution:\\n{df['race'].value_counts(dropna=False).to_dict()}\")\n",
    "\n",
    "    if \"action_taken\" not in df.columns:\n",
    "        print(f\"️ Skipping {year}: Missing 'action_taken'\")\n",
    "        return None\n",
    "\n",
    "    df = df[df[\"action_taken\"].isin([1, 2])].copy()\n",
    "    print(f\"   🧮 Approval/denial applications: {len(df)}\")\n",
    "\n",
    "    if \"lei\" in df.columns and len(bank_leis) > 0:\n",
    "        df[\"lei\"] = df[\"lei\"].astype(str)\n",
    "        df[\"lender_type\"] = df[\"lei\"].apply(lambda x: \"Bank\" if x in bank_leis else \"Non-Bank\")\n",
    "        print(f\"    Using LEI for lender classification\")\n",
    "    elif \"purchaser_type\" in df.columns:\n",
    "        print(f\"   ️ {year} missing 'lei' column, using purchaser_type fallback\")\n",
    "        df[\"lender_type\"] = df[\"purchaser_type\"].apply(lambda x: \"Bank\" if x == 6 else \"Non-Bank\")\n",
    "    else:\n",
    "        print(f\"    {year}: No lender classification possible\")\n",
    "        return None\n",
    "\n",
    "    print(f\"    Lender type distribution: {df['lender_type'].value_counts().to_dict()}\")\n",
    "\n",
    "    print(f\"    Merging with election data...\")\n",
    "    print(f\"    Election FIPS sample: {election_df['county_fips'].head().tolist()}\")\n",
    "    \n",
    "    hmda_fips = set(df['county_fips'].unique())\n",
    "    election_fips = set(election_df['county_fips'].unique())\n",
    "    overlap = hmda_fips.intersection(election_fips)\n",
    "    print(f\"    FIPS overlap: {len(overlap)} of {len(hmda_fips)} HMDA counties\")\n",
    "    \n",
    "    if len(overlap) < len(hmda_fips) * 0.5:  # Less than 50% match\n",
    "        print(f\"   ️ Low match rate! Sample unmatched HMDA FIPS: {list(hmda_fips - election_fips)[:5]}\")\n",
    "        print(f\"   ️ Sample election FIPS: {list(election_fips)[:5]}\")\n",
    "\n",
    "    df = df.merge(election_df, on=\"county_fips\", how=\"left\")\n",
    "    df[\"election_lean\"] = df[\"election_winner\"].map({True: \"Trump\", False: \"Biden\"})\n",
    "    \n",
    "    matched_count = df[\"election_lean\"].notna().sum()\n",
    "    print(f\"    Matched election data: {matched_count} of {len(df)} rows ({matched_count/len(df)*100:.1f}%)\")\n",
    "\n",
    "    df = df.dropna(subset=[\"race\", \"lender_type\"])\n",
    "    df[\"election_lean\"] = df[\"election_lean\"].fillna(\"Unknown\")\n",
    "    df[\"year\"] = year\n",
    "\n",
    "    grouped = df.groupby(\n",
    "        [\"year\", \"election_lean\", \"lender_type\", \"race\"]\n",
    "    ).size().reset_index(name=\"total_approved\")\n",
    "    \n",
    "    print(f\"    Final grouped data shape: {grouped.shape}\")\n",
    "    print(f\"    Sample grouped data:\\n{grouped.head()}\")\n",
    "\n",
    "    return grouped\n",
    "\n",
    "def main():\n",
    "    bank_leis = load_bank_leis()\n",
    "    \n",
    "    print(\"\\n Loading election results...\")\n",
    "    results_2016 = load_election_results(\"election_results/2016_US_County_Level_Presidential_Results.csv\")\n",
    "    results_2020 = load_election_results(\"election_results/2020_US_County_Level_Presidential_Results.csv\")\n",
    "\n",
    "    election_map = {\n",
    "        2018: results_2016,  # Pre-2020 election\n",
    "        2019: results_2016,  # Pre-2020 election  \n",
    "        2020: results_2016,  # 2020 election happened late in year\n",
    "        2021: results_2020,  # Post-2020 election\n",
    "        2022: results_2020,  # Post-2020 election\n",
    "        2023: results_2020,  # Post-2020 election\n",
    "        2024: results_2020,  # Post-2020 election\n",
    "    }\n",
    "\n",
    "    all_results = []\n",
    "    \n",
    "    for year in range(2018, 2025):\n",
    "        election_df = election_map[year]\n",
    "        result = process_hmda_year(year, hmda_folder, bank_leis, election_df)\n",
    "        \n",
    "        if result is not None:\n",
    "            all_results.append(result)\n",
    "        else:\n",
    "            print(f\" Failed to process {year}\")\n",
    "\n",
    "    if all_results:\n",
    "        final_df = pd.concat(all_results, ignore_index=True)\n",
    "        final_df.to_csv(output_file, index=False)\n",
    "        print(f\"\\n Successfully saved {len(final_df)} rows to: {output_file}\")\n",
    "        print(f\" Final summary by year:\")\n",
    "        print(final_df.groupby('year')['total_approved'].sum())\n",
    "    else:\n",
    "        print(\"\\n No usable data found - check your input files and data quality\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84aa84e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a0b299-8ae3-4bc6-9ac2-b92b37d798b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "hmda_folder = \"filtered_data_strict\"\n",
    "output_dir = \"processed_data\"\n",
    "\n",
    "Path(output_dir).mkdir(exist_ok=True)\n",
    "\n",
    "def load_raw_hmda_data():\n",
    "    \"\"\"Load raw HMDA data to calculate actual approval/denial rates\"\"\"\n",
    "    print(\" Loading raw HMDA data to calculate approval/denial rates...\")\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    election_map = {\n",
    "        2018: \"election_results/2016_US_County_Level_Presidential_Results.csv\",\n",
    "        2019: \"election_results/2016_US_County_Level_Presidential_Results.csv\",\n",
    "        2020: \"election_results/2016_US_County_Level_Presidential_Results.csv\",\n",
    "        2021: \"election_results/2020_US_County_Level_Presidential_Results.csv\", \n",
    "        2022: \"election_results/2020_US_County_Level_Presidential_Results.csv\",\n",
    "        2023: \"election_results/2020_US_County_Level_Presidential_Results.csv\",\n",
    "        2024: \"election_results/2020_US_County_Level_Presidential_Results.csv\",\n",
    "    }\n",
    "    \n",
    "    election_data = {}\n",
    "    for year, filepath in election_map.items():\n",
    "        if filepath not in election_data:\n",
    "            df = pd.read_csv(filepath)\n",
    "            df[\"county_fips\"] = df[\"county_fips\"].astype(str).str.zfill(5)\n",
    "            df[\"election_winner\"] = df[\"per_gop\"] > df[\"per_dem\"]\n",
    "            df[\"election_lean\"] = df[\"election_winner\"].map({True: \"Trump\", False: \"Biden\"})\n",
    "            election_data[filepath] = df[[\"county_fips\", \"election_lean\"]]\n",
    "    \n",
    "    for year in range(2018, 2025):\n",
    "        file_path = os.path.join(hmda_folder, f\"{year}_filtered_hmda.csv\")\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\" Skipping {year}: File not found\")\n",
    "            continue\n",
    "            \n",
    "        print(f\" Processing {year}...\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path, dtype={\"county_code\": str, \"state_code\": str}, low_memory=False)\n",
    "            \n",
    "            sample_county = str(df['county_code'].dropna().iloc[0]) if len(df) > 0 else \"\"\n",
    "            \n",
    "            if len(sample_county.replace('.0', '')) == 5 and sample_county.replace('.0', '').isdigit():\n",
    "                df[\"county_fips\"] = df[\"county_code\"].astype(str).str.replace('.0', '', regex=False)\n",
    "            else:\n",
    "                df[\"county_fips\"] = df[\"county_code\"].astype(str).str.replace('.0', '', regex=False)\n",
    "            \n",
    "            df = df.dropna(subset=[\"county_fips\"])\n",
    "            df = df[df[\"county_fips\"].str.len() == 5]\n",
    "            df = df[df[\"county_fips\"].str.match(r'^\\d{5}$')]\n",
    "            \n",
    "            race_col = \"applicant_race_1\" if year == 2024 else \"applicant_race-1\"\n",
    "            if race_col not in df.columns:\n",
    "                print(f\"️ Skipping {year}: Missing race column\")\n",
    "                continue\n",
    "                \n",
    "            df[race_col] = pd.to_numeric(df[race_col], errors=\"coerce\")\n",
    "            race_map = {\n",
    "                1: \"White\",\n",
    "                2: \"Black\", \n",
    "                3: \"Asian\",\n",
    "                4: \"Native Hawaiian\",\n",
    "                5: \"American Indian or Alaska Native\"\n",
    "            }\n",
    "            df[\"race\"] = df[race_col].map(race_map)\n",
    "            \n",
    "            if \"action_taken\" not in df.columns:\n",
    "                continue\n",
    "                \n",
    "            df = df[df[\"action_taken\"].isin([1, 2])].copy()\n",
    "            df[\"approved\"] = (df[\"action_taken\"] == 1).astype(int)\n",
    "            df[\"denied\"] = (df[\"action_taken\"] == 2).astype(int)\n",
    "            \n",
    "            election_year_data = election_data[election_map[year]]\n",
    "            df = df.merge(election_year_data, on=\"county_fips\", how=\"left\")\n",
    "            df[\"election_lean\"] = df[\"election_lean\"].fillna(\"Unknown\")\n",
    "            \n",
    "            df = df.dropna(subset=[\"race\"])\n",
    "            df[\"year\"] = year\n",
    "            \n",
    "            df_clean = df[[\"year\", \"county_fips\", \"election_lean\", \"race\", \"approved\", \"denied\"]].copy()\n",
    "            all_data.append(df_clean)\n",
    "            \n",
    "            print(f\"    Processed {len(df_clean)} applications\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" Error processing {year}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if all_data:\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        print(f\"\\n Combined data: {len(combined_df)} total applications\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def calculate_approval_denial_rates(df):\n",
    "    \"\"\"Calculate approval and denial rates by race and political lean\"\"\"\n",
    "    print(\"\\n Calculating approval and denial rates...\")\n",
    "    \n",
    "    df_clean = df[df['election_lean'].isin(['Biden', 'Trump'])].copy()\n",
    "    df_clean['political_lean'] = df_clean['election_lean'].map({\n",
    "        'Biden': 'Democratic Counties',\n",
    "        'Trump': 'Republican Counties'\n",
    "    })\n",
    "    \n",
    "    rates_by_race = df_clean.groupby(['race', 'political_lean']).agg({\n",
    "        'approved': 'sum',\n",
    "        'denied': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    rates_by_race['total_applications'] = rates_by_race['approved'] + rates_by_race['denied']\n",
    "    rates_by_race['approval_rate'] = (rates_by_race['approved'] / rates_by_race['total_applications'] * 100).round(2)\n",
    "    rates_by_race['denial_rate'] = (rates_by_race['denied'] / rates_by_race['total_applications'] * 100).round(2)\n",
    "    \n",
    "    yearly_rates = df_clean.groupby(['year', 'race', 'political_lean']).agg({\n",
    "        'approved': 'sum',\n",
    "        'denied': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    yearly_rates['total_applications'] = yearly_rates['approved'] + yearly_rates['denied']\n",
    "    yearly_rates['approval_rate'] = (yearly_rates['approved'] / yearly_rates['total_applications'] * 100)\n",
    "    \n",
    "    print(\" Sample approval rates by race and political lean:\")\n",
    "    print(rates_by_race.head(10))\n",
    "    \n",
    "    return df_clean, rates_by_race, yearly_rates\n",
    "\n",
    "def create_detailed_summary(rates_by_race):\n",
    "    \"\"\"Create detailed summary statistics\"\"\"\n",
    "    print(\"\\n Creating detailed summary statistics...\")\n",
    "    \n",
    "    summary_pivot = rates_by_race.pivot(index='race', columns='political_lean', \n",
    "                                       values=['approval_rate', 'denial_rate', 'total_applications'])\n",
    "    \n",
    "    summary_pivot.columns = [f\"{col[1]}_{col[0]}\" for col in summary_pivot.columns]\n",
    "    \n",
    "    if 'Democratic Counties_approval_rate' in summary_pivot.columns and 'Republican Counties_approval_rate' in summary_pivot.columns:\n",
    "        summary_pivot['Approval_Rate_Difference'] = (summary_pivot['Democratic Counties_approval_rate'] - \n",
    "                                                    summary_pivot['Republican Counties_approval_rate'])\n",
    "    \n",
    "    print(\" Detailed Summary by Race:\")\n",
    "    print(summary_pivot.round(2))\n",
    "    \n",
    "    return summary_pivot\n",
    "\n",
    "def save_processed_data(df_clean, rates_by_race, yearly_rates, summary_pivot, output_dir):\n",
    "    \"\"\"Save all processed data to files\"\"\"\n",
    "    print(f\"\\n Saving processed data to {output_dir}...\")\n",
    "    \n",
    "    df_clean.to_csv(f\"{output_dir}/cleaned_raw_data.csv\", index=False)\n",
    "    print(f\"    Saved: cleaned_raw_data.csv\")\n",
    "    \n",
    "    rates_by_race.to_csv(f\"{output_dir}/rates_by_race.csv\", index=False)\n",
    "    print(f\"    Saved: rates_by_race.csv\")\n",
    "    \n",
    "    yearly_rates.to_csv(f\"{output_dir}/yearly_rates.csv\", index=False)\n",
    "    print(f\"    Saved: yearly_rates.csv\")\n",
    "    \n",
    "    summary_pivot.round(2).to_csv(f\"{output_dir}/detailed_summary.csv\")\n",
    "    print(f\"    Saved: detailed_summary.csv\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\" HMDA Data Processing Script\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    df = load_raw_hmda_data()\n",
    "    if df is None:\n",
    "        print(\" No data could be loaded\")\n",
    "        return\n",
    "    \n",
    "    df_clean, rates_by_race, yearly_rates = calculate_approval_denial_rates(df)\n",
    "    \n",
    "    summary = create_detailed_summary(rates_by_race)\n",
    "    \n",
    "    save_processed_data(df_clean, rates_by_race, yearly_rates, summary, output_dir)\n",
    "    \n",
    "    print(f\"\\n Data processing complete! Processed files saved in '{output_dir}' folder:\")\n",
    "    print(\"   Data files:\")\n",
    "    print(\"     - cleaned_raw_data.csv (raw application-level data)\")\n",
    "    print(\"     - rates_by_race.csv (aggregated rates by race and political lean)\")\n",
    "    print(\"     - yearly_rates.csv (rates by year for time series)\")\n",
    "    print(\"     - detailed_summary.csv (summary statistics)\")\n",
    "    print(\"\\n Run the plotting script next to create visualizations!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a210405b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca12b654-7c09-4f69-bba8-8626502e5e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "processed_data_dir = \"processed_data\"\n",
    "output_dir = \"plots\"\n",
    "\n",
    "Path(output_dir).mkdir(exist_ok=True)\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "def load_processed_data():\n",
    "    \"\"\"Load previously processed data\"\"\"\n",
    "    print(\" Loading processed data...\")\n",
    "    \n",
    "    try:\n",
    "        rates_by_race = pd.read_csv(f\"{processed_data_dir}/rates_by_race.csv\")\n",
    "        yearly_rates = pd.read_csv(f\"{processed_data_dir}/yearly_rates.csv\")\n",
    "        summary_pivot = pd.read_csv(f\"{processed_data_dir}/detailed_summary.csv\", index_col=0)\n",
    "        \n",
    "        print(f\"    Loaded rates_by_race: {len(rates_by_race)} rows\")\n",
    "        print(f\"    Loaded yearly_rates: {len(yearly_rates)} rows\")\n",
    "        print(f\"    Loaded summary_pivot: {len(summary_pivot)} rows\")\n",
    "        \n",
    "        return rates_by_race, yearly_rates, summary_pivot\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\" Error loading processed data: {e}\")\n",
    "        print(\" Please run the data processing script first!\")\n",
    "        return None, None, None\n",
    "\n",
    "def plot_approval_rates_by_race(rates_by_race, output_dir):\n",
    "    \"\"\"Plot approval rates by race and political lean\"\"\"\n",
    "    print(\"\\n Creating approval rate plots by race...\")\n",
    "    \n",
    "    approval_pivot = rates_by_race.pivot(index='race', columns='political_lean', values='approval_rate').fillna(0)\n",
    "    denial_pivot = rates_by_race.pivot(index='race', columns='political_lean', values='denial_rate').fillna(0)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 12))\n",
    "    \n",
    "    approval_pivot.plot(kind='bar', ax=ax1, color=['#1f77b4', '#d62728'], alpha=0.8, width=0.7)\n",
    "    ax1.set_title('Loan Approval Rates by Race and County Political Lean', fontsize=16, fontweight='bold', pad=20)\n",
    "    ax1.set_ylabel('Approval Rate (%)', fontsize=12)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.legend(title='County Political Lean', title_fontsize=12, fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for container in ax1.containers:\n",
    "        ax1.bar_label(container, fmt='%.1f%%', fontsize=9, rotation=0, padding=3)\n",
    "    \n",
    "    denial_pivot.plot(kind='bar', ax=ax2, color=['#1f77b4', '#d62728'], alpha=0.8, width=0.7)\n",
    "    ax2.set_title('Loan Denial Rates by Race and County Political Lean', fontsize=16, fontweight='bold', pad=20)\n",
    "    ax2.set_ylabel('Denial Rate (%)', fontsize=12)\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.legend(title='County Political Lean', title_fontsize=12, fontsize=11)\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for container in ax2.containers:\n",
    "        ax2.bar_label(container, fmt='%.1f%%', fontsize=9, rotation=0, padding=3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/approval_denial_rates_by_race.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(f\"{output_dir}/approval_denial_rates_by_race.svg\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return approval_pivot, denial_pivot\n",
    "\n",
    "def plot_approval_rate_differences(rates_by_race, output_dir):\n",
    "    \"\"\"Plot the difference in approval rates between political leans\"\"\"\n",
    "    print(\"\\n Creating approval rate difference analysis...\")\n",
    "    \n",
    "    approval_pivot = rates_by_race.pivot(index='race', columns='political_lean', values='approval_rate').fillna(0)\n",
    "    \n",
    "    if 'Democratic Counties' in approval_pivot.columns and 'Republican Counties' in approval_pivot.columns:\n",
    "        approval_pivot['Difference (Dem - Rep)'] = (approval_pivot['Democratic Counties'] - \n",
    "                                                   approval_pivot['Republican Counties'])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    colors = ['#2ca02c' if x > 0 else '#d62728' for x in approval_pivot['Difference (Dem - Rep)']]\n",
    "    bars = ax.bar(range(len(approval_pivot.index)), approval_pivot['Difference (Dem - Rep)'], \n",
    "                  color=colors, alpha=0.7, width=0.6)\n",
    "    \n",
    "    ax.set_title('Approval Rate Differences by Race\\n(Democratic Counties - Republican Counties)', \n",
    "                fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.set_ylabel('Approval Rate Difference (Percentage Points)', fontsize=12)\n",
    "    ax.set_xlabel('Race', fontsize=12)\n",
    "    ax.set_xticks(range(len(approval_pivot.index)))\n",
    "    ax.set_xticklabels(approval_pivot.index, rotation=45, ha='right')\n",
    "    ax.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:+.1f}pp',\n",
    "                   xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                   xytext=(0, 3 if height > 0 else -15), \n",
    "                   textcoords=\"offset points\",\n",
    "                   ha='center', va='bottom' if height > 0 else 'top', \n",
    "                   fontweight='bold', fontsize=10)\n",
    "    \n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor='#2ca02c', alpha=0.7, label='Higher in Democratic Counties'),\n",
    "                      Patch(facecolor='#d62728', alpha=0.7, label='Higher in Republican Counties')]\n",
    "    ax.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/approval_rate_differences.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(f\"{output_dir}/approval_rate_differences.svg\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_time_series_by_race(yearly_rates, output_dir):\n",
    "    \"\"\"Plot approval rate trends over time by race\"\"\"\n",
    "    print(\"\\n Creating time series by race...\")\n",
    "    \n",
    "    races = sorted(yearly_rates['race'].unique())\n",
    "    fig, axes = plt.subplots(len(races), 1, figsize=(14, 4*len(races)))\n",
    "    \n",
    "    if len(races) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, race in enumerate(races):\n",
    "        race_data = yearly_rates[yearly_rates['race'] == race]\n",
    "        \n",
    "        for political_lean in race_data['political_lean'].unique():\n",
    "            data = race_data[race_data['political_lean'] == political_lean]\n",
    "            color = '#1f77b4' if 'Democratic' in political_lean else '#d62728'\n",
    "            axes[i].plot(data['year'], data['approval_rate'], \n",
    "                        marker='o', linewidth=2.5, markersize=8, \n",
    "                        label=political_lean, color=color, alpha=0.8)\n",
    "        \n",
    "        axes[i].set_title(f'Approval Rate Trends: {race}', fontsize=14, fontweight='bold')\n",
    "        axes[i].set_ylabel('Approval Rate (%)')\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        axes[i].set_ylim(90, 100)  # Start y-axis at 90%\n",
    "    \n",
    "    axes[-1].set_xlabel('Year')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/approval_trends_by_race_over_time.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(f\"{output_dir}/approval_trends_by_race_over_time.svg\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def create_all_plots(rates_by_race, yearly_rates, output_dir):\n",
    "    \"\"\"Create all plots\"\"\"\n",
    "    print(\"\\n Creating all plots...\")\n",
    "    \n",
    "    approval_pivot, denial_pivot = plot_approval_rates_by_race(rates_by_race, output_dir)\n",
    "    plot_approval_rate_differences(rates_by_race, output_dir) \n",
    "    plot_time_series_by_race(yearly_rates, output_dir)\n",
    "    \n",
    "    return approval_pivot, denial_pivot\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\" HMDA Plotting Script\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    rates_by_race, yearly_rates, summary_pivot = load_processed_data()\n",
    "    if rates_by_race is None:\n",
    "        return\n",
    "    \n",
    "    approval_pivot, denial_pivot = create_all_plots(rates_by_race, yearly_rates, output_dir)\n",
    "    \n",
    "    print(f\"\\n Plotting complete! Check the '{output_dir}' folder for:\")\n",
    "    print(\"   PNG files (high-resolution raster images):\")\n",
    "    print(\"     - approval_denial_rates_by_race.png\")\n",
    "    print(\"     - approval_rate_differences.png\")\n",
    "    print(\"     - approval_trends_by_race_over_time.png\")\n",
    "    print(\"   SVG files (scalable vector graphics):\")\n",
    "    print(\"     - approval_denial_rates_by_race.svg\") \n",
    "    print(\"     - approval_rate_differences.svg\")\n",
    "    print(\"     - approval_trends_by_race_over_time.svg\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6798cbfb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fc979e-5883-485d-8ac7-ca8648b83646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "processed_data_dir = \"processed_data\"\n",
    "output_dir = \"plots\"\n",
    "\n",
    "Path(output_dir).mkdir(exist_ok=True)\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"Load processed data and prepare for presidential analysis\"\"\"\n",
    "    print(\" Loading and preparing data for presidential analysis...\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(f\"{processed_data_dir}/cleaned_raw_data.csv\")\n",
    "        \n",
    "        print(f\"    Loaded data: {len(df)} applications\")\n",
    "        \n",
    "        def get_president(year):\n",
    "            if year <= 2020:\n",
    "                return \"Trump (Republican)\"\n",
    "            else:\n",
    "                return \"Biden (Democrat)\"\n",
    "        \n",
    "        df['president'] = df['year'].apply(get_president)\n",
    "        \n",
    "        df_dem_counties = df[df['election_lean'] == 'Biden'].copy()\n",
    "        df_dem_counties['political_lean'] = 'Democratic Counties'\n",
    "        \n",
    "        print(f\"    Filtered to Democratic counties: {len(df_dem_counties)} applications\")\n",
    "        print(f\"    Years available: {sorted(df['year'].unique())}\")\n",
    "        print(f\"   ️ Presidential periods: {df['president'].value_counts().to_dict()}\")\n",
    "        \n",
    "        return df_dem_counties\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\" Error loading data: {e}\")\n",
    "        print(\" Please run the data processing script first!\")\n",
    "        return None\n",
    "\n",
    "def calculate_rates_by_president(df):\n",
    "    \"\"\"Calculate approval rates by race and presidential administration\"\"\"\n",
    "    print(\"\\n Calculating approval rates by race and presidential administration...\")\n",
    "    \n",
    "    rates_by_president = df.groupby(['race', 'president']).agg({\n",
    "        'approved': 'sum',\n",
    "        'denied': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    rates_by_president['total_applications'] = rates_by_president['approved'] + rates_by_president['denied']\n",
    "    rates_by_president['approval_rate'] = (rates_by_president['approved'] / rates_by_president['total_applications'] * 100).round(2)\n",
    "    rates_by_president['denial_rate'] = (rates_by_president['denied'] / rates_by_president['total_applications'] * 100).round(2)\n",
    "    \n",
    "    print(\" Approval rates by race and president:\")\n",
    "    print(rates_by_president[['race', 'president', 'approval_rate', 'total_applications']])\n",
    "    \n",
    "    return rates_by_president\n",
    "\n",
    "def plot_approval_rates_by_president(rates_by_president, output_dir):\n",
    "    \"\"\"Plot approval rates by race and presidential administration\"\"\"\n",
    "    print(\"\\n Creating approval rate plots by presidential administration...\")\n",
    "    \n",
    "    approval_pivot = rates_by_president.pivot(index='race', columns='president', values='approval_rate').fillna(0)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    approval_pivot.plot(kind='bar', ax=ax, color=['#d62728', '#1f77b4'], alpha=0.8, width=0.7)\n",
    "    \n",
    "    ax.set_title('Loan Approval Rates by Race in Democratic Counties\\nDuring Different Presidential Administrations', \n",
    "                fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.set_ylabel('Approval Rate (%)', fontsize=12)\n",
    "    ax.set_xlabel('Race', fontsize=12)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.legend(title='Presidential Administration', title_fontsize=12, fontsize=11)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt='%.1f%%', fontsize=9, rotation=0, padding=3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/approval_rates_by_president_dem_counties.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(f\"{output_dir}/approval_rates_by_president_dem_counties.svg\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return approval_pivot\n",
    "\n",
    "def plot_approval_rate_differences_by_president(rates_by_president, output_dir):\n",
    "    \"\"\"Plot the difference in approval rates between presidential administrations\"\"\"\n",
    "    print(\"\\n Creating presidential approval rate difference analysis...\")\n",
    "    \n",
    "    approval_pivot = rates_by_president.pivot(index='race', columns='president', values='approval_rate').fillna(0)\n",
    "    \n",
    "    if 'Biden (Democrat)' in approval_pivot.columns and 'Trump (Republican)' in approval_pivot.columns:\n",
    "        approval_pivot['Difference (Biden - Trump)'] = (approval_pivot['Biden (Democrat)'] - \n",
    "                                                       approval_pivot['Trump (Republican)'])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    colors = ['#2ca02c' if x > 0 else '#d62728' for x in approval_pivot['Difference (Biden - Trump)']]\n",
    "    bars = ax.bar(range(len(approval_pivot.index)), approval_pivot['Difference (Biden - Trump)'], \n",
    "                  color=colors, alpha=0.7, width=0.6)\n",
    "    \n",
    "    ax.set_title('Approval Rate Differences by Race in Democratic Counties\\n(Biden Administration - Trump Administration)', \n",
    "                fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.set_ylabel('Approval Rate Difference (Percentage Points)', fontsize=12)\n",
    "    ax.set_xlabel('Race', fontsize=12)\n",
    "    ax.set_xticks(range(len(approval_pivot.index)))\n",
    "    ax.set_xticklabels(approval_pivot.index, rotation=45, ha='right')\n",
    "    ax.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:+.1f}pp',\n",
    "                   xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                   xytext=(0, 3 if height > 0 else -15), \n",
    "                   textcoords=\"offset points\",\n",
    "                   ha='center', va='bottom' if height > 0 else 'top', \n",
    "                   fontweight='bold', fontsize=10)\n",
    "    \n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor='#2ca02c', alpha=0.7, label='Higher under Biden'),\n",
    "                      Patch(facecolor='#d62728', alpha=0.7, label='Higher under Trump')]\n",
    "    ax.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/approval_rate_differences_by_president.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(f\"{output_dir}/approval_rate_differences_by_president.svg\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return approval_pivot\n",
    "\n",
    "def plot_yearly_trends_by_president(df, output_dir):\n",
    "    \"\"\"Plot yearly approval rate trends with presidential transition highlighted\"\"\"\n",
    "    print(\"\\n Creating yearly trends with presidential transition...\")\n",
    "    \n",
    "    yearly_rates = df.groupby(['year', 'race']).agg({\n",
    "        'approved': 'sum',\n",
    "        'denied': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    yearly_rates['total_applications'] = yearly_rates['approved'] + yearly_rates['denied']\n",
    "    yearly_rates['approval_rate'] = (yearly_rates['approved'] / yearly_rates['total_applications'] * 100)\n",
    "    yearly_rates['president'] = yearly_rates['year'].apply(lambda x: \"Trump\" if x <= 2020 else \"Biden\")\n",
    "    \n",
    "    races = sorted(yearly_rates['race'].unique())\n",
    "    fig, axes = plt.subplots(len(races), 1, figsize=(14, 4*len(races)))\n",
    "    \n",
    "    if len(races) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, race in enumerate(races):\n",
    "        race_data = yearly_rates[yearly_rates['race'] == race]\n",
    "        \n",
    "        axes[i].plot(race_data['year'], race_data['approval_rate'], \n",
    "                    marker='o', linewidth=3, markersize=10, \n",
    "                    color='#1f77b4', alpha=0.8, label='Approval Rate')\n",
    "        \n",
    "        axes[i].axvline(x=2020.5, color='red', linestyle='--', alpha=0.7, linewidth=2, \n",
    "                       label='Presidential Transition')\n",
    "        \n",
    "        trump_years = race_data[race_data['president'] == 'Trump']['year']\n",
    "        biden_years = race_data[race_data['president'] == 'Biden']['year']\n",
    "        \n",
    "        if len(trump_years) > 0:\n",
    "            axes[i].axvspan(trump_years.min() - 0.5, 2020.5, alpha=0.1, color='red', label='Trump Era')\n",
    "        if len(biden_years) > 0:\n",
    "            axes[i].axvspan(2020.5, biden_years.max() + 0.5, alpha=0.1, color='blue', label='Biden Era')\n",
    "        \n",
    "        axes[i].set_title(f'Approval Rate Trends in Democratic Counties: {race}', \n",
    "                         fontsize=14, fontweight='bold')\n",
    "        axes[i].set_ylabel('Approval Rate (%)')\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        axes[i].set_ylim(90, 100)  # Start y-axis at 90%\n",
    "        \n",
    "        for _, row in race_data.iterrows():\n",
    "            axes[i].annotate(f'{row[\"approval_rate\"]:.1f}%',\n",
    "                           xy=(row['year'], row['approval_rate']),\n",
    "                           xytext=(0, 10), \n",
    "                           textcoords=\"offset points\",\n",
    "                           ha='center', va='bottom', \n",
    "                           fontsize=9, fontweight='bold')\n",
    "    \n",
    "    axes[-1].set_xlabel('Year')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/yearly_trends_presidential_transition.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(f\"{output_dir}/yearly_trends_presidential_transition.svg\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def create_summary_table(rates_by_president, output_dir):\n",
    "    \"\"\"Create a detailed summary table\"\"\"\n",
    "    print(\"\\n Creating summary table...\")\n",
    "    \n",
    "    summary_pivot = rates_by_president.pivot(index='race', columns='president', \n",
    "                                           values=['approval_rate', 'total_applications'])\n",
    "    \n",
    "    summary_pivot.columns = [f\"{col[1]}_{col[0]}\" for col in summary_pivot.columns]\n",
    "    \n",
    "    if 'Biden (Democrat)_approval_rate' in summary_pivot.columns and 'Trump (Republican)_approval_rate' in summary_pivot.columns:\n",
    "        summary_pivot['Approval_Rate_Difference_Biden_minus_Trump'] = (\n",
    "            summary_pivot['Biden (Democrat)_approval_rate'] - \n",
    "            summary_pivot['Trump (Republican)_approval_rate']\n",
    "        )\n",
    "    \n",
    "    print(\" Summary by Race and Presidential Administration:\")\n",
    "    print(summary_pivot.round(2))\n",
    "    \n",
    "    summary_pivot.round(2).to_csv(f\"{output_dir}/presidential_analysis_summary.csv\")\n",
    "    \n",
    "    return summary_pivot\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"️ Presidential Administration Analysis: Loan Approval Rates in Democratic Counties\")\n",
    "    print(\"=\" * 90)\n",
    "    \n",
    "    df = load_and_prepare_data()\n",
    "    if df is None:\n",
    "        return\n",
    "    \n",
    "    rates_by_president = calculate_rates_by_president(df)\n",
    "    \n",
    "    approval_pivot = plot_approval_rates_by_president(rates_by_president, output_dir)\n",
    "    plot_approval_rate_differences_by_president(rates_by_president, output_dir)\n",
    "    plot_yearly_trends_by_president(df, output_dir)\n",
    "    \n",
    "    summary = create_summary_table(rates_by_president, output_dir)\n",
    "    \n",
    "    print(f\"\\n Presidential analysis complete! Check the '{output_dir}' folder for:\")\n",
    "    print(\"   PNG files:\")\n",
    "    print(\"     - approval_rates_by_president_dem_counties.png\")\n",
    "    print(\"     - approval_rate_differences_by_president.png\")\n",
    "    print(\"     - yearly_trends_presidential_transition.png\")\n",
    "    print(\"   SVG files:\")\n",
    "    print(\"     - approval_rates_by_president_dem_counties.svg\") \n",
    "    print(\"     - approval_rate_differences_by_president.svg\")\n",
    "    print(\"     - yearly_trends_presidential_transition.svg\")\n",
    "    print(\"   Data file:\")\n",
    "    print(\"     - presidential_analysis_summary.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f2ed0e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d362ab-1b6f-475e-a9d1-046aad4f62e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import os\n",
    "\n",
    "DEBUG = True\n",
    "\n",
    "processed_data_dir = \"processed_data\"\n",
    "output_dir = \"plots\"\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG if DEBUG else logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s'\n",
    ")\n",
    "\n",
    "Path(output_dir).mkdir(exist_ok=True)\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "def load_and_prepare_state_data():\n",
    "    \"\"\"Load processed data and prepare for state-level analysis\"\"\"\n",
    "    logging.info(\" Loading and preparing data for state-level analysis...\")\n",
    "\n",
    "    try:\n",
    "        file_path = f\"{processed_data_dir}/cleaned_raw_data.csv\"\n",
    "        if not os.path.exists(file_path):\n",
    "            logging.error(f\"File not found: {file_path}\")\n",
    "            return None\n",
    "        df = pd.read_csv(file_path)\n",
    "        logging.info(f\"    Loaded data: {len(df)} applications\")\n",
    "\n",
    "        if DEBUG:\n",
    "            logging.debug(\"First few rows of loaded data:\\n%s\", df.head())\n",
    "\n",
    "        df['county_fips'] = df['county_fips'].astype(str).str.zfill(5)\n",
    "        df['state_fips'] = df['county_fips'].str[:2]\n",
    "\n",
    "        state_fips_map = {\n",
    "            '01': 'Alabama', '02': 'Alaska', '04': 'Arizona', '05': 'Arkansas', '06': 'California',\n",
    "            '08': 'Colorado', '09': 'Connecticut', '10': 'Delaware', '11': 'DC', '12': 'Florida',\n",
    "            '13': 'Georgia', '15': 'Hawaii', '16': 'Idaho', '17': 'Illinois', '18': 'Indiana',\n",
    "            '19': 'Iowa', '20': 'Kansas', '21': 'Kentucky', '22': 'Louisiana', '23': 'Maine',\n",
    "            '24': 'Maryland', '25': 'Massachusetts', '26': 'Michigan', '27': 'Minnesota', '28': 'Mississippi',\n",
    "            '29': 'Missouri', '30': 'Montana', '31': 'Nebraska', '32': 'Nevada', '33': 'New Hampshire',\n",
    "            '34': 'New Jersey', '35': 'New Mexico', '36': 'New York', '37': 'North Carolina', '38': 'North Dakota',\n",
    "            '39': 'Ohio', '40': 'Oklahoma', '41': 'Oregon', '42': 'Pennsylvania', '44': 'Rhode Island',\n",
    "            '45': 'South Carolina', '46': 'South Dakota', '47': 'Tennessee', '48': 'Texas', '49': 'Utah',\n",
    "            '50': 'Vermont', '51': 'Virginia', '53': 'Washington', '54': 'West Virginia', '55': 'Wisconsin',\n",
    "            '56': 'Wyoming'\n",
    "        }\n",
    "\n",
    "        df['state_name'] = df['state_fips'].map(state_fips_map)\n",
    "        prev_len = len(df)\n",
    "        df = df.dropna(subset=['state_name'])  # Remove unmapped states\n",
    "        logging.info(f\"    Mapped to states: {len(df)} applications (removed {prev_len-len(df)})\")\n",
    "        logging.info(f\"   ️ States available: {sorted(df['state_name'].unique())}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.exception(\" Error loading data:\")\n",
    "        return None\n",
    "\n",
    "def determine_state_political_lean(df):\n",
    "    \"\"\"Determine each state's political lean based on county voting patterns\"\"\"\n",
    "    logging.info(\"\\n️ Determining state political leans...\")\n",
    "\n",
    "    try:\n",
    "        state_lean = df.groupby(['state_name', 'election_lean']).agg({\n",
    "            'approved': 'count',\n",
    "            'denied': 'count'\n",
    "        }).reset_index()\n",
    "        state_lean['total_apps'] = state_lean['approved'] + state_lean['denied']\n",
    "\n",
    "        state_totals = state_lean.groupby(['state_name', 'election_lean'])['total_apps'].sum().reset_index()\n",
    "        state_pivot = state_totals.pivot(index='state_name', columns='election_lean', values='total_apps').fillna(0)\n",
    "\n",
    "        if 'Trump' in state_pivot.columns and 'Biden' in state_pivot.columns:\n",
    "            state_pivot['state_political_lean'] = (state_pivot['Biden'] > state_pivot['Trump']).map({\n",
    "                True: 'Democratic State', \n",
    "                False: 'Republican State'\n",
    "            })\n",
    "        else:\n",
    "            state_pivot['state_political_lean'] = 'Mixed/Unknown'\n",
    "\n",
    "        state_pivot['total_apps'] = state_pivot.get('Biden', 0) + state_pivot.get('Trump', 0)\n",
    "        state_pivot['biden_pct'] = (state_pivot.get('Biden', 0) / state_pivot['total_apps'] * 100).round(1)\n",
    "        state_pivot['trump_pct'] = (state_pivot.get('Trump', 0) / state_pivot['total_apps'] * 100).round(1)\n",
    "\n",
    "        logging.info(\" State political leans (based on loan application distribution):\")\n",
    "        state_summary = state_pivot[['state_political_lean', 'biden_pct', 'trump_pct', 'total_apps']].copy()\n",
    "        logging.info(\"\\n%s\", state_summary.head(15))\n",
    "\n",
    "        return state_pivot.reset_index()[['state_name', 'state_political_lean', 'biden_pct', 'trump_pct']]\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Error in determine_state_political_lean\")\n",
    "        return None\n",
    "\n",
    "def calculate_state_approval_rates(df, state_lean_df):\n",
    "    \"\"\"Calculate approval rates by state and race\"\"\"\n",
    "    logging.info(\"\\n Calculating approval rates by state and race...\")\n",
    "\n",
    "    try:\n",
    "        df_with_state_lean = df.merge(state_lean_df, on='state_name', how='left')\n",
    "        state_rates = df_with_state_lean.groupby(['state_name', 'state_political_lean', 'race']).agg({\n",
    "            'approved': 'sum',\n",
    "            'denied': 'sum'\n",
    "        }).reset_index()\n",
    "        state_rates['total_applications'] = state_rates['approved'] + state_rates['denied']\n",
    "        state_rates['approval_rate'] = (state_rates['approved'] / state_rates['total_applications'] * 100).round(2)\n",
    "        state_rates = state_rates[state_rates['total_applications'] >= 100].copy()\n",
    "        logging.info(f\"    Calculated rates for {len(state_rates)} state-race combinations\")\n",
    "        logging.info(f\"   ️ States included: {len(state_rates['state_name'].unique())}\")\n",
    "        logging.info(f\"    Races included: {sorted(state_rates['race'].unique())}\")\n",
    "        return state_rates\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Error in calculate_state_approval_rates\")\n",
    "        return None\n",
    "\n",
    "def create_state_facet_plot(state_rates, output_dir, max_states_per_plot=20):\n",
    "    \"\"\"Create facet plots showing approval rates by race for each state\"\"\"\n",
    "    logging.info(f\"\\n Creating state facet plots (max {max_states_per_plot} states per plot)...\")\n",
    "\n",
    "    try:\n",
    "        states = sorted(state_rates['state_name'].unique())\n",
    "        n_plots = (len(states) + max_states_per_plot - 1) // max_states_per_plot\n",
    "\n",
    "        for plot_num in range(n_plots):\n",
    "            start_idx = plot_num * max_states_per_plot\n",
    "            end_idx = min((plot_num + 1) * max_states_per_plot, len(states))\n",
    "            states_chunk = states[start_idx:end_idx]\n",
    "            chunk_data = state_rates[state_rates['state_name'].isin(states_chunk)].copy()\n",
    "\n",
    "            n_states = len(states_chunk)\n",
    "            cols = min(4, n_states)\n",
    "            rows = (n_states + cols - 1) // cols\n",
    "\n",
    "            fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 4*rows))\n",
    "            if n_states == 1:\n",
    "                axes = [axes]\n",
    "            elif rows == 1:\n",
    "                axes = axes.reshape(1, -1)\n",
    "            axes_flat = axes.flatten() if n_states > 1 else axes\n",
    "            for i, state in enumerate(states_chunk):\n",
    "                ax = axes_flat[i]\n",
    "                state_data = chunk_data[chunk_data['state_name'] == state]\n",
    "                if len(state_data) == 0:\n",
    "                    ax.set_visible(False)\n",
    "                    continue\n",
    "                state_lean = state_data['state_political_lean'].iloc[0]\n",
    "                color_map = {'Democratic State': '#1f77b4', 'Republican State': '#d62728', 'Mixed/Unknown': '#AAAAAA'}\n",
    "                main_color = color_map.get(state_lean, '#AAAAAA')\n",
    "                races = state_data['race'].unique()\n",
    "                x_pos = np.arange(len(races))\n",
    "                bars = ax.bar(x_pos, state_data['approval_rate'], color=main_color, alpha=0.7, width=0.6)\n",
    "                ax.set_title(f'{state}\\n({state_lean})', fontsize=10, fontweight='bold', pad=10)\n",
    "                ax.set_ylabel('Approval Rate (%)', fontsize=9)\n",
    "                ax.set_xticks(x_pos)\n",
    "                ax.set_xticklabels([race.replace(' ', '\\n') for race in races], fontsize=8, rotation=0)\n",
    "                ax.grid(True, alpha=0.3, axis='y')\n",
    "                ax.set_ylim(85, 100)\n",
    "                for bar, rate in zip(bars, state_data['approval_rate']):\n",
    "                    height = bar.get_height()\n",
    "                    ax.annotate(f'{rate:.1f}%',\n",
    "                               xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                               xytext=(0, 3),\n",
    "                               textcoords=\"offset points\",\n",
    "                               ha='center', va='bottom',\n",
    "                               fontsize=8, fontweight='bold')\n",
    "            for i in range(n_states, len(axes_flat)):\n",
    "                axes_flat[i].set_visible(False)\n",
    "            fig.suptitle(f'Loan Approval Rates by Race and State Political Lean (Part {plot_num + 1})',\n",
    "                        fontsize=16, fontweight='bold', y=0.98)\n",
    "            plt.tight_layout()\n",
    "            plt.subplots_adjust(top=0.93)\n",
    "            filename = f\"state_facet_approval_rates_part_{plot_num + 1}\" if n_plots > 1 else \"state_facet_approval_rates\"\n",
    "            plt.savefig(f\"{output_dir}/{filename}.png\", dpi=300, bbox_inches='tight')\n",
    "            plt.savefig(f\"{output_dir}/{filename}.svg\", bbox_inches='tight')\n",
    "            plt.show()\n",
    "            logging.info(f\"    Created plot {plot_num + 1}/{n_plots} with {len(states_chunk)} states\")\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Error in create_state_facet_plot\")\n",
    "\n",
    "def create_summary_by_state_lean(state_rates, output_dir):\n",
    "    \"\"\"Create summary comparing Democratic vs Republican states\"\"\"\n",
    "    logging.info(\"\\n Creating summary by state political lean...\")\n",
    "\n",
    "    try:\n",
    "        summary = state_rates.groupby(['state_political_lean', 'race']).agg({\n",
    "            'approval_rate': ['mean', 'std', 'count'],\n",
    "            'total_applications': 'sum'\n",
    "        }).round(2)\n",
    "        summary.columns = ['_'.join(col).strip() for col in summary.columns]\n",
    "        summary = summary.reset_index()\n",
    "        logging.info(\" Average approval rates by state political lean and race:\\n%s\", summary)\n",
    "        summary.to_csv(f\"{output_dir}/state_political_lean_summary.csv\", index=False)\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        plot_data = state_rates.groupby(['state_political_lean', 'race'])['approval_rate'].mean().reset_index()\n",
    "        plot_pivot = plot_data.pivot(index='race', columns='state_political_lean', values='approval_rate')\n",
    "        plot_pivot.plot(kind='bar', ax=ax, color=['#1f77b4', '#d62728'], alpha=0.8, width=0.7)\n",
    "        ax.set_title('Average Loan Approval Rates by Race\\nComparing Democratic vs Republican States',\n",
    "                    fontsize=16, fontweight='bold', pad=20)\n",
    "        ax.set_ylabel('Average Approval Rate (%)', fontsize=12)\n",
    "        ax.set_xlabel('Race', fontsize=12)\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        ax.legend(title='State Political Lean', title_fontsize=12, fontsize=11)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        for container in ax.containers:\n",
    "            ax.bar_label(container, fmt='%.1f%%', fontsize=9, rotation=0, padding=3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/approval_rates_dem_vs_rep_states.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.savefig(f\"{output_dir}/approval_rates_dem_vs_rep_states.svg\", bbox_inches='tight')\n",
    "        plt.show()\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Error in create_summary_by_state_lean\")\n",
    "        return None\n",
    "\n",
    "def create_state_ranking(state_rates, output_dir):\n",
    "    \"\"\"Create ranking of states by approval rates for each race\"\"\"\n",
    "    logging.info(\"\\n Creating state rankings by race...\")\n",
    "\n",
    "    try:\n",
    "        for race in sorted(state_rates['race'].unique()):\n",
    "            race_data = state_rates[state_rates['race'] == race].copy()\n",
    "            race_data = race_data.sort_values('approval_rate', ascending=False)\n",
    "            fig, ax = plt.subplots(figsize=(12, max(8, len(race_data) * 0.4)))\n",
    "            colors = [('#1f77b4' if lean == 'Democratic State' else '#d62728') \n",
    "                     for lean in race_data['state_political_lean']]\n",
    "            bars = ax.barh(range(len(race_data)), race_data['approval_rate'], \n",
    "                          color=colors, alpha=0.7)\n",
    "            ax.set_yticks(range(len(race_data)))\n",
    "            ax.set_yticklabels([f\"{state} ({lean.split()[0]})\" \n",
    "                               for state, lean in zip(race_data['state_name'], race_data['state_political_lean'])],\n",
    "                              fontsize=10)\n",
    "            ax.set_xlabel('Approval Rate (%)', fontsize=12)\n",
    "            ax.set_title(f'State Rankings: Loan Approval Rates for {race} Applicants', \n",
    "                        fontsize=14, fontweight='bold', pad=20)\n",
    "            ax.grid(True, alpha=0.3, axis='x')\n",
    "            for i, (bar, rate) in enumerate(zip(bars, race_data['approval_rate'])):\n",
    "                width = bar.get_width()\n",
    "                ax.annotate(f'{rate:.1f}%',\n",
    "                           xy=(width, bar.get_y() + bar.get_height() / 2),\n",
    "                           xytext=(3, 0), \n",
    "                           textcoords=\"offset points\",\n",
    "                           ha='left', va='center', \n",
    "                           fontsize=9, fontweight='bold')\n",
    "            from matplotlib.patches import Patch\n",
    "            legend_elements = [Patch(facecolor='#1f77b4', alpha=0.7, label='Democratic State'),\n",
    "                              Patch(facecolor='#d62728', alpha=0.7, label='Republican State')]\n",
    "            ax.legend(handles=legend_elements, loc='lower right')\n",
    "            plt.tight_layout()\n",
    "            race_clean = race.replace(' ', '_').replace('/', '_')\n",
    "            plt.savefig(f\"{output_dir}/state_ranking_{race_clean}.png\", dpi=300, bbox_inches='tight')\n",
    "            plt.savefig(f\"{output_dir}/state_ranking_{race_clean}.svg\", bbox_inches='tight')\n",
    "            plt.show()\n",
    "            logging.info(f\"    Created ranking for {race}\")\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Error in create_state_ranking\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    logging.info(\"️ State-Level Analysis: Approval Rates by Race and State Political Lean\")\n",
    "    logging.info(\"=\" * 80)\n",
    "    df = load_and_prepare_state_data()\n",
    "    if df is None:\n",
    "        return\n",
    "    state_lean_df = determine_state_political_lean(df)\n",
    "    if state_lean_df is None:\n",
    "        return\n",
    "    state_rates = calculate_state_approval_rates(df, state_lean_df)\n",
    "    if state_rates is None:\n",
    "        return\n",
    "    create_state_facet_plot(state_rates, output_dir)\n",
    "    summary = create_summary_by_state_lean(state_rates, output_dir)\n",
    "    create_state_ranking(state_rates, output_dir)\n",
    "    state_rates.to_csv(f\"{output_dir}/detailed_state_approval_rates.csv\", index=False)\n",
    "    logging.info(f\"\\n State analysis complete! Check the '{output_dir}' folder for:\")\n",
    "    logging.info(\"   Facet plots showing each state:\")\n",
    "    logging.info(\"     - state_facet_approval_rates.png (or multiple parts)\")\n",
    "    logging.info(\"   Summary comparisons:\")\n",
    "    logging.info(\"     - approval_rates_dem_vs_rep_states.png\")\n",
    "    logging.info(\"   State rankings by race:\")\n",
    "    logging.info(\"     - state_ranking_[race].png (one per race)\")\n",
    "    logging.info(\"   Data files:\")\n",
    "    logging.info(\"     - state_political_lean_summary.csv\")\n",
    "    logging.info(\"     - detailed_state_approval_rates.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccd94b5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7a884b-07b4-40d8-a963-c4dd2163b270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_folder = \"filtered_data_strict\"  # Change to equity folder if needed\n",
    "output_file = \"avg_interest_vs_approval_by_lei_race_year.csv\"\n",
    "years = range(2018, 2025)\n",
    "\n",
    "race_map = {\n",
    "    2.0: 'Asian', 21: 'Asian', 22: 'Asian', 23: 'Asian', 24: 'Asian', 25: 'Asian', 26: 'Asian', 27: 'Asian',\n",
    "    3.0: 'Black',\n",
    "    5.0: 'White'\n",
    "}\n",
    "\n",
    "def classify_lender_from_purchaser_type(ptype):\n",
    "    try:\n",
    "        if int(ptype) == 6:  # 6 means banks\n",
    "            return \"Bank\"\n",
    "        else:\n",
    "            return \"Non-Bank\"\n",
    "    except:\n",
    "        return \"Unknown\"\n",
    "\n",
    "rows = []\n",
    "\n",
    "for year in years:\n",
    "    file_path = os.path.join(data_folder, f\"{year}_filtered_hmda.csv\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\" Skipping {year}, file not found: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    print(f\" Loading {file_path}\")\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "    race_col = 'applicant_race-1' if 'applicant_race-1' in df.columns else 'applicant_race_1'\n",
    "    df['race_label'] = df[race_col].map(race_map)\n",
    "    df = df[df['race_label'].notna()]\n",
    "\n",
    "    if 'interest_rate' not in df.columns:\n",
    "        print(f\"️ No interest_rate column for {year}, skipping.\")\n",
    "        continue\n",
    "    df['interest_rate'] = pd.to_numeric(df['interest_rate'], errors='coerce')\n",
    "\n",
    "    df['lender_type'] = df['purchaser_type'].apply(classify_lender_from_purchaser_type)\n",
    "\n",
    "    approved = df[df['action_taken'].isin([1, 2])]\n",
    "    total = df[df['action_taken'].isin([1, 2, 3])]\n",
    "\n",
    "    app_counts = (\n",
    "        approved.groupby(['lei', 'race_label'])\n",
    "        .size().reset_index(name='approved_count')\n",
    "    )\n",
    "    total_counts = (\n",
    "        total.groupby(['lei', 'race_label'])\n",
    "        .size().reset_index(name='total_applications')\n",
    "    )\n",
    "\n",
    "    merged = total_counts.merge(app_counts, on=['lei', 'race_label'], how='left')\n",
    "    merged['approved_count'] = merged['approved_count'].fillna(0)\n",
    "    merged['approval_rate'] = merged['approved_count'] / merged['total_applications']\n",
    "\n",
    "    avg_ir = (\n",
    "        df.groupby(['lei', 'race_label'])['interest_rate']\n",
    "        .mean().reset_index(name='avg_interest_rate')\n",
    "    )\n",
    "\n",
    "    lender_type_map = df.groupby('lei')['lender_type'].first().to_dict()\n",
    "\n",
    "    merged = merged.merge(avg_ir, on=['lei', 'race_label'], how='left')\n",
    "    merged['year'] = year\n",
    "    merged['lender_type'] = merged['lei'].map(lender_type_map)\n",
    "\n",
    "    rows.append(merged)\n",
    "\n",
    "result_df = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "result_df.to_csv(output_file, index=False)\n",
    "print(f\" Saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ae149d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42af833-ca52-46a3-bc97-b27399c7cd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_nonbank_avg = (\n",
    "    result_df\n",
    "    .groupby(['year', 'race_label', 'lender_type'])['avg_interest_rate']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .sort_values(['year', 'race_label', 'lender_type'])\n",
    ")\n",
    "\n",
    "bank_nonbank_avg.to_csv(\"bank_nonbank_interest_rate_by_race_year.csv\", index=False)\n",
    "print(\" Saved bank vs non-bank averages to bank_nonbank_interest_rate_by_race_year.csv\")\n",
    "\n",
    "print(bank_nonbank_avg.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff21115",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90abb78-90fa-4e6a-9c1d-28214b087b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"bank_nonbank_interest_rate_by_race_year.csv\")\n",
    "\n",
    "counts = (\n",
    "    result_df  # This is the LEI-level dataset before grouping\n",
    "    .groupby(['year', 'race_label', 'lender_type'])\n",
    "    .size()\n",
    "    .reset_index(name='num_data_points')\n",
    ")\n",
    "\n",
    "bank_nonbank_with_counts = df.merge(counts, on=['year', 'race_label', 'lender_type'], how='left')\n",
    "\n",
    "bank_nonbank_with_counts.to_csv(\"bank_nonbank_interest_rate_with_counts.csv\", index=False)\n",
    "print(bank_nonbank_with_counts.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d74f1e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35ae967-9201-4259-9eb3-e1737f3e5988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"bank_nonbank_interest_rate_by_race_year.csv\")\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "g = sns.catplot(\n",
    "    data=df,\n",
    "    x=\"year\", y=\"avg_interest_rate\",\n",
    "    hue=\"lender_type\",\n",
    "    col=\"race_label\",\n",
    "    kind=\"bar\",\n",
    "    height=4, aspect=1.2,\n",
    "    palette=\"Set2\",\n",
    "    ci=None\n",
    ")\n",
    "\n",
    "g.set_titles(\"{col_name}\")\n",
    "g.set_axis_labels(\"Year\", \"Average Interest Rate\")\n",
    "g.add_legend(title=\"Lender Type\")\n",
    "\n",
    "plt.subplots_adjust(top=0.85)\n",
    "g.fig.suptitle(\"Bank vs Non-Bank Average Interest Rates by Race and Year\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189ffffc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ae26dd-b6f2-4205-ab12-2a83f2a58d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "from pathlib import Path\n",
    "\n",
    "CSV_PATH = \"avg_interest_vs_approval_by_lei_race_year.csv\"\n",
    "\n",
    "OUT_DIR = Path(\".\")\n",
    "APPROVAL_PNG = OUT_DIR / \"banks_vs_nonbanks_approval_facets.png\"\n",
    "APPROVAL_SVG = OUT_DIR / \"banks_vs_nonbanks_approval_facets.svg\"\n",
    "INTEREST_PNG = OUT_DIR / \"banks_vs_nonbanks_interest_facets.png\"\n",
    "INTEREST_SVG = OUT_DIR / \"banks_vs_nonbanks_interest_facets.svg\"\n",
    "\n",
    "APPROVAL_YLIM  = (0, 100)         # percent\n",
    "INTEREST_YLIM  = (0, 7.5)         # interest rate\n",
    "INTEREST_TICKS = np.arange(0, 7.5, 0.5)\n",
    "BAR_WIDTH      = 0.38\n",
    "LENDER_ORDER   = [\"Bank\", \"Non-Bank\"]  # enforce order\n",
    "COLORS         = {\"Bank\": \"#66c2a5\", \"Non-Bank\": \"#fc8d62\"}\n",
    "MARKERS        = {\"Bank\": \"o\", \"Non-Bank\": \"s\"}\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "df[\"year\"] = (\n",
    "    df[\"year\"].astype(str).str.extract(r\"(\\d{4})\")[0].astype(float)\n",
    ")\n",
    "df = df.dropna(subset=[\"year\"]).copy()\n",
    "df[\"year\"] = df[\"year\"].astype(int)\n",
    "\n",
    "if df[\"approval_rate\"].max() <= 1.00001:\n",
    "    df[\"approval_rate\"] = df[\"approval_rate\"] * 100.0\n",
    "\n",
    "df = df[df[\"lender_type\"].isin(LENDER_ORDER)].copy()\n",
    "df[\"lender_type\"] = pd.Categorical(df[\"lender_type\"], categories=LENDER_ORDER, ordered=True)\n",
    "\n",
    "agg = (\n",
    "    df.groupby([\"year\", \"race_label\", \"lender_type\"], dropna=False)\n",
    "      .agg(\n",
    "          avg_interest_rate=(\"avg_interest_rate\", \"mean\"),\n",
    "          avg_approval_rate=(\"approval_rate\", \"mean\")\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "races = sorted(agg[\"race_label\"].dropna().unique().tolist())\n",
    "\n",
    "def _years_x(sub):\n",
    "    years = sorted(sub[\"year\"].unique())\n",
    "    x = np.arange(len(years))\n",
    "    return years, x\n",
    "\n",
    "def _vals_by(sub, years, col, lender_type):\n",
    "    vals = []\n",
    "    for y in years:\n",
    "        chunk = sub[(sub[\"year\"] == y) & (sub[\"lender_type\"] == lender_type)]\n",
    "        vals.append(chunk[col].iloc[0] if not chunk.empty else np.nan)\n",
    "    return vals\n",
    "\n",
    "fig1, axes1 = plt.subplots(1, max(1, len(races)), figsize=(6 * max(1, len(races)), 5), sharey=False)\n",
    "if len(races) == 1:\n",
    "    axes1 = [axes1]\n",
    "\n",
    "for ax, race in zip(axes1, races):\n",
    "    sub = agg[agg[\"race_label\"] == race].copy()\n",
    "    years, x = _years_x(sub)\n",
    "\n",
    "    for i, lt in enumerate(LENDER_ORDER):\n",
    "        ys = _vals_by(sub, years, \"avg_approval_rate\", lt)\n",
    "        offset = (i - (len(LENDER_ORDER)-1)/2) * BAR_WIDTH\n",
    "        ax.bar(x + offset, ys, BAR_WIDTH, color=COLORS[lt], label=lt)\n",
    "\n",
    "    ax.set_title(str(race))\n",
    "    ax.set_xlabel(\"Year\")\n",
    "    ax.set_ylabel(\"Approval Rate (%)\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([str(y) for y in years])\n",
    "    ax.set_ylim(*APPROVAL_YLIM)\n",
    "    ax.grid(True, axis=\"y\", alpha=0.25)\n",
    "\n",
    "legend_handles = [Patch(facecolor=COLORS[lt], label=lt) for lt in LENDER_ORDER]\n",
    "fig1.legend(\n",
    "    handles=legend_handles,\n",
    "    labels=[h.get_label() for h in legend_handles],\n",
    "    loc=\"upper center\",\n",
    "    ncol=len(LENDER_ORDER),\n",
    "    frameon=False,\n",
    "    bbox_to_anchor=(0.5, 1.03)\n",
    ")\n",
    "plt.suptitle(\"Bank vs Non-Bank: Approval Rates by Race and Year\", y=0.98)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.94])\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "fig1.savefig(APPROVAL_PNG, dpi=300, bbox_inches=\"tight\")\n",
    "fig1.savefig(APPROVAL_SVG, bbox_inches=\"tight\")\n",
    "print(f\"Saved: {APPROVAL_PNG}\")\n",
    "print(f\"Saved: {APPROVAL_SVG}\")\n",
    "\n",
    "fig2, axes2 = plt.subplots(1, max(1, len(races)), figsize=(6 * max(1, len(races)), 5), sharey=False)\n",
    "if len(races) == 1:\n",
    "    axes2 = [axes2]\n",
    "\n",
    "for ax, race in zip(axes2, races):\n",
    "    sub = agg[agg[\"race_label\"] == race].copy()\n",
    "    years, x = _years_x(sub)\n",
    "\n",
    "    for lt in LENDER_ORDER:\n",
    "        ys = _vals_by(sub, years, \"avg_interest_rate\", lt)\n",
    "        ax.plot(x, ys, marker=MARKERS[lt], linewidth=2, color=COLORS[lt], label=lt)\n",
    "\n",
    "    ax.set_title(str(race))\n",
    "    ax.set_xlabel(\"Year\")\n",
    "    ax.set_ylabel(\"Average Interest Rate\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([str(y) for y in years])\n",
    "    ax.set_ylim(*INTEREST_YLIM)\n",
    "    ax.set_yticks(INTEREST_TICKS)\n",
    "    ax.grid(True, axis=\"y\", alpha=0.25)\n",
    "\n",
    "legend_handles2 = [\n",
    "    Line2D([0],[0], color=COLORS[lt], marker=MARKERS[lt], lw=2, label=lt) for lt in LENDER_ORDER\n",
    "]\n",
    "fig2.legend(\n",
    "    handles=legend_handles2,\n",
    "    labels=[h.get_label() for h in legend_handles2],\n",
    "    loc=\"upper center\",\n",
    "    ncol=len(LENDER_ORDER),\n",
    "    frameon=False,\n",
    "    bbox_to_anchor=(0.5, 1.03)\n",
    ")\n",
    "plt.suptitle(\"Bank vs Non-Bank: Interest Rates by Race and Year\", y=0.98)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.94])\n",
    "\n",
    "fig2.savefig(INTEREST_PNG, dpi=300, bbox_inches=\"tight\")\n",
    "fig2.savefig(INTEREST_SVG, bbox_inches=\"tight\")\n",
    "print(f\"Saved: {INTEREST_PNG}\")\n",
    "print(f\"Saved: {INTEREST_SVG}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23a2fb3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a63eb8e-66a0-49ae-88b3-a97998fe5e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_DIR = Path(\"filtered_data_strict\")  # folder with 2018_filtered_hmda.csv, 2019_filtered_hmda.csv, etc.\n",
    "PATTERN  = re.compile(r\"(\\d{4})_filtered_hmda\\.csv$\")\n",
    "OUT_CSV  = \"bank_nonbank_app_counts_by_year.csv\"\n",
    "OUT_PNG  = \"bank_nonbank_app_counts_by_year.png\"\n",
    "OUT_SVG  = \"bank_nonbank_app_counts_by_year.svg\"\n",
    "\n",
    "frames = []\n",
    "for f in DATA_DIR.glob(\"*_filtered_hmda.csv\"):\n",
    "    m = PATTERN.match(f.name)\n",
    "    if not m:\n",
    "        continue\n",
    "    year = int(m.group(1))\n",
    "    df = pd.read_csv(f, low_memory=False)\n",
    "\n",
    "    lt = (df.get(\"purchaser_type\")\n",
    "            .fillna(-1)\n",
    "            .astype(\"Int64\")\n",
    "            .apply(lambda x: \"Bank\" if x == 6 else \"Non-Bank\"))\n",
    "\n",
    "    frames.append(pd.DataFrame({\"year\": year, \"lender_type\": lt}))\n",
    "\n",
    "if not frames:\n",
    "    raise SystemExit(\"No *_filtered_hmda.csv files found. Check DATA_DIR and filenames.\")\n",
    "\n",
    "stack = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "counts = (\n",
    "    stack\n",
    "    .groupby([\"year\", \"lender_type\"])\n",
    "    .size()\n",
    "    .rename(\"applications\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "years = sorted(counts[\"year\"].unique())\n",
    "for y in years:\n",
    "    for lt in [\"Bank\", \"Non-Bank\"]:\n",
    "        if not ((counts[\"year\"] == y) & (counts[\"lender_type\"] == lt)).any():\n",
    "            counts = pd.concat([counts, pd.DataFrame({\"year\": [y], \"lender_type\": [lt], \"applications\": [0]})],\n",
    "                               ignore_index=True)\n",
    "\n",
    "counts = counts.sort_values([\"year\", \"lender_type\"]).reset_index(drop=True)\n",
    "counts.to_csv(OUT_CSV, index=False)\n",
    "print(f\"Saved counts -> {OUT_CSV}\")\n",
    "\n",
    "pivot = counts.pivot(index=\"year\", columns=\"lender_type\", values=\"applications\").fillna(0)\n",
    "pivot = pivot[[\"Bank\", \"Non-Bank\"]]  # fixed order if both exist\n",
    "\n",
    "ax = pivot.plot(kind=\"bar\", figsize=(10, 5))\n",
    "ax.set_title(\"Total Applications by Lender Type, Year by Year\")\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"Number of Applications\")\n",
    "ax.legend(title=\"Lender Type\", frameon=False)\n",
    "\n",
    "for p in ax.patches:\n",
    "    height = int(p.get_height())\n",
    "    if height > 0:\n",
    "        ax.annotate(f\"{height:,}\", (p.get_x() + p.get_width()/2, height),\n",
    "                    ha=\"center\", va=\"bottom\", fontsize=8, xytext=(0, 2), textcoords=\"offset points\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_PNG, dpi=300, bbox_inches=\"tight\")\n",
    "plt.savefig(OUT_SVG, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"Saved plot -> {OUT_PNG} / {OUT_SVG}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b151b92",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b90f0d-c13a-46fa-963f-42d887fefa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "from pathlib import Path\n",
    "from math import floor, log10\n",
    "\n",
    "CSV_PATH = \"avg_interest_vs_approval_by_lei_race_year.csv\"\n",
    "OUT_DIR = Path(\".\")\n",
    "OUT_PNG = OUT_DIR / \"faceted_total_approved_by_race.png\"\n",
    "OUT_SVG = OUT_DIR / \"faceted_total_approved_by_race.svg\"\n",
    "\n",
    "FACETS = [\"Non-Bank\", \"Bank\"]\n",
    "\n",
    "RACE_ORDER = None  # e.g. [\"Asian\",\"Black\",\"White\",\"Hispanic/Latino\",\"Other/Unknown\"]\n",
    "\n",
    "RACE_COLOR_MAP = {\n",
    "    \"Asian\":  \"#1f77b4\",\n",
    "    \"Black\":  \"#ff7f0e\",\n",
    "    \"White\":  \"#2ca02c\",\n",
    "}\n",
    "\n",
    "ANNOTATE_TOTALS = True\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "required = {\"year\", \"lender_type\", \"race_label\"}\n",
    "missing = required - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"CSV missing required columns: {missing}\")\n",
    "\n",
    "df[\"year\"] = df[\"year\"].astype(str).str.extract(r\"(\\d{4})\")[0].astype(float)\n",
    "df = df.dropna(subset=[\"year\"]).copy()\n",
    "df[\"year\"] = df[\"year\"].astype(int)\n",
    "\n",
    "LENDER_MAP = {\n",
    "    \"bank\": \"Bank\", \"banks\": \"Bank\",\n",
    "    \"non-bank\": \"Non-Bank\", \"nonbank\": \"Non-Bank\", \"non bank\": \"Non-Bank\"\n",
    "}\n",
    "def norm_lender(val: str) -> str:\n",
    "    s = str(val).strip().lower()\n",
    "    if s in LENDER_MAP: return LENDER_MAP[s]\n",
    "    if \"non\" in s and \"bank\" in s: return \"Non-Bank\"\n",
    "    if \"bank\" in s: return \"Bank\"\n",
    "    return val if val in (\"Bank\", \"Non-Bank\") else s.title()\n",
    "df[\"lender_type\"] = df[\"lender_type\"].apply(norm_lender)\n",
    "\n",
    "approved_cols = [c for c in df.columns if c.lower() in {\"approved_count\",\"n_approved\",\"num_approved\"}]\n",
    "if approved_cols:\n",
    "    df[\"approved_count\"] = pd.to_numeric(df[approved_cols[0]], errors=\"coerce\")\n",
    "else:\n",
    "    rate_col = \"approval_rate\" if \"approval_rate\" in df.columns else (\"avg_approval_rate\" if \"avg_approval_rate\" in df.columns else None)\n",
    "    total_candidates = [c for c in df.columns if c.lower() in {\"applications_total\",\"n_applications\",\"applications\",\"count\",\"n\"}]\n",
    "    if not rate_col or not total_candidates:\n",
    "        raise ValueError(\"Need 'approved_count' OR (approval_rate/avg_approval_rate AND total applications column).\")\n",
    "    total_col = total_candidates[0]\n",
    "    rate  = pd.to_numeric(df[rate_col],  errors=\"coerce\")\n",
    "    total = pd.to_numeric(df[total_col], errors=\"coerce\")\n",
    "    rate = np.where(rate > 1.0, rate / 100.0, rate)  # normalize to 0–1\n",
    "    df[\"approved_count\"] = (rate * total).round()\n",
    "\n",
    "df[\"approved_count\"] = pd.to_numeric(df[\"approved_count\"], errors=\"coerce\").fillna(0).clip(lower=0)\n",
    "\n",
    "agg = (\n",
    "    df.groupby([\"lender_type\",\"year\",\"race_label\"], dropna=False)[\"approved_count\"]\n",
    "      .sum()\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "all_years = sorted(agg[\"year\"].unique().tolist())\n",
    "\n",
    "if RACE_ORDER is not None:\n",
    "    race_order = RACE_ORDER\n",
    "else:\n",
    "    race_order = sorted(agg[\"race_label\"].dropna().unique().tolist())\n",
    "\n",
    "def build_pivot(ltype: str) -> pd.DataFrame:\n",
    "    sub = agg[agg[\"lender_type\"] == ltype].copy()\n",
    "    frame = (\n",
    "        sub.pivot_table(index=\"year\", columns=\"race_label\", values=\"approved_count\", aggfunc=\"sum\", fill_value=0)\n",
    "           .reindex(index=all_years, fill_value=0)\n",
    "    )\n",
    "    for r in race_order:\n",
    "        if r not in frame.columns:\n",
    "            frame[r] = 0\n",
    "    frame = frame.reindex(columns=race_order)\n",
    "    return frame\n",
    "\n",
    "facet_pivots = {ft: build_pivot(ft) for ft in FACETS}\n",
    "\n",
    "fallback_palette = plt.cm.tab10.colors\n",
    "race_colors = {}\n",
    "for i, r in enumerate(race_order):\n",
    "    race_colors[r] = RACE_COLOR_MAP.get(r, fallback_palette[i % len(fallback_palette)])\n",
    "\n",
    "def nice_ceil(x):\n",
    "    if x <= 0: return 1\n",
    "    exp = floor(log10(x)); frac = x / (10 ** exp)\n",
    "    nice_frac = 1 if frac <= 1 else 2 if frac <= 2 else 5 if frac <= 5 else 10\n",
    "    return int(nice_frac * (10 ** exp))\n",
    "\n",
    "global_max = max(p.sum(axis=1).max() for p in facet_pivots.values())\n",
    "ymax = nice_ceil(global_max)\n",
    "yticks = np.linspace(0, ymax, 6)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(FACETS), figsize=(12, 7), sharey=True)\n",
    "if len(FACETS) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "def plot_panel(ax, pivot_df: pd.DataFrame, title: str):\n",
    "    years = [str(y) for y in pivot_df.index.tolist()]\n",
    "    bottoms = np.zeros(len(pivot_df), dtype=float)\n",
    "    for col in pivot_df.columns:\n",
    "        ax.bar(years, pivot_df[col].values, bottom=bottoms,\n",
    "               label=str(col), color=race_colors.get(col))\n",
    "        bottoms += pivot_df[col].values\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Year\")\n",
    "    ax.set_ylim(0, ymax)\n",
    "    ax.set_yticks(yticks)\n",
    "    ax.grid(True, axis=\"y\", alpha=0.25)\n",
    "    ax.yaxis.set_major_formatter(mtick.FuncFormatter(lambda x, p: f\"{int(x):,}\"))\n",
    "\n",
    "    if ANNOTATE_TOTALS:\n",
    "        totals = pivot_df.sum(axis=1).values\n",
    "        for i, tot in enumerate(totals):\n",
    "            ax.text(i, tot, f\"{int(tot):,}\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "\n",
    "axes[0].set_ylabel(\"Total Approved Applications\")\n",
    "for ax, facet in zip(axes, FACETS):\n",
    "    plot_panel(ax, facet_pivots[facet], facet)\n",
    "\n",
    "handles = [plt.Rectangle((0,0),1,1,color=race_colors[r]) for r in race_order]\n",
    "fig.legend(\n",
    "    handles, race_order,\n",
    "    loc=\"upper center\",\n",
    "    ncol=min(5, len(race_order)),\n",
    "    frameon=False,\n",
    "    title=\"Race\",\n",
    "    bbox_to_anchor=(0.5, 0.92),          # lower than the title\n",
    "    bbox_transform=fig.transFigure``\n",
    ")\n",
    "\n",
    "plt.suptitle(\"Total Approved Applications by Race — Non-Bank vs Bank\", y=0.98)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.88])   # leave space at top for title + legend\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "fig.savefig(OUT_PNG, dpi=300, bbox_inches=\"tight\")\n",
    "fig.savefig(OUT_SVG, bbox_inches=\"tight\")\n",
    "print(f\"Saved: {OUT_PNG}\\nSaved: {OUT_SVG}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d4c0bb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372ed9aa-3a05-484c-bfc8-a99e34c93636",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Faceted chart (Non-Bank vs Bank) of TOTAL APPROVED APPLICATIONS by race over time.\n",
    "- One figure with two facets (Non-Bank | Bank)\n",
    "- Shared years, shared y-axis, consistent race colors\n",
    "- Saves PNG + SVG\n",
    "\n",
    "Expected columns:\n",
    "  - year\n",
    "  - lender_type  (values like \"Bank\", \"Non-Bank\" — script normalizes variants)\n",
    "  - race_label\n",
    "  - approved_count  (optional; if missing, computed from approval_rate × applications_total)\n",
    "  - approval_rate   (0–1 or 0–100; used only if approved_count missing)\n",
    "  - applications_total (or n_applications/applications/count/n; used only if approved_count missing)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "from pathlib import Path\n",
    "from math import floor, log10\n",
    "\n",
    "CSV_PATH = \"avg_interest_vs_approval_by_lei_race_year.csv\"\n",
    "OUT_DIR = Path(\".\")\n",
    "OUT_PNG = OUT_DIR / \"faceted_total_approved_by_race.png\"\n",
    "OUT_SVG = OUT_DIR / \"faceted_total_approved_by_race.svg\"\n",
    "\n",
    "FACETS = [\"Non-Bank\", \"Bank\"]\n",
    "\n",
    "RACE_ORDER = None  # e.g., [\"White\", \"Black\", \"Hispanic/Latino\", \"Asian\", \"Other/Unknown\"]\n",
    "\n",
    "ANNOTATE_TOTALS = True\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "required = {\"year\", \"lender_type\", \"race_label\"}\n",
    "missing = required - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"CSV missing required columns: {missing}\")\n",
    "\n",
    "df[\"year\"] = df[\"year\"].astype(str).str.extract(r\"(\\d{4})\")[0].astype(float)\n",
    "df = df.dropna(subset=[\"year\"]).copy()\n",
    "df[\"year\"] = df[\"year\"].astype(int)\n",
    "\n",
    "LENDER_MAP = {\n",
    "    \"bank\": \"Bank\", \"banks\": \"Bank\",\n",
    "    \"non-bank\": \"Non-Bank\", \"nonbank\": \"Non-Bank\", \"non bank\": \"Non-Bank\"\n",
    "}\n",
    "def norm_lender(val: str) -> str:\n",
    "    s = str(val).strip().lower()\n",
    "    if s in LENDER_MAP: return LENDER_MAP[s]\n",
    "    if \"non\" in s and \"bank\" in s: return \"Non-Bank\"\n",
    "    if \"bank\" in s: return \"Bank\"\n",
    "    return val if val in (\"Bank\", \"Non-Bank\") else s.title()\n",
    "\n",
    "df[\"lender_type\"] = df[\"lender_type\"].apply(norm_lender)\n",
    "\n",
    "approved_cols = [c for c in df.columns if c.lower() in {\"approved_count\", \"n_approved\", \"num_approved\"}]\n",
    "if approved_cols:\n",
    "    df[\"approved_count\"] = pd.to_numeric(df[approved_cols[0]], errors=\"coerce\")\n",
    "else:\n",
    "    rate_col = (\n",
    "        \"approval_rate\" if \"approval_rate\" in df.columns\n",
    "        else \"avg_approval_rate\" if \"avg_approval_rate\" in df.columns\n",
    "        else None\n",
    "    )\n",
    "    total_candidates = [c for c in df.columns if c.lower() in {\"applications_total\", \"n_applications\", \"applications\", \"count\", \"n\"}]\n",
    "    if not rate_col or not total_candidates:\n",
    "        raise ValueError(\"Need 'approved_count' OR (approval_rate/avg_approval_rate AND total applications column).\")\n",
    "    total_col = total_candidates[0]\n",
    "    rate = pd.to_numeric(df[rate_col], errors=\"coerce\")\n",
    "    total = pd.to_numeric(df[total_col], errors=\"coerce\")\n",
    "    rate = np.where(rate > 1.0, rate / 100.0, rate)  # normalize to 0–1\n",
    "    df[\"approved_count\"] = (rate * total).round()\n",
    "\n",
    "df[\"approved_count\"] = pd.to_numeric(df[\"approved_count\"], errors=\"coerce\").fillna(0).clip(lower=0)\n",
    "\n",
    "agg = (\n",
    "    df.groupby([\"lender_type\", \"year\", \"race_label\"], dropna=False)[\"approved_count\"]\n",
    "      .sum()\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "all_years = sorted(agg[\"year\"].unique().tolist())`\n",
    "\n",
    "if RACE_ORDER is not None:\n",
    "    race_order = RACE_ORDER\n",
    "else:\n",
    "    race_order = sorted(agg[\"race_label\"].dropna().unique().tolist())\n",
    "\n",
    "def build_pivot(ltype: str) -> pd.DataFrame:\n",
    "    sub = agg[agg[\"lender_type\"] == ltype].copy()\n",
    "    frame = (\n",
    "        sub.pivot_table(index=\"year\", columns=\"race_label\", values=\"approved_count\", aggfunc=\"sum\", fill_value=0)\n",
    "           .reindex(index=all_years, fill_value=0)\n",
    "    )\n",
    "    for r in race_order:\n",
    "        if r not in frame.columns:\n",
    "            frame[r] = 0\n",
    "    frame = frame.reindex(columns=race_order)\n",
    "    return frame\n",
    "\n",
    "facet_pivots = {ft: build_pivot(ft) for ft in FACETS}\n",
    "\n",
    "palette = plt.cm.tab20.colors if len(race_order) > 10 else plt.cm.tab10.colors\n",
    "race_colors = {r: palette[i % len(palette)] for i, r in enumerate(race_order)}\n",
    "\n",
    "def nice_ceil(x):\n",
    "    if x <= 0: return 1\n",
    "    exp = floor(log10(x))\n",
    "    frac = x / (10 ** exp)\n",
    "    nice_frac = 1 if frac <= 1 else 2 if frac <= 2 else 5 if frac <= 5 else 10\n",
    "    return int(nice_frac * (10 ** exp))\n",
    "\n",
    "global_max = max(p.sum(axis=1).max() for p in facet_pivots.values())\n",
    "ymax = nice_ceil(global_max)\n",
    "yticks = np.linspace(0, ymax, 6)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(FACETS), figsize=(12, 6), sharey=True)\n",
    "if len(FACETS) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "def plot_panel(ax, pivot_df: pd.DataFrame, title: str):\n",
    "    years = [str(y) for y in pivot_df.index.tolist()]\n",
    "    bottoms = np.zeros(len(pivot_df), dtype=float)\n",
    "    for col in pivot_df.columns:\n",
    "        ax.bar(years, pivot_df[col].values, bottom=bottoms,\n",
    "               label=str(col), color=race_colors.get(col))\n",
    "        bottoms += pivot_df[col].values\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Year\")\n",
    "    ax.set_ylim(0, ymax)\n",
    "    ax.set_yticks(yticks)\n",
    "    ax.grid(True, axis=\"y\", alpha=0.25)\n",
    "    ax.yaxis.set_major_formatter(mtick.FuncFormatter(lambda x, p: f\"{int(x):,}\"))\n",
    "\n",
    "    if ANNOTATE_TOTALS:\n",
    "        totals = pivot_df.sum(axis=1).values\n",
    "        for i, tot in enumerate(totals):\n",
    "            ax.text(i, tot, f\"{int(tot):,}\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "\n",
    "axes[0].set_ylabel(\"Total Approved Applications\")\n",
    "\n",
    "for ax, facet in zip(axes, FACETS):\n",
    "    plot_panel(ax, facet_pivots[facet], facet)\n",
    "\n",
    "handles = [plt.Rectangle((0,0),1,1,color=race_colors[r]) for r in race_order]\n",
    "fig.legend(handles, race_order, loc=\"upper center\", ncol=min(5, len(race_order)),\n",
    "           frameon=False, title=\"Race\", bbox_to_anchor=(0.5, 1.02))\n",
    "\n",
    "plt.suptitle(\"Total Approved Applications by Race — Non-Bank vs Bank\", y=0.98)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.94])\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "fig.savefig(OUT_PNG, dpi=300, bbox_inches=\"tight\")\n",
    "fig.savefig(OUT_SVG, bbox_inches=\"tight\")\n",
    "print(f\"Saved: {OUT_PNG}\\nSaved: {OUT_SVG}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70300c2e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eb2044-3be9-4f23-917f-05e9991b8ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_DIR = Path(\"filtered_data_strict\")  # folder with 2018_filtered_hmda.csv, 2019_filtered_hmda.csv, etc.\n",
    "PATTERN  = re.compile(r\"(\\d{4})_filtered_hmda\\.csv$\")\n",
    "OUT_CSV  = \"bank_nonbank_approval_share_by_year.csv\"\n",
    "OUT_PNG_LINE  = \"bank_nonbank_approval_share_by_year_line.png\"\n",
    "OUT_PNG_STACK = \"bank_nonbank_approval_share_by_year_100pct_stacked.png\"\n",
    "LENDER_ORDER  = [\"Bank\", \"Non-Bank\"]\n",
    "\n",
    "COLORS = {\n",
    "    \"Bank\": \"#66c2a5\",      # blue\n",
    "    \"Non-Bank\": \"#fc8d62\",  # orange\n",
    "}\n",
    "\n",
    "APPROVED_CODES = {1, 2}\n",
    "\n",
    "frames = []\n",
    "for f in DATA_DIR.glob(\"*_filtered_hmda.csv\"):\n",
    "    m = PATTERN.match(f.name)\n",
    "    if not m:\n",
    "        continue\n",
    "    year = int(m.group(1))\n",
    "    df = pd.read_csv(f, low_memory=False)\n",
    "\n",
    "    missing = [c for c in [\"purchaser_type\", \"action_taken\"] if c not in df.columns]\n",
    "    if missing:\n",
    "        print(f\"Warning: {f.name} missing columns {missing}; skipping.\")\n",
    "        continue\n",
    "\n",
    "    lender_type = (\n",
    "        df[\"purchaser_type\"]\n",
    "        .fillna(-1)\n",
    "        .astype(\"Int64\")\n",
    "        .apply(lambda x: \"Bank\" if x == 6 else \"Non-Bank\")\n",
    "    )\n",
    "\n",
    "    approved_mask = df[\"action_taken\"].isin(APPROVED_CODES)\n",
    "    sub = pd.DataFrame(\n",
    "        {\n",
    "            \"year\": year,\n",
    "            \"lender_type\": lender_type[approved_mask],\n",
    "        }\n",
    "    )\n",
    "    frames.append(sub)\n",
    "\n",
    "if not frames:\n",
    "    raise SystemExit(\"No *_filtered_hmda.csv files found (or files missing required columns). Check DATA_DIR and filenames.\")\n",
    "\n",
    "stack = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "counts = (\n",
    "    stack\n",
    "    .groupby([\"year\", \"lender_type\"])\n",
    "    .size()\n",
    "    .rename(\"approvals\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "years = sorted(counts[\"year\"].unique())\n",
    "for y in years:\n",
    "    for lt in LENDER_ORDER:\n",
    "        if not ((counts[\"year\"] == y) & (counts[\"lender_type\"] == lt)).any():\n",
    "            counts = pd.concat(\n",
    "                [counts, pd.DataFrame({\"year\": [y], \"lender_type\": [lt], \"approvals\": [0]})],\n",
    "                ignore_index=True\n",
    "            )\n",
    "\n",
    "counts = counts.sort_values([\"year\", \"lender_type\"]).reset_index(drop=True)\n",
    "\n",
    "counts[\"year_total_approvals\"] = counts.groupby(\"year\")[\"approvals\"].transform(\"sum\")\n",
    "counts[\"share_pct\"] = np.where(\n",
    "    counts[\"year_total_approvals\"] > 0,\n",
    "    counts[\"approvals\"] / counts[\"year_total_approvals\"] * 100.0,\n",
    "    0.0\n",
    ")\n",
    "\n",
    "counts.to_csv(OUT_CSV, index=False)\n",
    "print(f\"Saved -> {OUT_CSV}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "for lt in LENDER_ORDER:\n",
    "    sub = counts[counts[\"lender_type\"] == lt].sort_values(\"year\")\n",
    "    ax.plot(sub[\"year\"], sub[\"share_pct\"], marker=\"o\", linewidth=2, label=lt, color=COLORS[lt])\n",
    "\n",
    "ax.set_title(\"Share of Approved Applications by Lender Type, Year by Year\")\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"Share of Approvals (%)\")\n",
    "ax.set_ylim(0, 100)\n",
    "ax.set_xticks(sorted(counts[\"year\"].unique()))\n",
    "ax.legend(title=\"Lender Type\", frameon=False, loc=\"upper left\")\n",
    "ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_PNG_LINE, dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"Saved -> {OUT_PNG_LINE}\")\n",
    "\n",
    "pivot = (\n",
    "    counts.pivot(index=\"year\", columns=\"lender_type\", values=\"share_pct\")\n",
    "    .reindex(columns=LENDER_ORDER)\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "ax2 = pivot.plot(\n",
    "    kind=\"bar\",\n",
    "    stacked=True,\n",
    "    figsize=(9, 5),\n",
    "    width=0.8,\n",
    "    color=[COLORS[lt] for lt in LENDER_ORDER]\n",
    ")\n",
    "ax2.set_title(\"Approved Applications: Bank vs Non-Bank (100% Stacked)\")\n",
    "ax2.set_xlabel(\"Year\")\n",
    "ax2.set_ylabel(\"Share of Approvals (%)\")\n",
    "ax2.set_ylim(0, 100)\n",
    "ax2.legend(title=\"Lender Type\", frameon=False, loc=\"upper center\", ncol=2)\n",
    "\n",
    "for p in ax2.patches:\n",
    "    height = p.get_height()\n",
    "    if height > 0:\n",
    "        ax2.annotate(f\"{height:.1f}%\",\n",
    "                     (p.get_x() + p.get_width()/2, p.get_y() + height/2),\n",
    "                     ha=\"center\", va=\"center\", fontsize=8, color=\"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_PNG_STACK, dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"Saved -> {OUT_PNG_STACK}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e22153",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb36d00-a540-41eb-bfa8-fc3df565239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "src = pivot.copy()\n",
    "\n",
    "TARGET_STATES = [\"MS\", \"LA\", \"AR\"]\n",
    "\n",
    "COLOR_WHITE = \"#6e6e6e\"   # gray line for White\n",
    "COLOR_BLACK = \"#000000\"   # black line for Black\n",
    "COLOR_GAP   = \"#f7c5cc\"   # soft pink fill\n",
    "MARKER      = \"o\"\n",
    "\n",
    "Y_MIN, Y_MAX = 50, 100\n",
    "\n",
    "fig, axes = plt.subplots(1, len(TARGET_STATES), figsize=(13, 4), sharey=True)\n",
    "if len(TARGET_STATES) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, st in zip(axes, TARGET_STATES):\n",
    "    sub = src[src[\"state_abbr\"] == st].sort_values(\"year\")\n",
    "    if sub.empty:\n",
    "        ax.text(0.5, 0.5, \"No data\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "        ax.set_title(st); ax.set_xlabel(\"Year\"); continue\n",
    "\n",
    "    x   = sub[\"year\"].to_numpy()\n",
    "    yw  = sub[\"White\"].to_numpy(dtype=float)\n",
    "    yb  = sub[\"Black\"].to_numpy(dtype=float)\n",
    "\n",
    "    mask = np.isfinite(yw) & np.isfinite(yb)\n",
    "\n",
    "    ax.fill_between(x[mask], yb[mask], yw[mask], facecolor=COLOR_GAP, alpha=0.5, label=\"Gap\")\n",
    "\n",
    "    ax.plot(x, yw, marker=MARKER, linewidth=2.2, color=COLOR_WHITE, label=\"White Approval Rate\")\n",
    "    ax.plot(x, yb, marker=MARKER, linewidth=2.2, color=COLOR_BLACK, label=\"Black Approval Rate\")\n",
    "\n",
    "    if mask.any():\n",
    "        last_idx = np.where(mask)[0][-1]\n",
    "        latest_gap = (yw[last_idx] - yb[last_idx])\n",
    "        ax.annotate(f\"Latest Gap: {latest_gap:.1f}%\",\n",
    "                    (x[last_idx], yw[last_idx]),\n",
    "                    xytext=(8, -18), textcoords=\"offset points\",\n",
    "                    ha=\"left\", va=\"top\",\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.25\", fc=\"white\", ec=\"#cccccc\"),\n",
    "                    fontsize=9)\n",
    "\n",
    "    ax.set_title(st)\n",
    "    ax.set_xlabel(\"Year\")\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.35)\n",
    "    ax.set_xticks(x)\n",
    "    if 'Y_MIN' in locals() and 'Y_MAX' in locals():\n",
    "        ax.set_ylim(Y_MIN, Y_MAX)\n",
    "\n",
    "axes[0].set_ylabel(\"Approval Rate (%)\")\n",
    "\n",
    "legend_handles = [\n",
    "    Patch(facecolor=COLOR_GAP, alpha=0.5, label=\"Gap\"),\n",
    "    Line2D([0],[0], color=COLOR_WHITE, marker=MARKER, lw=2.2, label=\"White Approval Rate\"),\n",
    "    Line2D([0],[0], color=COLOR_BLACK, marker=MARKER, lw=2.2, label=\"Black Approval Rate\"),\n",
    "]\n",
    "fig.legend(handles=legend_handles, loc=\"upper center\", ncol=3, frameon=False, bbox_to_anchor=(0.5, 1.02))\n",
    "\n",
    "plt.suptitle(\"White vs Black Approval Rates — Shaded Gap\", y=1.08)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3e70cc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9b01a3-12a4-4b84-bced-131282bf6ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "CSV_PATH = \"bank_nonbank_approval_share_by_year.csv\"  # cols: year, lender_type, share_pct (0–100)\n",
    "BASELINE_YEAR = 2018\n",
    "COLOR_NB = \"#ff7f0e\"  # non-bank gain (orange)\n",
    "COLOR_BK = \"#1f77b4\"  # bank loss (blue)\n",
    "OUT_PNG = \"diverging_share_change_lollipop.png\"\n",
    "OUT_SVG = \"diverging_share_change_lollipop.svg\"\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df = df[df[\"lender_type\"].isin([\"Bank\", \"Non-Bank\"])].copy()\n",
    "df[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\").astype(int)\n",
    "\n",
    "wide = (\n",
    "    df.pivot_table(index=\"year\", columns=\"lender_type\", values=\"share_pct\", aggfunc=\"mean\")\n",
    "      .sort_index()\n",
    ")\n",
    "if BASELINE_YEAR not in wide.index:\n",
    "    raise ValueError(f\"Baseline year {BASELINE_YEAR} not in data\")\n",
    "\n",
    "delta_nb = wide[\"Non-Bank\"] - wide.loc[BASELINE_YEAR, \"Non-Bank\"]\n",
    "delta_bk = wide[\"Bank\"]     - wide.loc[BASELINE_YEAR, \"Bank\"]\n",
    "\n",
    "years = delta_nb.index.tolist()\n",
    "vals_nb = delta_nb.values\n",
    "vals_bk = delta_bk.values\n",
    "ypos = np.arange(len(years))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8.5, 5.5))\n",
    "\n",
    "for y, nb, bk in zip(ypos, vals_nb, vals_bk):\n",
    "    if not np.isnan(nb):\n",
    "        ax.hlines(y, 0, nb, color=COLOR_NB, lw=2)\n",
    "    if not np.isnan(bk):\n",
    "        ax.hlines(y, 0, bk, color=COLOR_BK, lw=2)\n",
    "\n",
    "ax.scatter(vals_nb, ypos, s=70, color=COLOR_NB, zorder=3, label=\"Non-Bank Δ vs 2018\")\n",
    "ax.scatter(vals_bk, ypos, s=70, color=COLOR_BK, zorder=3, label=\"Bank Δ vs 2018\")\n",
    "\n",
    "ax.axvline(0, color=\"#888\", lw=1, ls=\"--\")\n",
    "\n",
    "ax.set_yticks(ypos)\n",
    "ax.set_yticklabels(years)\n",
    "ax.set_xlabel(\"Change in Share of Approved Applications vs 2018 (pp)\")\n",
    "ax.set_title(\"Banks vs Non-Banks: Share Change (2018 = 0)\")\n",
    "\n",
    "xmax = np.nanmax(np.abs(np.concatenate([vals_nb, vals_bk])))\n",
    "pad = max(1, 0.06 * xmax)\n",
    "ax.set_xlim(-xmax - pad, xmax + pad)\n",
    "\n",
    "for y, v in zip(ypos, vals_nb):\n",
    "    if np.isnan(v): continue\n",
    "    ax.text(v + (0.35 if v >= 0 else -0.35), y, f\"{v:+.1f} pp\",\n",
    "            va=\"center\", ha=\"left\" if v >= 0 else \"right\", fontsize=9)\n",
    "for y, v in zip(ypos, vals_bk):\n",
    "    if np.isnan(v): continue\n",
    "    ax.text(v - (0.35 if v < 0 else -0.35), y, f\"{v:+.1f} pp\",\n",
    "            va=\"center\", ha=\"right\" if v < 0 else \"left\", fontsize=9)\n",
    "\n",
    "ax.legend(frameon=False, loc=\"lower right\")\n",
    "ax.grid(axis=\"x\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "Path(\".\").mkdir(exist_ok=True, parents=True)\n",
    "plt.savefig(OUT_PNG, dpi=300, bbox_inches=\"tight\")\n",
    "plt.savefig(OUT_SVG, bbox_inches=\"tight\")\n",
    "print(f\"Saved: {OUT_PNG}\\nSaved: {OUT_SVG}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8883e2ce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6044f375-b5d8-4aa9-824b-00992a72a002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "CSV_PATH = \"bank_nonbank_approval_share_by_year.csv\"  # cols: year, lender_type, share_pct (0–100 or 0–1)\n",
    "BASELINE_YEAR = 2018\n",
    "COLOR_NB = \"#ff7f0e\"  # Non-Bank\n",
    "COLOR_BK = \"#1f77b4\"  # Bank\n",
    "OUT_PNG = \"nonbank_absolute_and_bank_change_from_2018.png\"\n",
    "OUT_SVG = \"nonbank_absolute_and_bank_change_from_2018.svg\"\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df = df[df[\"lender_type\"].isin([\"Bank\", \"Non-Bank\"])].copy()\n",
    "df[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\").astype(int)\n",
    "\n",
    "if df[\"share_pct\"].dropna().max() <= 1.000001:\n",
    "    df[\"share_pct\"] *= 100.0\n",
    "\n",
    "wide = (\n",
    "    df.pivot_table(index=\"year\", columns=\"lender_type\", values=\"share_pct\", aggfunc=\"mean\")\n",
    "      .sort_index()\n",
    ")\n",
    "\n",
    "if BASELINE_YEAR not in wide.index:\n",
    "    raise ValueError(f\"Baseline year {BASELINE_YEAR} not in data\")\n",
    "\n",
    "nb_series = wide[\"Non-Bank\"].copy()\n",
    "nb_baseline = nb_series.loc[BASELINE_YEAR]\n",
    "\n",
    "bk_delta = wide[\"Bank\"] - wide.loc[BASELINE_YEAR, \"Bank\"]\n",
    "\n",
    "years = wide.index.to_list()\n",
    "ypos  = np.arange(len(years))  # for lollipop panel\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "ax1, ax2 = axes\n",
    "\n",
    "ax1.plot(years, nb_series.values, marker=\"o\", lw=2.2, color=COLOR_NB)\n",
    "ax1.axhline(nb_baseline, color=\"#bbbbbb\", lw=1, ls=\"--\", label=f\"{BASELINE_YEAR} level\")\n",
    "ax1.set_title(\"Non-Bank Share of Approved Applications\")\n",
    "ax1.set_xlabel(\"Year\"); ax1.set_ylabel(\"Share (%)\")\n",
    "ax1.set_xticks(years)\n",
    "ax1.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "ax1.legend(frameon=False, loc=\"lower right\")\n",
    "\n",
    "vals_bk = bk_delta.values\n",
    "for y, v in zip(ypos, vals_bk):\n",
    "    ax2.hlines(y, 0, v, color=COLOR_BK, lw=2)\n",
    "ax2.scatter(vals_bk, ypos, s=70, color=COLOR_BK, zorder=3, label=\"Bank Δ vs 2018\")\n",
    "\n",
    "ax2.axvline(0, color=\"#888\", lw=1, ls=\"--\")\n",
    "ax2.set_yticks(ypos); ax2.set_yticklabels(years)\n",
    "ax2.set_xlabel(\"Change vs 2018 (pp)\")\n",
    "ax2.set_title(\"Bank Share Change (2018 = 0)\")\n",
    "xmax = np.nanmax(np.abs(vals_bk)); pad = max(1, 0.06 * xmax)\n",
    "ax2.set_xlim(-xmax - pad, xmax + pad)\n",
    "ax2.grid(axis=\"x\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "for y, v in zip(ypos, vals_bk):\n",
    "    ax2.text(v + (0.35 if v >= 0 else -0.35), y, f\"{v:+.1f} pp\",\n",
    "             va=\"center\", ha=\"left\" if v >= 0 else \"right\", fontsize=9)\n",
    "\n",
    "plt.suptitle(\"Non-Bank Absolute Share and Bank Change vs 2018\", y=1.02)\n",
    "plt.tight_layout()\n",
    "Path(\".\").mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(OUT_PNG, dpi=300, bbox_inches=\"tight\")\n",
    "plt.savefig(OUT_SVG, bbox_inches=\"tight\")\n",
    "print(f\"Saved: {OUT_PNG}\\nSaved: {OUT_SVG}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302427a4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55baa0b-0453-4da3-ad9d-ac7d8fdbd900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "CSV_PATH = \"bank_nonbank_approval_share_by_year.csv\"  # year, lender_type, share_pct\n",
    "BASELINE_YEAR = 2018\n",
    "COLOR_BK = \"#1f77b4\"   # Bank\n",
    "COLOR_NB = \"#ff7f0e\"   # Non-Bank\n",
    "OUT_PNG = \"bank_vs_nonbank_change_vs_2018.png\"\n",
    "OUT_SVG = \"bank_vs_nonbank_change_vs_2018.svg\"\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df = df[df[\"lender_type\"].isin([\"Bank\", \"Non-Bank\"])].copy()\n",
    "df[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\").astype(int)\n",
    "\n",
    "if df[\"share_pct\"].max() <= 1.000001:\n",
    "    df[\"share_pct\"] *= 100\n",
    "\n",
    "wide = (\n",
    "    df.pivot_table(index=\"year\", columns=\"lender_type\", values=\"share_pct\", aggfunc=\"mean\")\n",
    "      .sort_index()\n",
    ")\n",
    "\n",
    "if BASELINE_YEAR not in wide.index:\n",
    "    raise ValueError(f\"Baseline year {BASELINE_YEAR} not in data\")\n",
    "\n",
    "delta_bk = wide[\"Bank\"] - wide.loc[BASELINE_YEAR, \"Bank\"]\n",
    "delta_nb = wide[\"Non-Bank\"] - wide.loc[BASELINE_YEAR, \"Non-Bank\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "ax.axhline(0, color=\"#888\", lw=1, ls=\"--\")\n",
    "\n",
    "ax.plot(wide.index, delta_bk, marker=\"o\", color=COLOR_BK, lw=2, label=\"Bank\")\n",
    "for x, y in zip(wide.index, delta_bk):\n",
    "    ax.text(x, y + (0.3 if y >= 0 else -0.5), f\"{y:+.1f} pp\",\n",
    "            ha=\"center\", va=\"bottom\" if y >= 0 else \"top\", fontsize=9, color=COLOR_BK)\n",
    "\n",
    "ax.plot(wide.index, delta_nb, marker=\"o\", color=COLOR_NB, lw=2, label=\"Non-Bank\")\n",
    "for x, y in zip(wide.index, delta_nb):\n",
    "    ax.text(x, y + (0.3 if y >= 0 else -0.5), f\"{y:+.1f} pp\",\n",
    "            ha=\"center\", va=\"bottom\" if y >= 0 else \"top\", fontsize=9, color=COLOR_NB)\n",
    "\n",
    "ax.set_title(\"Change in Share of Approved Applications vs 2018\")\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"Change vs 2018 (percentage points)\")\n",
    "ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_PNG, dpi=300)\n",
    "plt.savefig(OUT_SVG)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d0c1ee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04152539-982d-4722-888f-b8a8f52c5e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "CSV_PATH = \"bank_nonbank_approval_share_by_year.csv\"  # columns: year, lender_type, share_pct\n",
    "COLOR_BK = \"#1f77b4\"   # Bank blue\n",
    "BASELINE_YEAR = 2018\n",
    "OUT_PNG = \"bank_share_over_time.png\"\n",
    "OUT_SVG = \"bank_share_over_time.svg\"\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df = df[df[\"lender_type\"].isin([\"Bank\",\"Non-Bank\"])].copy()\n",
    "df[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\").astype(int)\n",
    "\n",
    "if df[\"share_pct\"].dropna().max() <= 1.000001:\n",
    "    df[\"share_pct\"] *= 100.0\n",
    "\n",
    "wide = (\n",
    "    df.pivot_table(index=\"year\", columns=\"lender_type\", values=\"share_pct\", aggfunc=\"mean\")\n",
    "      .sort_index()\n",
    ")\n",
    "\n",
    "if \"Bank\" not in wide.columns:\n",
    "    raise ValueError(\"No 'Bank' rows found in the CSV.\")\n",
    "if BASELINE_YEAR not in wide.index:\n",
    "    raise ValueError(f\"Baseline year {BASELINE_YEAR} not present in data.\")\n",
    "\n",
    "bank_series = wide[\"Bank\"]\n",
    "baseline_val = bank_series.loc[BASELINE_YEAR]\n",
    "years = bank_series.index.tolist()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "ax.plot(years, bank_series.values, color=COLOR_BK, lw=2.5, marker=\"o\")\n",
    "ax.axhline(baseline_val, color=\"#bbbbbb\", lw=1, ls=\"--\")\n",
    "ax.text(years[0], baseline_val, \"  2018 level\", va=\"center\", ha=\"left\", color=\"#888888\")\n",
    "\n",
    "ax.set_title(\"Bank Share of Approved Applications\")\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"Share (%)\")\n",
    "ax.set_xticks(years)\n",
    "ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.35)\n",
    "\n",
    "plt.tight_layout()\n",
    "Path(\".\").mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(OUT_PNG, dpi=300, bbox_inches=\"tight\")\n",
    "plt.savefig(OUT_SVG, dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"Saved: {OUT_PNG}\\nSaved: {OUT_SVG}\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9877f9f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e941a413-fa6e-4914-9602-40cb66106eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "CSV_PATH = \"bank_nonbank_interest_rate_by_race_year.csv\"\n",
    "COLOR_BK = \"#66c2a5\"   # Bank — teal\n",
    "COLOR_NB = \"#ff7f0e\"   # Non-Bank — orange\n",
    "MARKERS   = {\"Bank\":\"o\",\"Non-Bank\":\"s\"}\n",
    "\n",
    "PLOT_BY_RACE = False   # set True to get one panel per race\n",
    "\n",
    "OUT_PNG = \"avg_interest_rate_banks_vs_nonbanks_by_year.png\"\n",
    "OUT_SVG = \"avg_interest_rate_banks_vs_nonbanks_by_year.svg\"\n",
    "\n",
    "df = pd.read_csv(CSV_PATH, low_memory=False)\n",
    "\n",
    "def pick(cands):\n",
    "    for c in cands:\n",
    "        if c in df.columns: return c\n",
    "    return None\n",
    "\n",
    "year_col   = pick([\"year\",\"activity_year\"])\n",
    "rate_col   = pick([\"avg_interest_rate\",\"interest_rate\",\"mean_interest_rate\",\"rate\",\"rate_percent\"])\n",
    "ltype_col  = pick([\"lender_type\",\"lender\",\"lenderClass\",\"lender_category\"])\n",
    "race_col   = pick([\"race_label\",\"race\",\"derived_race\"])\n",
    "weight_col = pick([\"n_loans\",\"count\",\"applications_total\",\"n\",\"obs\",\"records\"])\n",
    "\n",
    "if year_col is None or rate_col is None or ltype_col is None:\n",
    "    raise ValueError(f\"Need year/lender/rate columns. Found: {df.columns.tolist()}\")\n",
    "\n",
    "df[\"year\"] = pd.to_numeric(df[year_col], errors=\"coerce\").astype(\"Int64\")\n",
    "df[rate_col] = pd.to_numeric(df[rate_col], errors=\"coerce\")\n",
    "\n",
    "def norm_lt(s):\n",
    "    s = str(s).strip().lower()\n",
    "    if \"non\" in s and \"bank\" in s: return \"Non-Bank\"\n",
    "    if \"bank\" in s: return \"Bank\"\n",
    "    return None\n",
    "df[\"lender_type\"] = df[ltype_col].apply(norm_lt)\n",
    "\n",
    "df = df.dropna(subset=[\"year\",\"lender_type\", rate_col]).copy()\n",
    "\n",
    "df = df[(df[rate_col] > 0) & (df[rate_col] < 40)].copy()\n",
    "\n",
    "def weighted_mean(g, val, w):\n",
    "    if w is None:  # simple mean\n",
    "        return g[val].mean()\n",
    "    wv = pd.to_numeric(g[w], errors=\"coerce\").fillna(0)\n",
    "    if wv.sum() == 0: return g[val].mean()\n",
    "    return np.average(g[val], weights=wv)\n",
    "\n",
    "group_keys = [\"year\",\"lender_type\"] + ([race_col] if (PLOT_BY_RACE and race_col) else [])\n",
    "\n",
    "agg = (\n",
    "    df.groupby(group_keys, dropna=False)\n",
    "      .apply(lambda g: weighted_mean(g, rate_col, weight_col))\n",
    "      .rename(\"avg_interest_rate\")\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "if PLOT_BY_RACE and race_col:\n",
    "    races = [r for r in agg[race_col].dropna().unique().tolist()]\n",
    "    n = max(1, len(races))\n",
    "    fig, axes = plt.subplots(1, n, figsize=(6*n, 4), sharey=True)\n",
    "    if n == 1: axes = [axes]\n",
    "\n",
    "    for ax, r in zip(axes, races):\n",
    "        sub = agg[agg[race_col] == r].pivot(index=\"year\", columns=\"lender_type\", values=\"avg_interest_rate\")\n",
    "        sub = sub.reindex(columns=[\"Bank\",\"Non-Bank\"]).sort_index()\n",
    "\n",
    "        years = sub.index.values\n",
    "        if \"Bank\" in sub:\n",
    "            ax.plot(years, sub[\"Bank\"], marker=MARKERS[\"Bank\"], lw=2.2, color=COLOR_BK, label=\"Bank\")\n",
    "        if \"Non-Bank\" in sub:\n",
    "            ax.plot(years, sub[\"Non-Bank\"], marker=MARKERS[\"Non-Bank\"], lw=2.2, color=COLOR_NB, label=\"Non-Bank\")\n",
    "\n",
    "        ax.set_title(str(r))\n",
    "        ax.set_xlabel(\"Year\"); ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.35)\n",
    "        ax.set_xticks(years)\n",
    "\n",
    "    axes[0].set_ylabel(\"Average Interest Rate (%)\")\n",
    "    fig.legend(loc=\"upper center\", ncol=2, frameon=False, bbox_to_anchor=(0.5, 1.02))\n",
    "    plt.suptitle(\"Average Interest Rate by Year — Banks vs Non-Banks\", y=1.04)\n",
    "    plt.tight_layout(rect=[0,0,1,0.96])\n",
    "else:\n",
    "    wide = agg.pivot_table(index=\"year\", columns=\"lender_type\", values=\"avg_interest_rate\", aggfunc=\"mean\")\n",
    "    wide = wide.reindex(columns=[\"Bank\",\"Non-Bank\"]).sort_index()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    years = wide.index.values\n",
    "    if \"Bank\" in wide:\n",
    "        ax.plot(years, wide[\"Bank\"], marker=MARKERS[\"Bank\"], lw=2.2, color=COLOR_BK, label=\"Bank\")\n",
    "    if \"Non-Bank\" in wide:\n",
    "        ax.plot(years, wide[\"Non-Bank\"], marker=MARKERS[\"Non-Bank\"], lw=2.2, color=COLOR_NB, label=\"Non-Bank\")\n",
    "\n",
    "    ax.set_title(\"Average Interest Rate by Year — Banks vs Non-Banks\")\n",
    "    ax.set_xlabel(\"Year\"); ax.set_ylabel(\"Average Interest Rate (%)\")\n",
    "    ax.set_xticks(years); ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.35)\n",
    "    ax.legend(frameon=False, loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.savefig(OUT_PNG, dpi=300, bbox_inches=\"tight\")\n",
    "plt.savefig(OUT_SVG, dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"Saved: {OUT_PNG}\\nSaved: {OUT_SVG}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758fdb8c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778385c8-6343-48ad-8607-0cb0a1fd4805",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21fdfe4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a604c07-d6f3-42dd-98e0-b0fc2fd57738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "folder_path = \"filtered_data_strict\"  # adjust if needed\n",
    "row_count = 0\n",
    "\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith(\".csv\"):\n",
    "        df = pd.read_csv(os.path.join(folder_path, file))\n",
    "        numeric_rows = df[pd.to_numeric(df['purchaser_type'], errors='coerce').notna()]\n",
    "        row_count += len(numeric_rows)\n",
    "\n",
    "print(f\"Total rows with numeric purchaser_type: {row_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3643095",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379c250f-b19f-4b9c-a3aa-28ab9763b10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "folder_path = \"filtered_data_strict\"  # adjust if needed\n",
    "\n",
    "total_rows = 0\n",
    "numeric_rows = 0\n",
    "\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith(\".csv\"):\n",
    "        df = pd.read_csv(os.path.join(folder_path, file))\n",
    "        total_rows += len(df)\n",
    "        numeric_rows += df[pd.to_numeric(df['purchaser_type'], errors='coerce').notna()].shape[0]\n",
    "\n",
    "print(f\"Total rows: {total_rows}\")\n",
    "print(f\"Rows with numeric purchaser_type: {numeric_rows}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e116cc2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325a9369-e52a-4ffb-b5cf-f78bc802ac6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
